# Awesome llm hallucination (truth-gpt)

- [Awesome llm hallucination (truth-gpt)](#awesome-llm-hallucination-truth-gpt)
	- [Survey](#survey)
	- [Papers](#papers)
	- [Evalution](#evalution)
	- [Other](#other)
	- [Extra reference](#extra-reference)


## Survey
- **Can We Catch the Elephant? The Evolvement of Hallucination Evaluation on
  Natural Language Generation: A Survey**, `arXiv, 2404.12041`, [arxiv](http://arxiv.org/abs/2404.12041v1), [pdf](http://arxiv.org/pdf/2404.12041v1.pdf), cication: [**-1**](None)

	 *Siya Qi, Yulan He, Zheng Yuan*
- **A Survey on Hallucination in Large Vision-Language Models**, `arXiv, 2402.00253`, [arxiv](http://arxiv.org/abs/2402.00253v1), [pdf](http://arxiv.org/pdf/2402.00253v1.pdf), cication: [**-1**](None)

	 *Hanchao Liu, Wenyuan Xue, Yifei Chen, Dapeng Chen, Xiutian Zhao, Ke Wang, Liping Hou, Rongjun Li, Wei Peng*
- **TrustLLM: Trustworthiness in Large Language Models**, `arXiv, 2401.05561`, [arxiv](http://arxiv.org/abs/2401.05561v1), [pdf](http://arxiv.org/pdf/2401.05561v1.pdf), cication: [**-1**](None)

	 *Lichao Sun, Yue Huang, Haoran Wang, Siyuan Wu, Qihui Zhang, Chujie Gao, Yixin Huang, Wenhan Lyu, Yixuan Zhang, Xiner Li* · ([trustllmbenchmark.github](https://trustllmbenchmark.github.io/TrustLLM-Website/))
- **A Comprehensive Survey of Hallucination Mitigation Techniques in Large
  Language Models**, `arXiv, 2401.01313`, [arxiv](http://arxiv.org/abs/2401.01313v3), [pdf](http://arxiv.org/pdf/2401.01313v3.pdf), cication: [**-1**](None)

	 *S. M Towhidul Islam Tonmoy, S M Mehedi Zaman, Vinija Jain, Anku Rani, Vipula Rawte, Aman Chadha, Amitava Das*
- **A Survey on Hallucination in Large Language Models: Principles,
  Taxonomy, Challenges, and Open Questions**, `arXiv, 2311.05232`, [arxiv](http://arxiv.org/abs/2311.05232v1), [pdf](http://arxiv.org/pdf/2311.05232v1.pdf), cication: [**-1**](None)

	 *Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin*
- **Survey on Factuality in Large Language Models: Knowledge, Retrieval and
  Domain-Specificity**, `arXiv, 2310.07521`, [arxiv](http://arxiv.org/abs/2310.07521v2), [pdf](http://arxiv.org/pdf/2310.07521v2.pdf), cication: [**3**](https://scholar.google.com/scholar?cites=14854019000652979716&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Cunxiang Wang, Xiaoze Liu, Yuanhao Yue, Xiangru Tang, Tianhang Zhang, Cheng Jiayang, Yunzhi Yao, Wenyang Gao, Xuming Hu, Zehan Qi* · ([jiqizhixin](https://www.jiqizhixin.com/articles/2023-10-19-2))

## Papers
- [**Lamini-Memory-Tuning**](https://github.com/lamini-ai/Lamini-Memory-Tuning/tree/main) - lamini-ai ![Star](https://img.shields.io/github/stars/lamini-ai/Lamini-Memory-Tuning.svg?style=social&label=Star)

	 *Banishing LLM Hallucinations Requires Rethinking Generalization* · ([huggingface](https://huggingface.co/engineering-lamini/lamini-1-random))
- **To Believe or Not to Believe Your LLM**, `arXiv, 2406.02543`, [arxiv](http://arxiv.org/abs/2406.02543v1), [pdf](http://arxiv.org/pdf/2406.02543v1.pdf), cication: [**-1**](None)

	 *Yasin Abbasi Yadkori, Ilja Kuzborskij, András György, Csaba Szepesvári*
- **Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?**, `arXiv, 2405.05904`, [arxiv](http://arxiv.org/abs/2405.05904v1), [pdf](http://arxiv.org/pdf/2405.05904v1.pdf), cication: [**-1**](None)

	 *Zorik Gekhman, Gal Yona, Roee Aharoni, Matan Eyal, Amir Feder, Roi Reichart, Jonathan Herzig*
- **In Search of Truth: An Interrogation Approach to Hallucination Detection**, `arXiv, 2403.02889`, [arxiv](http://arxiv.org/abs/2403.02889v1), [pdf](http://arxiv.org/pdf/2403.02889v1.pdf), cication: [**-1**](None)

	 *Yakir Yehuda, Itzik Malkiel, Oren Barkan, Jonathan Weill, Royi Ronen, Noam Koenigstein*
- **Fine-grained Hallucination Detection and Editing for Language Models**, `arXiv, 2401.06855`, [arxiv](http://arxiv.org/abs/2401.06855v2), [pdf](http://arxiv.org/pdf/2401.06855v2.pdf), cication: [**-1**](None)

	 *Abhika Mishra, Akari Asai, Vidhisha Balachandran, Yizhong Wang, Graham Neubig, Yulia Tsvetkov, Hannaneh Hajishirzi* · ([huggingface](https://huggingface.co/spaces/fava-uw/fava))
- **Steering Llama 2 via Contrastive Activation Addition**, `arXiv, 2312.06681`, [arxiv](http://arxiv.org/abs/2312.06681v1), [pdf](http://arxiv.org/pdf/2312.06681v1.pdf), cication: [**-1**](None)

	 *Nina Rimsky, Nick Gabrieli, Julian Schulz, Meg Tong, Evan Hubinger, Alexander Matt Turner*
- **Unlocking Anticipatory Text Generation: A Constrained Approach for
  Faithful Decoding with Large Language Models**, `arXiv, 2312.06149`, [arxiv](http://arxiv.org/abs/2312.06149v1), [pdf](http://arxiv.org/pdf/2312.06149v1.pdf), cication: [**-1**](None)

	 *Lifu Tu, Semih Yavuz, Jin Qu, Jiacheng Xu, Rui Meng, Caiming Xiong, Yingbo Zhou*
- **Technical Report: Large Language Models can Strategically Deceive their
  Users when Put Under Pressure**, `arXiv, 2311.07590`, [arxiv](http://arxiv.org/abs/2311.07590v1), [pdf](http://arxiv.org/pdf/2311.07590v1.pdf), cication: [**-1**](None)

	 *Jérémy Scheurer, Mikita Balesni, Marius Hobbhahn*
- **Calibrated Language Models Must Hallucinate**, `arXiv, 2311.14648`, [arxiv](http://arxiv.org/abs/2311.14648v2), [pdf](http://arxiv.org/pdf/2311.14648v2.pdf), cication: [**-1**](None)

	 *Adam Tauman Kalai, Santosh S. Vempala* · ([jiqizhixin](https://www.jiqizhixin.com/articles/2024-01-02-19))
- **Fine-tuning Language Models for Factuality**, `arXiv, 2311.08401`, [arxiv](http://arxiv.org/abs/2311.08401v1), [pdf](http://arxiv.org/pdf/2311.08401v1.pdf), cication: [**-1**](None)

	 *Katherine Tian, Eric Mitchell, Huaxiu Yao, Christopher D. Manning, Chelsea Finn*
- **Holistic Analysis of Hallucination in GPT-4V(ision): Bias and
  Interference Challenges**, `arXiv, 2311.03287`, [arxiv](http://arxiv.org/abs/2311.03287v2), [pdf](http://arxiv.org/pdf/2311.03287v2.pdf), cication: [**-1**](None)

	 *Chenhang Cui, Yiyang Zhou, Xinyu Yang, Shirley Wu, Linjun Zhang, James Zou, Huaxiu Yao* · ([Bingo](https://github.com/gzcch/Bingo) - gzcch) ![Star](https://img.shields.io/github/stars/gzcch/Bingo.svg?style=social&label=Star)

	 · ([jiqizhixin](https://www.jiqizhixin.com/articles/2023-11-13-12))
- **The Perils & Promises of Fact-checking with Large Language Models**, `arXiv, 2310.13549`, [arxiv](http://arxiv.org/abs/2310.13549v1), [pdf](http://arxiv.org/pdf/2310.13549v1.pdf), cication: [**1**](https://scholar.google.com/scholar?cites=2141835654434627141&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Dorian Quelle, Alexandre Bovet*
- **Adaptation with Self-Evaluation to Improve Selective Prediction in LLMs**, `arXiv, 2310.11689`, [arxiv](http://arxiv.org/abs/2310.11689v2), [pdf](http://arxiv.org/pdf/2310.11689v2.pdf), cication: [**2**](https://scholar.google.com/scholar?cites=12344385740050837344&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Jiefeng Chen, Jinsung Yoon, Sayna Ebrahimi, Sercan O Arik, Tomas Pfister, Somesh Jha* · ([mp.weixin.qq](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652436159&idx=5&sn=efff909bd5be29b179379c4ac1d7ab0e))
- **Personas as a Way to Model Truthfulness in Language Models**, `arXiv, 2310.18168`, [arxiv](http://arxiv.org/abs/2310.18168v2), [pdf](http://arxiv.org/pdf/2310.18168v2.pdf), cication: [**-1**](None)

	 *Nitish Joshi, Javier Rando, Abulhair Saparov, Najoung Kim, He He*
- **Woodpecker: Hallucination Correction for Multimodal Large Language
  Models**, `arXiv, 2310.16045`, [arxiv](http://arxiv.org/abs/2310.16045v1), [pdf](http://arxiv.org/pdf/2310.16045v1.pdf), cication: [**1**](https://scholar.google.com/scholar?cites=522629295517903693&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Shukang Yin, Chaoyou Fu, Sirui Zhao, Tong Xu, Hao Wang, Dianbo Sui, Yunhang Shen, Ke Li, Xing Sun, Enhong Chen* · ([qbitai](https://www.qbitai.com/2023/10/93766.html))
- **The Reversal Curse: LLMs trained on "A is B" fail to learn "B is A"**, `arXiv, 2309.12288`, [arxiv](http://arxiv.org/abs/2309.12288v2), [pdf](http://arxiv.org/pdf/2309.12288v2.pdf), cication: [**3**](https://scholar.google.com/scholar?cites=5106607594083514979&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Lukas Berglund, Meg Tong, Max Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz Korbak, Owain Evans* · ([mp.weixin.qq](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652381664&idx=1&sn=f0a437cf9cde5d92451bec90bbea9091))
- **Chain-of-Verification Reduces Hallucination in Large Language Models**, `arXiv, 2309.11495`, [arxiv](http://arxiv.org/abs/2309.11495v2), [pdf](http://arxiv.org/pdf/2309.11495v2.pdf), cication: [**8**](https://scholar.google.com/scholar?cites=5213298442364780829&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, Jason Weston* · ([qbitai](https://www.qbitai.com/2023/09/85973.html))
- **DoLa: Decoding by Contrasting Layers Improves Factuality in Large
  Language Models**, `arXiv, 2309.03883`, [arxiv](http://arxiv.org/abs/2309.03883v1), [pdf](http://arxiv.org/pdf/2309.03883v1.pdf), cication: [**6**](https://scholar.google.com/scholar?cites=8391217322765741220&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James Glass, Pengcheng He* · ([mp.weixin.qq](https://mp.weixin.qq.com/s?__biz=MzAxMTk4NDkwNw==&mid=2247494594&idx=2&sn=1e9d5f174dc7327cc6553d11ab9a47fa))
- **Towards Measuring the Representation of Subjective Global Opinions in
  Language Models**, `arXiv, 2306.16388`, [arxiv](http://arxiv.org/abs/2306.16388v1), [pdf](http://arxiv.org/pdf/2306.16388v1.pdf), cication: [**17**](https://scholar.google.com/scholar?cites=5882147167026212372&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Esin Durmus, Karina Nyugen, Thomas I. Liao, Nicholas Schiefer, Amanda Askell, Anton Bakhtin, Carol Chen, Zac Hatfield-Dodds, Danny Hernandez, Nicholas Joseph*
- **Inference-Time Intervention: Eliciting Truthful Answers from a Language
  Model**, `arXiv, 2306.03341`, [arxiv](http://arxiv.org/abs/2306.03341v5), [pdf](http://arxiv.org/pdf/2306.03341v5.pdf), cication: [**14**](https://scholar.google.com/scholar?cites=8900411386972860415&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter Pfister, Martin Wattenberg* · ([mp.weixin.qq](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652343568&idx=3&sn=5f525fc561501e769711c9b29a717d1a))
- **DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT
  Models**, `arXiv, 2306.11698`, [arxiv](http://arxiv.org/abs/2306.11698v1), [pdf](http://arxiv.org/pdf/2306.11698v1.pdf), cication: [**24**](https://scholar.google.com/scholar?cites=12930725443717311591&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui Zhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer*
- **WikiChat: Stopping the Hallucination of Large Language Model Chatbots by
  Few-Shot Grounding on Wikipedia**, `arXiv, 2305.14292`, [arxiv](http://arxiv.org/abs/2305.14292v2), [pdf](http://arxiv.org/pdf/2305.14292v2.pdf), cication: [**-1**](None)

	 *Sina J. Semnani, Violet Z. Yao, Heidi C. Zhang, Monica S. Lam*

	 · ([wikichat](https://github.com/stanford-oval/wikichat?tab=readme-ov-file) - stanford-oval) ![Star](https://img.shields.io/github/stars/stanford-oval/wikichat?tab=readme-ov-file.svg?style=social&label=Star)

## Evalution
- **Long-form factuality in large language models**, `arXiv, 2403.18802`, [arxiv](http://arxiv.org/abs/2403.18802v1), [pdf](http://arxiv.org/pdf/2403.18802v1.pdf), cication: [**-1**](None)

	 *Jerry Wei, Chengrun Yang, Xinying Song, Yifeng Lu, Nathan Hu, Dustin Tran, Daiyi Peng, Ruibo Liu, Da Huang, Cosmo Du* · ([long-form-factuality](https://github.com/google-deepmind/long-form-factuality) - google-deepmind) ![Star](https://img.shields.io/github/stars/google-deepmind/long-form-factuality.svg?style=social&label=Star)
- **TofuEval: Evaluating Hallucinations of LLMs on Topic-Focused Dialogue
  Summarization**, `arXiv, 2402.13249`, [arxiv](http://arxiv.org/abs/2402.13249v1), [pdf](http://arxiv.org/pdf/2402.13249v1.pdf), cication: [**-1**](None)

	 *Liyan Tang, Igor Shalyminov, Amy Wing-mei Wong, Jon Burnsky, Jake W. Vincent, Yu'an Yang, Siffi Singh, Song Feng, Hwanjun Song, Hang Su*
- [The Hallucinations Leaderboard, an Open Effort to Measure Hallucinations in Large Language Models](https://huggingface.co/blog/leaderboards-on-the-hub-hallucinations)
- **HallusionBench: An Advanced Diagnostic Suite for Entangled Language
  Hallucination & Visual Illusion in Large Vision-Language Models**, `arXiv, 2310.14566`, [arxiv](http://arxiv.org/abs/2310.14566v2), [pdf](http://arxiv.org/pdf/2310.14566v2.pdf), cication: [**-1**](None)

	 *Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob* · ([HallusionBench](https://github.com/tianyi-lab/HallusionBench?tab=readme-ov-file) - tianyi-lab) ![Star](https://img.shields.io/github/stars/tianyi-lab/HallusionBench?tab=readme-ov-file.svg?style=social&label=Star)
- [**hallucination-leaderboard**](https://github.com/vectara/hallucination-leaderboard) - vectara ![Star](https://img.shields.io/github/stars/vectara/hallucination-leaderboard.svg?style=social&label=Star)

	 *Leaderboard Comparing LLM Performance at Producing Hallucinations when Summarizing Short Documents* · ([t](https://twitter.com/DrJimFan/status/1724464105371939301))
 
## Other

- [I fact-checked ChatGPT with Bard, Claude, and Copilot - and this AI was the most confidently incorrect](https://www.zdnet.com/article/i-fact-checked-chatgpt-with-bard-claude-and-copilot-and-this-ai-was-the-most-confidently-incorrect/?utm_source=ai-drop.beehiiv.com&utm_medium=referral&utm_campaign=bytedance-s-secret-use-of-openai-s-tech)
- [Automatic Hallucination detection with SelfCheckGPT NLI](https://huggingface.co/blog/dhuynh95/automatic-hallucination-detection)
- [大模型幻觉问题调研](https://mp.weixin.qq.com/s?__biz=MzAxMTk4NDkwNw==&mid=2247494247&idx=2&sn=a1614fa0f58c153330bb7f3c11652ad3)

## Extra reference