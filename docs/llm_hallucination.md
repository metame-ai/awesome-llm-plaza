# LLM Hallucination

- [LLM Hallucination](#llm-hallucination) 
  - [Survey](#survey)
  - [Hallucination](#hallucination)
  - [Evaluation](#evaluation)
  - [Multi Modal](#multi-modal)
  - [Projects](#projects)
  - [Misc](#misc)


## Survey

- **A Survey of Hallucination in Large Visual Language Models**, `arXiv, 2410.15359`, [arxiv](http://arxiv.org/abs/2410.15359v1), [pdf](http://arxiv.org/pdf/2410.15359v1.pdf), cication: [**-1**](None) 

	 *Wei Lan, Wenyi Chen, Qingfeng Chen, ..., Huiyu Zhou, Yi Pan*

## Hallucination

- [Detecting misbehavior in frontier reasoning models](https://openai.com/index/chain-of-thought-monitoring/) 
- **On the Trustworthiness of Generative Foundation Models: Guideline, 
  Assessment, and Perspective**, `arXiv, 2502.14296`, [arxiv](http://arxiv.org/abs/2502.14296v1), [pdf](http://arxiv.org/pdf/2502.14296v1.pdf), cication: [**-1**](None) 

	 *Yue Huang, Chujie Gao, Siyuan Wu, ..., Bo Li, Xiangliang Zhang* · ([trustgen.github](https://trustgen.github.io/)) · ([TrustEval-toolkit](https://github.com/TrustGen/TrustEval-toolkit) - TrustGen) ![Star](https://img.shields.io/github/stars/TrustGen/TrustEval-toolkit.svg?style=social&label=Star) · ([trusteval-docs.readthedocs](https://trusteval-docs.readthedocs.io/))
- **HALoGEN: Fantastic LLM Hallucinations and Where to Find Them**, `arXiv, 2501.08292`, [arxiv](http://arxiv.org/abs/2501.08292v1), [pdf](http://arxiv.org/pdf/2501.08292v1.pdf), cication: [**-1**](None) 

	 *Abhilasha Ravichander, Shrusti Ghela, David Wadden, ..., Yejin Choi*
- **The FACTS Grounding Leaderboard: Benchmarking LLMs' Ability to Ground 
  Responses to Long-Form Input**, `arXiv, 2501.03200`, [arxiv](http://arxiv.org/abs/2501.03200v1), [pdf](http://arxiv.org/pdf/2501.03200v1.pdf), cication: [**-1**](None) 

	 *Alon Jacovi, Andrew Wang, Chris Alberti, ..., Sasha Goldshtein, Dipanjan Das* · ([kaggle](https://www.kaggle.com/facts-leaderboard))
- **The Illusion-Illusion: Vision Language Models See Illusions Where There 
  are None**, `arXiv, 2412.18613`, [arxiv](http://arxiv.org/abs/2412.18613v1), [pdf](http://arxiv.org/pdf/2412.18613v1.pdf), cication: [**-1**](None) 

	 *Tomer Ullman*

	 · ([𝕏](https://x.com/TomerUllman/status/1869742224524861836))
- **Improving Factuality with Explicit Working Memory**, `arXiv, 2412.18069`, [arxiv](http://arxiv.org/abs/2412.18069v1), [pdf](http://arxiv.org/pdf/2412.18069v1.pdf), cication: [**-1**](None) 

	 *Mingda Chen, Yang Li, Karthik Padthe, ..., Gargi Gosh, Wen-tau Yih*
- **Improving Factuality with Explicit Working Memory**, `arXiv, 2412.18069`, [arxiv](http://arxiv.org/abs/2412.18069v1), [pdf](http://arxiv.org/pdf/2412.18069v1.pdf), cication: [**-1**](None) 

	 *Mingda Chen, Yang Li, Karthik Padthe, ..., Gargi Gosh, Wen-tau Yih*
- [FACTS Grounding: A new benchmark for evaluating the factuality of large language models](https://deepmind.google/discover/blog/facts-grounding-a-new-benchmark-for-evaluating-the-factuality-of-large-language-models/) 
- 🌟 **RetroLLM: Empowering Large Language Models to Retrieve Fine-grained 
  Evidence within Generation**, `arXiv, 2412.11919`, [arxiv](http://arxiv.org/abs/2412.11919v1), [pdf](http://arxiv.org/pdf/2412.11919v1.pdf), cication: [**-1**](None) 

	 *Xiaoxi Li, Jiajie Jin, Yujia Zhou, ..., Qi Ye, Zhicheng Dou* · ([RetroLLM.](https://github.com/sunnynexus/RetroLLM.) - sunnynexus) ![Star](https://img.shields.io/github/stars/sunnynexus/RetroLLM..svg?style=social&label=Star)
- **Distinguishing Ignorance from Error in LLM Hallucinations**, `arXiv, 2410.22071`, [arxiv](http://arxiv.org/abs/2410.22071v1), [pdf](http://arxiv.org/pdf/2410.22071v1.pdf), cication: [**-1**](None) 

	 *Adi Simhi, Jonathan Herzig, Idan Szpektor, ..., Yonatan Belinkov* · ([hallucination-mitigation](https://github.com/technion-cs-nlp/hallucination-mitigation) - technion-cs-nlp) ![Star](https://img.shields.io/github/stars/technion-cs-nlp/hallucination-mitigation.svg?style=social&label=Star) · ([x](https://x.com/AdiSimhi/status/1851650371615125563))
- **Mitigating Object Hallucination via Concentric Causal Attention**, `arXiv, 2410.15926`, [arxiv](http://arxiv.org/abs/2410.15926v1), [pdf](http://arxiv.org/pdf/2410.15926v1.pdf), cication: [**-1**](None) 

	 *Yun Xing, Yiheng Li, Ivan Laptev, ..., Shijian Lu*

	 · ([cca-llava](https://github.com/xing0047/cca-llava) - xing0047) ![Star](https://img.shields.io/github/stars/xing0047/cca-llava.svg?style=social&label=Star) · ([arxiv](https://arxiv.org/abs/2410.15926))
- **Can Knowledge Editing Really Correct Hallucinations?**, `arXiv, 2410.16251`, [arxiv](http://arxiv.org/abs/2410.16251v2), [pdf](http://arxiv.org/pdf/2410.16251v2.pdf), cication: [**-1**](None) 

	 *Baixiang Huang, Canyu Chen, Xiongxiao Xu, ..., Ali Payani, Kai Shu*

	 · ([llm-editing.github](https://llm-editing.github.io/))
- **DeCoRe: Decoding by Contrasting Retrieval Heads to Mitigate 
  Hallucinations**, `arXiv, 2410.18860`, [arxiv](http://arxiv.org/abs/2410.18860v1), [pdf](http://arxiv.org/pdf/2410.18860v1.pdf), cication: [**-1**](None)

	 *Aryo Pradipta Gema, Chen Jin, Ahmed Abdulaal, ..., Pasquale Minervini, Amrutha Saseendran*

	 · ([x](https://x.com/aryopg/status/1849812217765433674)) · ([aryopg.github](https://aryopg.github.io/decore/)) · ([decore](https://github.com/aryopg/decore) - aryopg) ![Star](https://img.shields.io/github/stars/aryopg/decore.svg?style=social&label=Star)

## Evaluation


## Multi Modal

- **MLLM can see? Dynamic Correction Decoding for Hallucination Mitigation**, `arXiv, 2410.11779`, [arxiv](http://arxiv.org/abs/2410.11779v1), [pdf](http://arxiv.org/pdf/2410.11779v1.pdf), cication: [**-1**](None) 

	 *Chenxi Wang, Xiang Chen, Ningyu Zhang, ..., Shumin Deng, Huajun Chen*
- **The Curse of Multi-Modalities: Evaluating Hallucinations of Large 
  Multimodal Models across Language, Visual, and Audio**, `arXiv, 2410.12787`, [arxiv](http://arxiv.org/abs/2410.12787v1), [pdf](http://arxiv.org/pdf/2410.12787v1.pdf), cication: [**-1**](None)

	 *Sicong Leng, Yun Xing, Zesen Cheng, ..., Chunyan Miao, Lidong Bing*

## Projects

- [**hallucination-leaderboard**](https://github.com/vectara/hallucination-leaderboard) - vectara ![Star](https://img.shields.io/github/stars/vectara/hallucination-leaderboard.svg?style=social&label=Star) 

## Misc
## Misc
- [papers on LLM hallucinations](https://x.com/omarsar0/status/1852733583036682710)  𝕏 