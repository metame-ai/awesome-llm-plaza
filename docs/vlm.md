# Vision-Language Models

- [Vision-Language Models](#vision-language-models)
  - [Survey](#survey)
  - [Vision-Language Models](#vision-language-models-1)
  - [Image](#image)
  - [Video](#video)
  - [Encoder](#encoder)
  - [Alignment](#alignment)
  - [Evaluation](#evaluation)
  - [Efficient](#efficient)
  - [Generation](#generation)
  - [Dataset](#dataset)
  - [Products](#products)
  - [Misc](#misc)


## Survey


## Vision-Language Models


## Image


## Video

- [Paper page - VidEgoThink: Assessing Egocentric Video Understanding Capabilities for  Embodied AI](https://huggingface.co/papers/2410.11623)
- [Paper page - OMCAT: Omni Context Aware Transformer](https://huggingface.co/papers/2410.12109)
   - [om-cat.github.io](https://om-cat.github.io/)

## Encoder


## Alignment


## Evaluation

- [Paper page - MMIE: Massive Multimodal Interleaved Comprehension Benchmark for Large  Vision-Language Models](https://huggingface.co/papers/2410.10139)
   - [mmie-bench.github.io](https://mmie-bench.github.io/)
   - [github.com](https://github.com/Lillianwei-h/MMIE)
   - [huggingface.co](https://huggingface.co/MMIE/MMIE-Score)
- [Paper page - MEGA-Bench: Scaling Multimodal Evaluation to over 500 Real-World Tasks](https://huggingface.co/papers/2410.10563)
   - [tiger-ai-lab.github.io](https://tiger-ai-lab.github.io/MEGA-Bench/)
- [Paper page - LiveXiv -- A Multi-Modal Live Benchmark Based on Arxiv Papers Content](https://huggingface.co/papers/2410.10783)
- [Paper page - TemporalBench: Benchmarking Fine-grained Temporal Understanding for  Multimodal Video Models](https://huggingface.co/papers/2410.10818)
- [Paper page - NaturalBench: Evaluating Vision-Language Models on Natural Adversarial  Samples](https://huggingface.co/papers/2410.14669)
   - [arxiv.org](https://arxiv.org/abs/2410.14669)
   - [huggingface.co](https://huggingface.co/datasets/BaiqiL/NaturalBench)
   - [linzhiqiu.github.io](https://linzhiqiu.github.io/papers/naturalbench/)
- [Paper page - WorldCuisines: A Massive-Scale Benchmark for Multilingual and  Multicultural Visual Question Answering on Global Cuisines](https://huggingface.co/papers/2410.12705)
- [Paper page - HumanEval-V: Evaluating Visual Understanding and Reasoning Abilities of  Large Multimodal Models Through Coding Tasks](https://huggingface.co/papers/2410.12381)

## Efficient


## Generation

- [Paper page - Janus: Decoupling Visual Encoding for Unified Multimodal Understanding  and Generation](https://huggingface.co/papers/2410.13848)
- [Paper page - PUMA: Empowering Unified MLLM with Multi-granular Visual Generation](https://huggingface.co/papers/2410.13861)

## Dataset

- [Paper page - Harnessing Webpage UIs for Text-Rich Visual Understanding](https://huggingface.co/papers/2410.13824)
- [Paper page - LVD-2M: A Long-take Video Dataset with Temporally Dense Captions](https://huggingface.co/papers/2410.10816)
   - [silentview.github.io](https://silentview.github.io/LVD-2M/)

## Products


## Misc


