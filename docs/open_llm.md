# Open LLM

- [Open LLM](#open-llm) 
  - [Survey](#survey)
  - [Open LLM](#open-llm-1)
  - [English Models](#english-models)
  - [Chinese Models](#chinese-models)
  - [Small Language Models](#small-language-models)
  - [Multilingual Models](#multilingual-models)
  - [Toolkits](#toolkits)
  - [Misc](#misc)


## Survey


## Open LLM


## English Models

- [Mistral-Large-Instruct-2411 is an advanced dense Large Language Model (LLM) of 123B parameters with state-of-the-art reasoning](https://huggingface.co/mistralai/Mistral-Large-Instruct-2411)  ü§ó
- üåü [Athene-V2-Chat-72B, an open-weights LLM on-par with GPT-4o across benchmarks](https://huggingface.co/Nexusflow/Athene-V2-Chat)  ü§ó 

	 ¬∑ ([nexusflow](https://nexusflow.ai/blogs/athene-v2))
- [TableGPT2-7B, a large-scale decoder specifically tailored for data-intensive tasks, with a focus on interpreting and analyzing tabular data.](https://huggingface.co/tablegpt/TableGPT2-7B)  ü§ó 

	 ¬∑ ([tablegpt-agent](https://github.com/tablegpt/tablegpt-agent) - tablegpt) ![Star](https://img.shields.io/github/stars/tablegpt/tablegpt-agent.svg?style=social&label=Star)
- [Tess-R1 is designed with test-time compute in mind, and has the capabilities to produce a Chain-of-Thought (CoT) reasoning before producing the final output.](https://huggingface.co/migtissera/Tess-R1-Limerick-Llama-3.1-70B)  ü§ó 
- [AMD-OLMo are a series of 1 billion parameter language models trained by AMD on AMD Instinct‚Ñ¢ MI250 GPUs based on OLMo.](https://huggingface.co/collections/amd/amd-olmo-6723e7d04a49116d8ec95070)  ü§ó 
- [Steiner is a series of reasoning models trained on synthetic data using reinforcement learning.](https://huggingface.co/peakji/steiner-32b-preview)  ü§ó 
- [Granite 3.0 models](https://huggingface.co/collections/ibm-granite/granite-30-models-66fdb59bbb54785c3512114f)  ü§ó 

## Chinese Models

- **Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated 
  Parameters by Tencent**, `arXiv, 2411.02265`, [arxiv](http://arxiv.org/abs/2411.02265v3), [pdf](http://arxiv.org/pdf/2411.02265v3.pdf), cication: [**-1**](None) 

	 *Xingwu Sun, Yanfeng Chen, Yiqing Huang, ..., Di Wang, Jie Jiang* ¬∑ ([huggingface](https://huggingface.co/tencent/Tencent-Hunyuan-Large)) ¬∑ ([Hunyuan-Large](https://github.com/Tencent/Hunyuan-Large) - Tencent) ![Star](https://img.shields.io/github/stars/Tencent/Hunyuan-Large.svg?style=social&label=Star)

## Small Language Models

- üåü [**smollm**](https://github.com/huggingface/smollm) - huggingface ![Star](https://img.shields.io/github/stars/huggingface/smollm.svg?style=social&label=Star)

	 *135M, 360M, and 1.7B parameters.* ¬∑ ([smollm](https://github.com/huggingface/smollm) - huggingface) ![Star](https://img.shields.io/github/stars/huggingface/smollm.svg?style=social&label=Star) ¬∑ ([ùïè](https://x.com/_philschmid/status/1859598525723488478)) ¬∑ ([huggingface](https://huggingface.co/datasets/HuggingFaceTB/smoltalk))
- [State-of-the-art compact LLMs for on-device applications: 1.7B, 360M, 135M    Upvote 107   +97](https://huggingface.co/collections/HuggingFaceTB/smollm2-6723884218bcda64b34d7db9)  ü§ó 

	 ¬∑ ([x](https://x.com/loubnabenallal1/status/1852055582494294414?s=46&t=MGz8l5Z36lvN2cHgl1IVqA))
- [SmolLM2 is a family of compact language models available in three size: 135M, 360M, and 1.7B parameters.](https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct)  ü§ó 
- [MobileLLM is an auto-regressive language model leveraging an optimized transformer architecture](https://huggingface.co/facebook/MobileLLM-125M)  ü§ó 

	 ¬∑ ([arxiv](https://arxiv.org/abs/2402.14905))

## Multilingual Models

- **Xmodel-1.5: An 1B-scale Multilingual LLM**, `arXiv, 2411.10083`, [arxiv](http://arxiv.org/abs/2411.10083v1), [pdf](http://arxiv.org/pdf/2411.10083v1.pdf), cication: [**-1**](None) 

	 *Wang Qun, Liu Yang, Lin Qingquan, ..., Jiang Ling* ¬∑ ([XmodelLM](https://github.com/XiaoduoAILab/XmodelLM) - XiaoduoAILab) ![Star](https://img.shields.io/github/stars/XiaoduoAILab/XmodelLM.svg?style=social&label=Star)
- **Bielik 7B v0.1: A Polish Language Model -- Development, Insights, and 
  Evaluation**, `arXiv, 2410.18565`, [arxiv](http://arxiv.org/abs/2410.18565v1), [pdf](http://arxiv.org/pdf/2410.18565v1.pdf), cication: [**-1**](None)

	 *Krzysztof Ociepa, ≈Åukasz Flis, Krzysztof Wr√≥bel, ..., Adrian Gwo≈∫dziej, Remigiusz Kinas* ¬∑ ([bielik](https://bielik.ai/)) ¬∑ ([huggingface](https://huggingface.co/speakleash/Bielik-11B-v2.3-Instruct))
- :clapper: [Multilinguality and LLMs Special Session](https://www.youtube.com/watch?v=aNPa00_-DbA) 
- **Pangea: A Fully Open Multilingual Multimodal LLM for 39 Languages**, `arXiv, 2410.16153`, [arxiv](http://arxiv.org/abs/2410.16153v1), [pdf](http://arxiv.org/pdf/2410.16153v1.pdf), cication: [**-1**](None) 

	 *Xiang Yue, Yueqi Song, Akari Asai, ..., Sathyanarayanan Ramamoorthy, Graham Neubig* ¬∑ ([Pangea](https://github.com/neulab/Pangea) - neulab) ![Star](https://img.shields.io/github/stars/neulab/Pangea.svg?style=social&label=Star) ¬∑ ([neulab.github](https://neulab.github.io/Pangea/)) ¬∑ ([x](https://x.com/xiangyue96/status/1848753709787795679))

## Toolkits


## Misc
## Misc
- [OLMo: Everything You Need to Train an Open Source LLM](https://twimlai.com/podcast/twimlai/olmo-everything-you-need-to-train-an-open-source-llm/) 
- [Opening the Language Model Pipeline: A Tutorial on Data Preparation, Model Training, and Adaptation](https://neurips.cc/virtual/2024/tutorial/99526) 