# Open LLM

- [Open LLM](#open-llm) 
  - [Survey](#survey)
  - [Open LLM](#open-llm-1)
  - [English Models](#english-models)
  - [Chinese Models](#chinese-models)
  - [Small Language Models](#small-language-models)
  - [Multilingual Models](#multilingual-models)
  - [Toolkits](#toolkits)
  - [Misc](#misc)


## Survey

- [**awesome-open-source-lms**](https://github.com/allenai/awesome-open-source-lms) - allenai ![Star](https://img.shields.io/github/stars/allenai/awesome-open-source-lms.svg?style=social&label=Star) 

	 · ([docs.google](https://docs.google.com/presentation/d/179dpzWSQ9G7EAUlvaJdeE0av9PLuk9Rl33nfhHSJ4xI/edit?usp=sharing))

## Open LLM

- **The Open Source Advantage in Large Language Models (LLMs)**, `arXiv, 2412.12004`, [arxiv](http://arxiv.org/abs/2412.12004v1), [pdf](http://arxiv.org/pdf/2412.12004v1.pdf), cication: [**-1**](None) 

	 *Jiya Manchanda, Laura Boettcher, Matheus Westphalen, ..., Jasser Jasser*

## English Models

- **gpt-oss-120b & gpt-oss-20b Model Card**, `arXiv, 2508.10925`, [arxiv](http://arxiv.org/abs/2508.10925v1), [pdf](http://arxiv.org/pdf/2508.10925v1.pdf), cication: [**-1**](None) 

	 *OpenAI, :, Sandhini Agarwal, ..., Eddie Zhang, Shengjia Zhao*
- 🌟 [**gpt-oss**](https://github.com/openai/gpt-oss) - openai ![Star](https://img.shields.io/github/stars/openai/gpt-oss.svg?style=social&label=Star) 
- [OpenReasoning-Nemotron-7B Overview](https://huggingface.co/nvidia/OpenReasoning-Nemotron-7B)  🤗 
- [SmolLM3: smol, multilingual, long-context reasoner](https://huggingface.co/blog/smollm3)  🤗 
- **Magistral**, `arXiv, 2506.10910`, [arxiv](http://arxiv.org/abs/2506.10910v1), [pdf](http://arxiv.org/pdf/2506.10910v1.pdf), cication: [**-1**](None) 

	 *Mistral-AI, :, Abhinav Rastogi, ..., Xuanyu Zhang, Yunhao Tang*
- [Announcing Gemma 3n preview: powerful, efficient, mobile-first AI](https://developers.googleblog.com/en/introducing-gemma-3n/) 
- **Phi-4-reasoning Technical Report**, `arXiv, 2504.21318`, [arxiv](http://arxiv.org/abs/2504.21318v1), [pdf](http://arxiv.org/pdf/2504.21318v1.pdf), cication: [**-1**](None) 

	 *Marah Abdin, Sahaj Agarwal, Ahmed Awadallah, ..., Safoora Yousefi, Guoqing Zheng*
- **Phi-4-Mini-Reasoning: Exploring the Limits of Small Reasoning Language 
  Models in Math**, `arXiv, 2504.21233`, [arxiv](http://arxiv.org/abs/2504.21233v1), [pdf](http://arxiv.org/pdf/2504.21233v1.pdf), cication: [**-1**](None) 

	 *Haoran Xu, Baolin Peng, Hany Awadalla, ..., Jianfeng Gao, Weizhu Chen*
- **Bielik v3 Small: Technical Report**, `arXiv, 2505.02550`, [arxiv](http://arxiv.org/abs/2505.02550v2), [pdf](http://arxiv.org/pdf/2505.02550v2.pdf), cication: [**-1**](None) 

	 *Krzysztof Ociepa, Łukasz Flis, Remigiusz Kinas, ..., Krzysztof Wróbel, Adrian Gwoździej*
- **Trillion 7B Technical Report**, `arXiv, 2504.15431`, [arxiv](http://arxiv.org/abs/2504.15431v1), [pdf](http://arxiv.org/pdf/2504.15431v1.pdf), cication: [**-1**](None) 

	 *Sungjun Han, Juyoung Suk, Suyeong An, ..., Seungtaek Choi, Jamin Shin*
- [Granite-3.3-8B-Instruct is a 8-billion parameter 128K context length language model fine-tuned for improved reasoning and instruction-following capabilities.](https://huggingface.co/ibm-granite/granite-3.3-8b-instruct)  🤗 
- **Gemma 3 Technical Report**, `arXiv, 2503.19786`, [arxiv](http://arxiv.org/abs/2503.19786v1), [pdf](http://arxiv.org/pdf/2503.19786v1.pdf), cication: [**-1**](None) 

	 *Gemma Team, Aishwarya Kamath, Johan Ferret, ..., Robert Dadashi, Léonard Hussenot*
- **Command A: An Enterprise-Ready Large Language Model**, `arXiv, 2504.00698`, [arxiv](http://arxiv.org/abs/2504.00698v1), [pdf](http://arxiv.org/pdf/2504.00698v1.pdf), cication: [**-1**](None) 

	 *Team Cohere, Aakanksha, Arash Ahmadian, ..., Zhenyu Zhao, Zhoujie Zhao*
- [Gemma3 - 27B, 12B, 4B & 1B - 128K context, multimodal AND multilingual!](https://x.com/reach_vb/status/1899728796586025282)  𝕏 

	 · ([huggingface](https://huggingface.co/collections/google/gemma-3-release-67c6c6f89c4f76621268bb6d)) · ([developers.googleblog](https://developers.googleblog.com/en/introducing-gemma3/))
- 🌟 [OLMo 2 32B: First fully open model to outperform GPT 3.5 and GPT 4o mini](https://allenai.org/blog/olmo2-32B) 

	 · ([huggingface](https://huggingface.co/collections/allenai/olmo-2-674117b93ab84e98afc72edc))
- [C4AI Command A is an open weights research release of a 111 billion parameter model](https://huggingface.co/CohereForAI/c4ai-command-a-03-2025)  🤗 
- [AI21 Jamba 1.6 family of models is state-of-the-art, hybrid SSM-Transformer instruction following foundation models.](https://huggingface.co/ai21labs/AI21-Jamba-Large-1.6)  🤗 
- **Phi-4-Mini Technical Report: Compact yet Powerful Multimodal Language 
  Models via Mixture-of-LoRAs**, `arXiv, 2503.01743`, [arxiv](http://arxiv.org/abs/2503.01743v2), [pdf](http://arxiv.org/pdf/2503.01743v2.pdf), cication: [**-1**](None) 

	 *Microsoft, :, Abdelrahman Abouelenin, ..., Yunan Zhang, Xiren Zhou* · ([huggingface](https://huggingface.co/microsoft/Phi-4-multimodal-instruct)) · ([huggingface](https://huggingface.co/microsoft/Phi-4-mini-instruct)) · ([azure.microsoft](https://azure.microsoft.com/en-us/blog/empowering-innovation-the-next-generation-of-the-phi-family/))
- [Phi-4-mini-instruct is a lightweight open model built upon synthetic data and filtered publicly available websites](https://huggingface.co/microsoft/Phi-4-mini-instruct)  🤗 
- [Dolphin 3.0 R1 is the next generation of the Dolphin series of instruct-tuned models.](https://huggingface.co/cognitivecomputations/Dolphin3.0-R1-Mistral-24B)  🤗 
- 🌟 **SmolLM2: When Smol Goes Big -- Data-Centric Training of a Small Language 
  Model**, `arXiv, 2502.02737`, [arxiv](http://arxiv.org/abs/2502.02737v1), [pdf](http://arxiv.org/pdf/2502.02737v1.pdf), cication: [**-1**](None) 

	 *Loubna Ben Allal, Anton Lozhkov, Elie Bakouch, ..., Leandro von Werra, Thomas Wolf* · ([huggingface](https://huggingface.co/collections/HuggingFaceTB/smollm2-6723884218bcda64b34d7db9)) · ([𝕏](https://x.com/LoubnaBenAllal1/status/1887500167055560922))
- [Mistral Small 3](https://mistral.ai/news/mistral-small-3/) 
- [Llama-3.1-Tulu-3-405B](https://huggingface.co/allenai/Llama-3.1-Tulu-3-405B)  🤗 
- [Virtuoso-Medium-v2 (32B) is our next-generation, 32-billion-parameter language model that builds upon the original Virtuoso-Medium architecture.](https://huggingface.co/arcee-ai/Virtuoso-Medium-v2)  🤗 
- [Helium-1 preview is a lightweight language model with 2B parameters, targeting edge and mobile devices.](https://huggingface.co/kyutai/helium-1-preview-2b)  🤗 
- [Dolphin 3.0 is the next generation of the Dolphin series of instruct-tuned models](https://huggingface.co/cognitivecomputations/Dolphin3.0-Llama3.1-8B)  🤗 
- **2 OLMo 2 Furious**, `arXiv, 2501.00656`, [arxiv](http://arxiv.org/abs/2501.00656v1), [pdf](http://arxiv.org/pdf/2501.00656v1.pdf), cication: [**-1**](None) 

	 *Team OLMo, Pete Walsh, Luca Soldaini, ..., Noah A. Smith, Hannaneh Hajishirzi* · ([𝕏](https://x.com/kylelostat/status/1875209445950804114)) · ([t](https://t.co/JR0tKbBgWz))
- [SmolLM2 1.7b Instruction Tuned & DPO Aligned through Tulu 3](https://huggingface.co/SultanR/SmolTulu-1.7b-Instruct)  🤗 
- [Granite 3.1 Language Models](https://huggingface.co/collections/ibm-granite/granite-31-language-models-6751dbbf2f3389bec5c6f02d)  🤗 
- [Hermes 3 3B is a full parameter fine-tune of the Llama-3.2 3B foundation model](https://huggingface.co/NousResearch/Hermes-3-Llama-3.2-3B)  🤗 
- [Introducing Phi-4: Microsoft’s Newest Small Language Model Specializing in Complex Reasoning](https://techcommunity.microsoft.com/blog/aiplatformblog/introducing-phi-4-microsoft%e2%80%99s-newest-small-language-model-specializing-in-comple/4357090) 
- **Phi-4 Technical Report**, `arXiv, 2412.08905`, [arxiv](http://arxiv.org/abs/2412.08905v1), [pdf](http://arxiv.org/pdf/2412.08905v1.pdf), cication: [**-1**](None) 

	 *Marah Abdin, Jyoti Aneja, Harkirat Behl, ..., Cyril Zhang, Yi Zhang*
- [C4AI Command R7B is an open weights research release of a 7B billion parameter model](https://huggingface.co/CohereForAI/c4ai-command-r7b-12-2024)  🤗 
- 🌟 **EXAONE 3.5: Series of Large Language Models for Real-world Use Cases**, `arXiv, 2412.04862`, [arxiv](http://arxiv.org/abs/2412.04862v2), [pdf](http://arxiv.org/pdf/2412.04862v2.pdf), cication: [**-1**](None) 

	 *LG AI Research, Soyoung An, Kyunghoon Bae, ..., Heuiyeen Yeen, Hyeongu Yun* · ([huggingface](https://huggingface.co/LGAI-EXAON))
- [Deepthought-8B is a small and capable reasoning model built on LLaMA-3.1 8B, designed to make AI reasoning more transparent and controllable.](https://huggingface.co/ruliad/deepthought-8b-llama-v0.01-alpha)  🤗 
- **Fully Open Source Moxin-7B Technical Report**, `arXiv, 2412.06845`, [arxiv](http://arxiv.org/abs/2412.06845v1), [pdf](http://arxiv.org/pdf/2412.06845v1.pdf), cication: [**-1**](None) 

	 *Pu Zhao, Xuan Shen, Zhenglun Kong, ..., Yong He, Yanzhi Wang* · ([Moxin-LLM](https://github.com/moxin-org/Moxin-LLM) - moxin-org) ![Star](https://img.shields.io/github/stars/moxin-org/Moxin-LLM.svg?style=social&label=Star)
- [A replication attempt of Tulu 3 on the Qwen 2.5 base models.](https://huggingface.co/allura-org/Teleut-7b)  🤗 
- 🌟 [OLMo 2: The best fully open language model to date](https://allenai.org/blog/olmo2) 

	 · ([𝕏](https://x.com/natolambert/status/1861511563125096593))
- [INTELLECT-1 is the world's first decentralized training of a 10B parameter model, enabling anyone to contribute compute and participate.](https://app.primeintellect.ai/intelligence) 
- [Mistral-Large-Instruct-2411 is an advanced dense Large Language Model (LLM) of 123B parameters with state-of-the-art reasoning](https://huggingface.co/mistralai/Mistral-Large-Instruct-2411)  🤗 
- 🌟 [Athene-V2-Chat-72B, an open-weights LLM on-par with GPT-4o across benchmarks](https://huggingface.co/Nexusflow/Athene-V2-Chat)  🤗 

	 · ([nexusflow](https://nexusflow.ai/blogs/athene-v2))
- [TableGPT2-7B, a large-scale decoder specifically tailored for data-intensive tasks, with a focus on interpreting and analyzing tabular data.](https://huggingface.co/tablegpt/TableGPT2-7B)  🤗 

	 · ([tablegpt-agent](https://github.com/tablegpt/tablegpt-agent) - tablegpt) ![Star](https://img.shields.io/github/stars/tablegpt/tablegpt-agent.svg?style=social&label=Star)
- [Tess-R1 is designed with test-time compute in mind, and has the capabilities to produce a Chain-of-Thought (CoT) reasoning before producing the final output.](https://huggingface.co/migtissera/Tess-R1-Limerick-Llama-3.1-70B)  🤗 
- [AMD-OLMo are a series of 1 billion parameter language models trained by AMD on AMD Instinct™ MI250 GPUs based on OLMo.](https://huggingface.co/collections/amd/amd-olmo-6723e7d04a49116d8ec95070)  🤗 
- [Steiner is a series of reasoning models trained on synthetic data using reinforcement learning.](https://huggingface.co/peakji/steiner-32b-preview)  🤗 
- [Granite 3.0 models](https://huggingface.co/collections/ibm-granite/granite-30-models-66fdb59bbb54785c3512114f)  🤗 

## Chinese Models

- [Seed-OSS Open-Source Models](https://huggingface.co/ByteDance-Seed/Seed-OSS-36B-Instruct)  🤗 
- **LongCat-Flash Technical Report**, `arXiv, 2509.01322`, [arxiv](http://arxiv.org/abs/2509.01322v1), [pdf](http://arxiv.org/pdf/2509.01322v1.pdf), cication: [**-1**](None) 

	 *Meituan LongCat Team, Bayan, Bei Li, ..., Zongyu Wang, Zunhai Su*
- 🌟 **GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models**, `arXiv, 2508.06471`, [arxiv](http://arxiv.org/abs/2508.06471v1), [pdf](http://arxiv.org/pdf/2508.06471v1.pdf), cication: [**-1**](None) 

	 *GLM-4. 5 Team, :, Aohan Zeng, ..., Yuxiao Dong, Jie Tang*
- [**Step3**](https://github.com/stepfun-ai/Step3) - stepfun-ai ![Star](https://img.shields.io/github/stars/stepfun-ai/Step3.svg?style=social&label=Star) 
- [**Hunyuan-7B**](https://github.com/Tencent-Hunyuan/Hunyuan-7B) - Tencent-Hunyuan ![Star](https://img.shields.io/github/stars/Tencent-Hunyuan/Hunyuan-7B.svg?style=social&label=Star) 
- [**Qwen3**](https://github.com/QwenLM/Qwen3) - QwenLM ![Star](https://img.shields.io/github/stars/QwenLM/Qwen3.svg?style=social&label=Star) 
- [**Kimi-K2**](https://github.com/MoonshotAI/Kimi-K2) - MoonshotAI ![Star](https://img.shields.io/github/stars/MoonshotAI/Kimi-K2.svg?style=social&label=Star) 
- **MiniCPM4: Ultra-Efficient LLMs on End Devices**, `arXiv, 2506.07900`, [arxiv](http://arxiv.org/abs/2506.07900v1), [pdf](http://arxiv.org/pdf/2506.07900v1.pdf), cication: [**-1**](None) 

	 *MiniCPM Team, Chaojun Xiao, Yuxuan Li, ..., Dahai Li, Maosong Sun*
- [**MiniMax-M1**](https://github.com/MiniMax-AI/MiniMax-M1) - MiniMax-AI ![Star](https://img.shields.io/github/stars/MiniMax-AI/MiniMax-M1.svg?style=social&label=Star) 
- [**MiniCPM**](https://github.com/OpenBMB/MiniCPM) - OpenBMB ![Star](https://img.shields.io/github/stars/OpenBMB/MiniCPM.svg?style=social&label=Star) 

	 *Ultra-Efficient LLMs on End Devices, achieving 5+ speedup on typical end-side chips          Resources*
- [**dots.llm1**](https://github.com/rednote-hilab/dots.llm1) - rednote-hilab ![Star](https://img.shields.io/github/stars/rednote-hilab/dots.llm1.svg?style=social&label=Star) 
- [Qwen3: Think Deeper, Act Faster](https://qwenlm.github.io/blog/qwen3/) 

	 · ([Qwen3](https://github.com/QwenLM/Qwen3) - QwenLM) ![Star](https://img.shields.io/github/stars/QwenLM/Qwen3.svg?style=social&label=Star)
- [MAI-DS-R1 is a DeepSeek-R1 reasoning model that has been post-trained by Microsoft AI team to fill in information gaps in the previous version of the model and to improve its risk profile, while maintaining R1 reasoning capabilities.](https://huggingface.co/microsoft/MAI-DS-R1)  🤗 
- [DeepSeek-V3-0324 demonstrates notable improvements over its predecessor, DeepSeek-V3, in several key aspects.](https://huggingface.co/deepseek-ai/DeepSeek-V3-0324)  🤗 
- [R1 1776 is a DeepSeek-R1 reasoning model that has been post-trained by Perplexity AI to remove Chinese Communist Party censorship.](https://huggingface.co/perplexity-ai/r1-1776)  🤗 

	 · ([perplexity](https://perplexity.ai/hub/blog/open-sourcing-r1-1776)) · ([𝕏](https://x.com/perplexity_ai/status/1891916573713236248))
- **Qwen2.5-1M Technical Report**, `arXiv, 2501.15383`, [arxiv](http://arxiv.org/abs/2501.15383v1), [pdf](http://arxiv.org/pdf/2501.15383v1.pdf), cication: [**-1**](None) 

	 *An Yang, Bowen Yu, Chengyuan Li, ..., Zhiying Xu, Zipeng Zhang* · ([qwenlm.github](https://qwenlm.github.io/blog/qwen2.5-1m/))
- [Qwen2.5-1M: Deploy Your Own Qwen with Context Length up to 1M Tokens](https://qwenlm.github.io/blog/qwen2.5-1m/) 
- **LLM360 K2: Building a 65B 360-Open-Source Large Language Model from 
  Scratch**, `arXiv, 2501.07124`, [arxiv](http://arxiv.org/abs/2501.07124v3), [pdf](http://arxiv.org/pdf/2501.07124v3.pdf), cication: [**-1**](None) 

	 *Zhengzhong Liu, Bowen Tan, Hongyi Wang, ..., Preslav Nakov, Eric Xing*
- **MiniMax-01: Scaling Foundation Models with Lightning Attention**, `arXiv, 2501.08313`, [arxiv](http://arxiv.org/abs/2501.08313v1), [pdf](http://arxiv.org/pdf/2501.08313v1.pdf), cication: [**-1**](None) 

	 *MiniMax, Aonian Li, Bangwei Gong, ..., Zhuo Jiang, Zijia Wu*
- [github.com](https://github.com/MiniMax-AI)
- [InternLM3 has open-sourced an 8-billion parameter instruction model, InternLM3-8B-Instruct, designed for general-purpose usage and advanced reasoning.](https://huggingface.co/internlm/internlm3-8b-instruct)  🤗 
- [Diving into MiniMax01 405B MoE](https://huggingface.co/blog/eliebak/minimax01-deepdive)  🤗 

	 · ([𝕏](https://x.com/eliebakouch/status/1879458716455759908))
- **Xmodel-2 Technical Report**, `arXiv, 2412.19638`, [arxiv](http://arxiv.org/abs/2412.19638v1), [pdf](http://arxiv.org/pdf/2412.19638v1.pdf), cication: [**-1**](None) 

	 *Wang Qun, Liu Yang, Lin Qingquan, ..., Qu Zhijiu, Jiang Ling* · ([Xmodel-2](https://github.com/XiaoduoAILab/Xmodel-2) - XiaoduoAILab) ![Star](https://img.shields.io/github/stars/XiaoduoAILab/Xmodel-2.svg?style=social&label=Star)
- 🌟 **YuLan-Mini: An Open Data-efficient Language Model**, `arXiv, 2412.17743`, [arxiv](http://arxiv.org/abs/2412.17743v2), [pdf](http://arxiv.org/pdf/2412.17743v2.pdf), cication: [**-1**](None) 

	 *Yiwen Hu, Huatong Song, Jia Deng, ..., Wayne Xin Zhao, Ji-Rong Wen* · ([YuLan-Mini](https://github.com/RUC-GSAI/YuLan-Mini) - RUC-GSAI) ![Star](https://img.shields.io/github/stars/RUC-GSAI/YuLan-Mini.svg?style=social&label=Star)
- 🌟 **DeepSeek-V3 Technical Report**, `arXiv, 2412.19437`, [arxiv](http://arxiv.org/abs/2412.19437v1), [pdf](http://arxiv.org/pdf/2412.19437v1.pdf), cication: [**-1**](None) 

	 *DeepSeek-AI, Aixin Liu, Bei Feng, ..., Ziyi Gao, Zizheng Pan* · ([DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) - deepseek-ai) ![Star](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3.svg?style=social&label=Star)
- [deepseek-ai  / DeepSeek-V3-Base](https://huggingface.co/deepseek-ai/DeepSeek-V3-Base/tree/main)  🤗 
- 🌟 [**DeepSeek-V3**](https://github.com/deepseek-ai/DeepSeek-V3) - deepseek-ai ![Star](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3.svg?style=social&label=Star) 
- **Qwen2.5 Technical Report**, `arXiv, 2412.15115`, [arxiv](http://arxiv.org/abs/2412.15115v1), [pdf](http://arxiv.org/pdf/2412.15115v1.pdf), cication: [**-1**](None) 

	 *Qwen, :, An Yang, ..., Zhenru Zhang, Zihan Qiu*
- [T-lite-it-1.0 is a model built upon the Qwen 2.5 model family and incorporates both continual pre-training and alignment techniques.](https://huggingface.co/t-tech/T-lite-it-1.0)  🤗 
- [DeepSeek-V2.5-1210 is an upgraded version of DeepSeek-V2.5, with improvements across various capabilities:](https://huggingface.co/deepseek-ai/DeepSeek-V2.5-1210)  🤗 
- **BayLing 2: A Multilingual Large Language Model with Efficient Language 
  Alignment**, `arXiv, 2411.16300`, [arxiv](http://arxiv.org/abs/2411.16300v1), [pdf](http://arxiv.org/pdf/2411.16300v1.pdf), cication: [**-1**](None) 

	 *Shaolei Zhang, Kehao Zhang, Qingkai Fang, ..., Xiaodong Liu, Yang Feng* · ([BayLing](https://github.com/ictnlp/BayLing) - ictnlp) ![Star](https://img.shields.io/github/stars/ictnlp/BayLing.svg?style=social&label=Star)
- **Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated 
  Parameters by Tencent**, `arXiv, 2411.02265`, [arxiv](http://arxiv.org/abs/2411.02265v3), [pdf](http://arxiv.org/pdf/2411.02265v3.pdf), cication: [**-1**](None) 

	 *Xingwu Sun, Yanfeng Chen, Yiqing Huang, ..., Di Wang, Jie Jiang* · ([huggingface](https://huggingface.co/tencent/Tencent-Hunyuan-Large)) · ([Hunyuan-Large](https://github.com/Tencent/Hunyuan-Large) - Tencent) ![Star](https://img.shields.io/github/stars/Tencent/Hunyuan-Large.svg?style=social&label=Star)

## Reasoning

- [**Skywork-OR1**](https://github.com/SkyworkAI/Skywork-OR1) - SkyworkAI ![Star](https://img.shields.io/github/stars/SkyworkAI/Skywork-OR1.svg?style=social&label=Star) 

	 · ([capricious-hydrogen-41c.notion](https://capricious-hydrogen-41c.notion.site/Skywork-Open-Reaonser-Series-1d0bc9ae823a80459b46c149e4f51680))
- [Llama-3.3-Nemotron-Super-49B-v1 is a large language model (LLM) which is a derivative of Meta Llama-3.3-70B-Instruct (AKA the reference model).](https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1)  🤗 
- [**QwQ**](https://github.com/QwenLM/QwQ) - QwenLM ![Star](https://img.shields.io/github/stars/QwenLM/QwQ.svg?style=social&label=Star) 

	 · ([huggingface](https://huggingface.co/Qwen/QwQ-32B))
- [Reka Flash 3 is a 21B general-purpose reasoning model that was trained from scratch.](https://huggingface.co/RekaAI/reka-flash-3)  🤗 
- [Tiny-R1-32B-Preview, which outperforms the 70B model Deepseek-R1-Distill-Llama-70B and nearly matches the full R1](https://huggingface.co/qihoo360/TinyR1-32B-Preview)  🤗 
- 🌟 [QwQ-32B: Embracing the Power of Reinforcement Learning](https://qwenlm.github.io/blog/qwq-32b/) 
- [Mistral-Small-Reasoning](https://huggingface.co/yentinglin/Mistral-Small-24B-Instruct-2501-reasoning)  🤗 
- 🌟 [**OpenSeek**](https://github.com/FlagAI-Open/OpenSeek) - FlagAI-Open ![Star](https://img.shields.io/github/stars/FlagAI-Open/OpenSeek.svg?style=social&label=Star) 
- [DeepHermes 3 Preview is the latest version of our flagship Hermes series of LLMs by Nous Research](https://huggingface.co/NousResearch/DeepHermes-3-Llama-3-8B-Preview)  🤗 
- [This is a 32B reasoning model trained from Qwen2.5-32B-Instruct with 17K data.](https://huggingface.co/NovaSky-AI/Sky-T1-32B-Preview)  🤗 

	 · ([𝕏](https://x.com/NovaSkyAI/status/1877793041957933347))
- [Bespoke-Stratos-32B, our reasoning model distilled from DeepSeek-R1 using Berkeley NovaSky’s Sky-T1 recipe.](https://x.com/madiator/status/1882131706515525919)  𝕏 
- [OpenThinker-32B](https://huggingface.co/open-thoughts/OpenThinker-32B)  🤗 

	 · ([huggingface](https://huggingface.co/datasets/open-thoughts/OpenThoughts-114k)) · ([LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory) - hiyouga) ![Star](https://img.shields.io/github/stars/hiyouga/LLaMA-Factory.svg?style=social&label=Star)
## Small Language Models

- [Hybrid Small Language Models.](https://huggingface.co/collections/nvidia/hymba-673c35516c12c4b98b5e845f)  🤗 

	 · ([𝕏](https://x.com/reach_vb/status/1861492854293459423))
- 🌟 [**smollm**](https://github.com/huggingface/smollm) - huggingface ![Star](https://img.shields.io/github/stars/huggingface/smollm.svg?style=social&label=Star) 

	 *135M, 360M, and 1.7B parameters.* · ([smollm](https://github.com/huggingface/smollm) - huggingface) ![Star](https://img.shields.io/github/stars/huggingface/smollm.svg?style=social&label=Star) · ([𝕏](https://x.com/_philschmid/status/1859598525723488478)) · ([huggingface](https://huggingface.co/datasets/HuggingFaceTB/smoltalk))
- [State-of-the-art compact LLMs for on-device applications: 1.7B, 360M, 135M    Upvote 107   +97](https://huggingface.co/collections/HuggingFaceTB/smollm2-6723884218bcda64b34d7db9)  🤗 

	 · ([x](https://x.com/loubnabenallal1/status/1852055582494294414?s=46&t=MGz8l5Z36lvN2cHgl1IVqA))
- [SmolLM2 is a family of compact language models available in three size: 135M, 360M, and 1.7B parameters.](https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct)  🤗 
- [MobileLLM is an auto-regressive language model leveraging an optimized transformer architecture](https://huggingface.co/facebook/MobileLLM-125M)  🤗 

	 · ([arxiv](https://arxiv.org/abs/2402.14905))

## Multilingual Models

- **Babel: Open Multilingual Large Language Models Serving Over 90% of 
  Global Speakers**, `arXiv, 2503.00865`, [arxiv](http://arxiv.org/abs/2503.00865v1), [pdf](http://arxiv.org/pdf/2503.00865v1.pdf), cication: [**-1**](None) 

	 *Yiran Zhao, Chaoqun Liu, Yue Deng, ..., Deli Zhao, Wenxuan Zhang*
- **Kanana: Compute-efficient Bilingual Language Models**, `arXiv, 2502.18934`, [arxiv](http://arxiv.org/abs/2502.18934v3), [pdf](http://arxiv.org/pdf/2502.18934v3.pdf), cication: [**-1**](None) 

	 *Kanana LLM Team, Yunju Bak, Hojin Lee, ..., Shinbok Lee, Gaeun Seo* · ([huggingface](https://huggingface.co/collections/kakaocorp/kanana-nano-21b-67a326cda1c449c8d4172259)) · ([kanana](https://github.com/kakao/kanana) - kakao) ![Star](https://img.shields.io/github/stars/kakao/kanana.svg?style=social&label=Star)
- **Sailor2: Sailing in South-East Asia with Inclusive Multilingual LLMs**, `arXiv, 2502.12982`, [arxiv](http://arxiv.org/abs/2502.12982v1), [pdf](http://arxiv.org/pdf/2502.12982v1.pdf), cication: [**-1**](None) 

	 *Longxu Dou, Qian Liu, Fan Zhou, ..., Wei Lu, Min Lin*
- **Marco-LLM: Bridging Languages via Massive Multilingual Training for 
  Cross-Lingual Enhancement**, `arXiv, 2412.04003`, [arxiv](http://arxiv.org/abs/2412.04003v1), [pdf](http://arxiv.org/pdf/2412.04003v1.pdf), cication: [**-1**](None) 

	 *Lingfeng Ming, Bo Zeng, Chenyang Lyu, ..., Weihua Luo, Kaifu Zhang*
- [Sailor2: Sailing in South-East Asia with Inclusive Multilingual LLMs](http://sea-sailor.github.io/blog/sailor2/) 

	 · ([𝕏](https://x.com/sivil_taram/status/1863989432170484008)) · ([sailor2](https://github.com/sail-sg/sailor2) - sail-sg) ![Star](https://img.shields.io/github/stars/sail-sg/sailor2.svg?style=social&label=Star)
- **Xmodel-1.5: An 1B-scale Multilingual LLM**, `arXiv, 2411.10083`, [arxiv](http://arxiv.org/abs/2411.10083v1), [pdf](http://arxiv.org/pdf/2411.10083v1.pdf), cication: [**-1**](None) 

	 *Wang Qun, Liu Yang, Lin Qingquan, ..., Jiang Ling* · ([XmodelLM](https://github.com/XiaoduoAILab/XmodelLM) - XiaoduoAILab) ![Star](https://img.shields.io/github/stars/XiaoduoAILab/XmodelLM.svg?style=social&label=Star)
- **Bielik 7B v0.1: A Polish Language Model -- Development, Insights, and 
  Evaluation**, `arXiv, 2410.18565`, [arxiv](http://arxiv.org/abs/2410.18565v1), [pdf](http://arxiv.org/pdf/2410.18565v1.pdf), cication: [**-1**](None)

	 *Krzysztof Ociepa, Łukasz Flis, Krzysztof Wróbel, ..., Adrian Gwoździej, Remigiusz Kinas* · ([bielik](https://bielik.ai/)) · ([huggingface](https://huggingface.co/speakleash/Bielik-11B-v2.3-Instruct))
- :clapper: [Multilinguality and LLMs Special Session](https://www.youtube.com/watch?v=aNPa00_-DbA) 
- **Pangea: A Fully Open Multilingual Multimodal LLM for 39 Languages**, `arXiv, 2410.16153`, [arxiv](http://arxiv.org/abs/2410.16153v1), [pdf](http://arxiv.org/pdf/2410.16153v1.pdf), cication: [**-1**](None) 

	 *Xiang Yue, Yueqi Song, Akari Asai, ..., Sathyanarayanan Ramamoorthy, Graham Neubig* · ([Pangea](https://github.com/neulab/Pangea) - neulab) ![Star](https://img.shields.io/github/stars/neulab/Pangea.svg?style=social&label=Star) · ([neulab.github](https://neulab.github.io/Pangea/)) · ([x](https://x.com/xiangyue96/status/1848753709787795679))

## Toolkits


## Misc

- [Models](https://app.hyperbolic.xyz/models/deepseek-r1) 
- [OLMo leads on the secrets of training language models (w Dirk Groeneveld, Kyle Lo, & Luca Soldaini)](https://www.youtube.com/watch?v=dS7QI99uJVc)  :clapper: 
- [Phi-4 bug fixes](https://x.com/danielhanchen/status/1877781452818968615)  𝕏 