# Open LLM

- [Open LLM](#open-llm) 
  - [Survey](#survey)
  - [Open LLM](#open-llm-1)
  - [English Models](#english-models)
  - [Chinese Models](#chinese-models)
  - [Small Language Models](#small-language-models)
  - [Multilingual Models](#multilingual-models)
  - [Toolkits](#toolkits)
  - [Misc](#misc)


## Survey


## Open LLM


## English Models

- [A replication attempt of Tulu 3 on the Qwen 2.5 base models.](https://huggingface.co/allura-org/Teleut-7b)  ğŸ¤—
- ğŸŒŸ [OLMo 2: The best fully open language model to date](https://allenai.org/blog/olmo2) 

	 Â· ([ğ•](https://x.com/natolambert/status/1861511563125096593))
- [INTELLECT-1 is the world's first decentralized training of a 10B parameter model, enabling anyone to contribute compute and participate.](https://app.primeintellect.ai/intelligence) 
- [Mistral-Large-Instruct-2411 is an advanced dense Large Language Model (LLM) of 123B parameters with state-of-the-art reasoning](https://huggingface.co/mistralai/Mistral-Large-Instruct-2411)  ğŸ¤— 
- ğŸŒŸ [Athene-V2-Chat-72B, an open-weights LLM on-par with GPT-4o across benchmarks](https://huggingface.co/Nexusflow/Athene-V2-Chat)  ğŸ¤— 

	 Â· ([nexusflow](https://nexusflow.ai/blogs/athene-v2))
- [TableGPT2-7B, a large-scale decoder specifically tailored for data-intensive tasks, with a focus on interpreting and analyzing tabular data.](https://huggingface.co/tablegpt/TableGPT2-7B)  ğŸ¤— 

	 Â· ([tablegpt-agent](https://github.com/tablegpt/tablegpt-agent) - tablegpt) ![Star](https://img.shields.io/github/stars/tablegpt/tablegpt-agent.svg?style=social&label=Star)
- [Tess-R1 is designed with test-time compute in mind, and has the capabilities to produce a Chain-of-Thought (CoT) reasoning before producing the final output.](https://huggingface.co/migtissera/Tess-R1-Limerick-Llama-3.1-70B)  ğŸ¤— 
- [AMD-OLMo are a series of 1 billion parameter language models trained by AMD on AMD Instinctâ„¢ MI250 GPUs based on OLMo.](https://huggingface.co/collections/amd/amd-olmo-6723e7d04a49116d8ec95070)  ğŸ¤— 
- [Steiner is a series of reasoning models trained on synthetic data using reinforcement learning.](https://huggingface.co/peakji/steiner-32b-preview)  ğŸ¤— 
- [Granite 3.0 models](https://huggingface.co/collections/ibm-granite/granite-30-models-66fdb59bbb54785c3512114f)  ğŸ¤— 

## Chinese Models

- **BayLing 2: A Multilingual Large Language Model with Efficient Language
  Alignment**, `arXiv, 2411.16300`, [arxiv](http://arxiv.org/abs/2411.16300v1), [pdf](http://arxiv.org/pdf/2411.16300v1.pdf), cication: [**-1**](None) 

	 *Shaolei Zhang, Kehao Zhang, Qingkai Fang, ..., Xiaodong Liu, Yang Feng* Â· ([BayLing](https://github.com/ictnlp/BayLing) - ictnlp) ![Star](https://img.shields.io/github/stars/ictnlp/BayLing.svg?style=social&label=Star)
- **Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated 
  Parameters by Tencent**, `arXiv, 2411.02265`, [arxiv](http://arxiv.org/abs/2411.02265v3), [pdf](http://arxiv.org/pdf/2411.02265v3.pdf), cication: [**-1**](None) 

	 *Xingwu Sun, Yanfeng Chen, Yiqing Huang, ..., Di Wang, Jie Jiang* Â· ([huggingface](https://huggingface.co/tencent/Tencent-Hunyuan-Large)) Â· ([Hunyuan-Large](https://github.com/Tencent/Hunyuan-Large) - Tencent) ![Star](https://img.shields.io/github/stars/Tencent/Hunyuan-Large.svg?style=social&label=Star)

## Small Language Models

- [Hybrid Small Language Models.](https://huggingface.co/collections/nvidia/hymba-673c35516c12c4b98b5e845f)  ğŸ¤—

	 Â· ([ğ•](https://x.com/reach_vb/status/1861492854293459423))
- ğŸŒŸ [**smollm**](https://github.com/huggingface/smollm) - huggingface ![Star](https://img.shields.io/github/stars/huggingface/smollm.svg?style=social&label=Star) 

	 *135M, 360M, and 1.7B parameters.* Â· ([smollm](https://github.com/huggingface/smollm) - huggingface) ![Star](https://img.shields.io/github/stars/huggingface/smollm.svg?style=social&label=Star) Â· ([ğ•](https://x.com/_philschmid/status/1859598525723488478)) Â· ([huggingface](https://huggingface.co/datasets/HuggingFaceTB/smoltalk))
- [State-of-the-art compact LLMs for on-device applications: 1.7B, 360M, 135M    Upvote 107   +97](https://huggingface.co/collections/HuggingFaceTB/smollm2-6723884218bcda64b34d7db9)  ğŸ¤— 

	 Â· ([x](https://x.com/loubnabenallal1/status/1852055582494294414?s=46&t=MGz8l5Z36lvN2cHgl1IVqA))
- [SmolLM2 is a family of compact language models available in three size: 135M, 360M, and 1.7B parameters.](https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct)  ğŸ¤— 
- [MobileLLM is an auto-regressive language model leveraging an optimized transformer architecture](https://huggingface.co/facebook/MobileLLM-125M)  ğŸ¤— 

	 Â· ([arxiv](https://arxiv.org/abs/2402.14905))

## Multilingual Models

- **Xmodel-1.5: An 1B-scale Multilingual LLM**, `arXiv, 2411.10083`, [arxiv](http://arxiv.org/abs/2411.10083v1), [pdf](http://arxiv.org/pdf/2411.10083v1.pdf), cication: [**-1**](None) 

	 *Wang Qun, Liu Yang, Lin Qingquan, ..., Jiang Ling* Â· ([XmodelLM](https://github.com/XiaoduoAILab/XmodelLM) - XiaoduoAILab) ![Star](https://img.shields.io/github/stars/XiaoduoAILab/XmodelLM.svg?style=social&label=Star)
- **Bielik 7B v0.1: A Polish Language Model -- Development, Insights, and 
  Evaluation**, `arXiv, 2410.18565`, [arxiv](http://arxiv.org/abs/2410.18565v1), [pdf](http://arxiv.org/pdf/2410.18565v1.pdf), cication: [**-1**](None)

	 *Krzysztof Ociepa, Åukasz Flis, Krzysztof WrÃ³bel, ..., Adrian GwoÅºdziej, Remigiusz Kinas* Â· ([bielik](https://bielik.ai/)) Â· ([huggingface](https://huggingface.co/speakleash/Bielik-11B-v2.3-Instruct))
- :clapper: [Multilinguality and LLMs Special Session](https://www.youtube.com/watch?v=aNPa00_-DbA) 
- **Pangea: A Fully Open Multilingual Multimodal LLM for 39 Languages**, `arXiv, 2410.16153`, [arxiv](http://arxiv.org/abs/2410.16153v1), [pdf](http://arxiv.org/pdf/2410.16153v1.pdf), cication: [**-1**](None) 

	 *Xiang Yue, Yueqi Song, Akari Asai, ..., Sathyanarayanan Ramamoorthy, Graham Neubig* Â· ([Pangea](https://github.com/neulab/Pangea) - neulab) ![Star](https://img.shields.io/github/stars/neulab/Pangea.svg?style=social&label=Star) Â· ([neulab.github](https://neulab.github.io/Pangea/)) Â· ([x](https://x.com/xiangyue96/status/1848753709787795679))

## Toolkits


## Misc
## Misc
- [OLMo: Everything You Need to Train an Open Source LLM](https://twimlai.com/podcast/twimlai/olmo-everything-you-need-to-train-an-open-source-llm/) 
- [Opening the Language Model Pipeline: A Tutorial on Data Preparation, Model Training, and Adaptation](https://neurips.cc/virtual/2024/tutorial/99526) 