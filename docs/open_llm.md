# Open LLM

- [Open LLM](#open-llm) 
  - [Survey](#survey)
  - [Open LLM](#open-llm-1)
  - [English Models](#english-models)
  - [Chinese Models](#chinese-models)
  - [Small Language Models](#small-language-models)
  - [Multilingual Models](#multilingual-models)
  - [Toolkits](#toolkits)
  - [Misc](#misc)


## Survey

- [**awesome-open-source-lms**](https://github.com/allenai/awesome-open-source-lms) - allenai ![Star](https://img.shields.io/github/stars/allenai/awesome-open-source-lms.svg?style=social&label=Star) 

	 ¬∑ ([docs.google](https://docs.google.com/presentation/d/179dpzWSQ9G7EAUlvaJdeE0av9PLuk9Rl33nfhHSJ4xI/edit?usp=sharing))

## Open LLM

- **The Open Source Advantage in Large Language Models (LLMs)**, `arXiv, 2412.12004`, [arxiv](http://arxiv.org/abs/2412.12004v1), [pdf](http://arxiv.org/pdf/2412.12004v1.pdf), cication: [**-1**](None) 

	 *Jiya Manchanda, Laura Boettcher, Matheus Westphalen, ..., Jasser Jasser*

## English Models

- [Dolphin 3.0 R1 is the next generation of the Dolphin series of instruct-tuned models.](https://huggingface.co/cognitivecomputations/Dolphin3.0-R1-Mistral-24B)  ü§ó 
- üåü **SmolLM2: When Smol Goes Big -- Data-Centric Training of a Small Language 
  Model**, `arXiv, 2502.02737`, [arxiv](http://arxiv.org/abs/2502.02737v1), [pdf](http://arxiv.org/pdf/2502.02737v1.pdf), cication: [**-1**](None) 

	 *Loubna Ben Allal, Anton Lozhkov, Elie Bakouch, ..., Leandro von Werra, Thomas Wolf* ¬∑ ([huggingface](https://huggingface.co/collections/HuggingFaceTB/smollm2-6723884218bcda64b34d7db9)) ¬∑ ([ùïè](https://x.com/LoubnaBenAllal1/status/1887500167055560922))
- [Mistral Small 3](https://mistral.ai/news/mistral-small-3/) 
- [Llama-3.1-Tulu-3-405B](https://huggingface.co/allenai/Llama-3.1-Tulu-3-405B)  ü§ó 
- [Virtuoso-Medium-v2 (32B) is our next-generation, 32-billion-parameter language model that builds upon the original Virtuoso-Medium architecture.](https://huggingface.co/arcee-ai/Virtuoso-Medium-v2)  ü§ó 
- [Helium-1 preview is a lightweight language model with 2B parameters, targeting edge and mobile devices.](https://huggingface.co/kyutai/helium-1-preview-2b)  ü§ó 
- [Dolphin 3.0 is the next generation of the Dolphin series of instruct-tuned models](https://huggingface.co/cognitivecomputations/Dolphin3.0-Llama3.1-8B)  ü§ó 
- **2 OLMo 2 Furious**, `arXiv, 2501.00656`, [arxiv](http://arxiv.org/abs/2501.00656v1), [pdf](http://arxiv.org/pdf/2501.00656v1.pdf), cication: [**-1**](None) 

	 *Team OLMo, Pete Walsh, Luca Soldaini, ..., Noah A. Smith, Hannaneh Hajishirzi* ¬∑ ([ùïè](https://x.com/kylelostat/status/1875209445950804114)) ¬∑ ([t](https://t.co/JR0tKbBgWz))
- [SmolLM2 1.7b Instruction Tuned & DPO Aligned through Tulu 3](https://huggingface.co/SultanR/SmolTulu-1.7b-Instruct)  ü§ó 
- [Granite 3.1 Language Models](https://huggingface.co/collections/ibm-granite/granite-31-language-models-6751dbbf2f3389bec5c6f02d)  ü§ó 
- [Hermes 3 3B is a full parameter fine-tune of the Llama-3.2 3B foundation model](https://huggingface.co/NousResearch/Hermes-3-Llama-3.2-3B)  ü§ó 
- [Introducing Phi-4: Microsoft‚Äôs Newest Small Language Model Specializing in Complex Reasoning](https://techcommunity.microsoft.com/blog/aiplatformblog/introducing-phi-4-microsoft%e2%80%99s-newest-small-language-model-specializing-in-comple/4357090) 
- **Phi-4 Technical Report**, `arXiv, 2412.08905`, [arxiv](http://arxiv.org/abs/2412.08905v1), [pdf](http://arxiv.org/pdf/2412.08905v1.pdf), cication: [**-1**](None) 

	 *Marah Abdin, Jyoti Aneja, Harkirat Behl, ..., Cyril Zhang, Yi Zhang*
- [C4AI Command R7B is an open weights research release of a 7B billion parameter model](https://huggingface.co/CohereForAI/c4ai-command-r7b-12-2024)  ü§ó 
- üåü **EXAONE 3.5: Series of Large Language Models for Real-world Use Cases**, `arXiv, 2412.04862`, [arxiv](http://arxiv.org/abs/2412.04862v2), [pdf](http://arxiv.org/pdf/2412.04862v2.pdf), cication: [**-1**](None) 

	 *LG AI Research, Soyoung An, Kyunghoon Bae, ..., Heuiyeen Yeen, Hyeongu Yun* ¬∑ ([huggingface](https://huggingface.co/LGAI-EXAON))
- [Deepthought-8B is a small and capable reasoning model built on LLaMA-3.1 8B, designed to make AI reasoning more transparent and controllable.](https://huggingface.co/ruliad/deepthought-8b-llama-v0.01-alpha)  ü§ó 
- **Fully Open Source Moxin-7B Technical Report**, `arXiv, 2412.06845`, [arxiv](http://arxiv.org/abs/2412.06845v1), [pdf](http://arxiv.org/pdf/2412.06845v1.pdf), cication: [**-1**](None) 

	 *Pu Zhao, Xuan Shen, Zhenglun Kong, ..., Yong He, Yanzhi Wang* ¬∑ ([Moxin-LLM](https://github.com/moxin-org/Moxin-LLM) - moxin-org) ![Star](https://img.shields.io/github/stars/moxin-org/Moxin-LLM.svg?style=social&label=Star)
- [A replication attempt of Tulu 3 on the Qwen 2.5 base models.](https://huggingface.co/allura-org/Teleut-7b)  ü§ó 
- üåü [OLMo 2: The best fully open language model to date](https://allenai.org/blog/olmo2) 

	 ¬∑ ([ùïè](https://x.com/natolambert/status/1861511563125096593))
- [INTELLECT-1 is the world's first decentralized training of a 10B parameter model, enabling anyone to contribute compute and participate.](https://app.primeintellect.ai/intelligence) 
- [Mistral-Large-Instruct-2411 is an advanced dense Large Language Model (LLM) of 123B parameters with state-of-the-art reasoning](https://huggingface.co/mistralai/Mistral-Large-Instruct-2411)  ü§ó 
- üåü [Athene-V2-Chat-72B, an open-weights LLM on-par with GPT-4o across benchmarks](https://huggingface.co/Nexusflow/Athene-V2-Chat)  ü§ó 

	 ¬∑ ([nexusflow](https://nexusflow.ai/blogs/athene-v2))
- [TableGPT2-7B, a large-scale decoder specifically tailored for data-intensive tasks, with a focus on interpreting and analyzing tabular data.](https://huggingface.co/tablegpt/TableGPT2-7B)  ü§ó 

	 ¬∑ ([tablegpt-agent](https://github.com/tablegpt/tablegpt-agent) - tablegpt) ![Star](https://img.shields.io/github/stars/tablegpt/tablegpt-agent.svg?style=social&label=Star)
- [Tess-R1 is designed with test-time compute in mind, and has the capabilities to produce a Chain-of-Thought (CoT) reasoning before producing the final output.](https://huggingface.co/migtissera/Tess-R1-Limerick-Llama-3.1-70B)  ü§ó 
- [AMD-OLMo are a series of 1 billion parameter language models trained by AMD on AMD Instinct‚Ñ¢ MI250 GPUs based on OLMo.](https://huggingface.co/collections/amd/amd-olmo-6723e7d04a49116d8ec95070)  ü§ó 
- [Steiner is a series of reasoning models trained on synthetic data using reinforcement learning.](https://huggingface.co/peakji/steiner-32b-preview)  ü§ó 
- [Granite 3.0 models](https://huggingface.co/collections/ibm-granite/granite-30-models-66fdb59bbb54785c3512114f)  ü§ó 

## Chinese Models

- [Qwen2.5-1M: Deploy Your Own Qwen with Context Length up to 1M Tokens](https://qwenlm.github.io/blog/qwen2.5-1m/) 
- **LLM360 K2: Building a 65B 360-Open-Source Large Language Model from 
  Scratch**, `arXiv, 2501.07124`, [arxiv](http://arxiv.org/abs/2501.07124v3), [pdf](http://arxiv.org/pdf/2501.07124v3.pdf), cication: [**-1**](None) 

	 *Zhengzhong Liu, Bowen Tan, Hongyi Wang, ..., Preslav Nakov, Eric Xing*
- **MiniMax-01: Scaling Foundation Models with Lightning Attention**, `arXiv, 2501.08313`, [arxiv](http://arxiv.org/abs/2501.08313v1), [pdf](http://arxiv.org/pdf/2501.08313v1.pdf), cication: [**-1**](None) 

	 *MiniMax, Aonian Li, Bangwei Gong, ..., Zhuo Jiang, Zijia Wu*
- [github.com](https://github.com/MiniMax-AI)
- [InternLM3 has open-sourced an 8-billion parameter instruction model, InternLM3-8B-Instruct, designed for general-purpose usage and advanced reasoning.](https://huggingface.co/internlm/internlm3-8b-instruct)  ü§ó 
- [Diving into MiniMax01 405B MoE](https://huggingface.co/blog/eliebak/minimax01-deepdive)  ü§ó 

	 ¬∑ ([ùïè](https://x.com/eliebakouch/status/1879458716455759908))
- **Xmodel-2 Technical Report**, `arXiv, 2412.19638`, [arxiv](http://arxiv.org/abs/2412.19638v1), [pdf](http://arxiv.org/pdf/2412.19638v1.pdf), cication: [**-1**](None) 

	 *Wang Qun, Liu Yang, Lin Qingquan, ..., Qu Zhijiu, Jiang Ling* ¬∑ ([Xmodel-2](https://github.com/XiaoduoAILab/Xmodel-2) - XiaoduoAILab) ![Star](https://img.shields.io/github/stars/XiaoduoAILab/Xmodel-2.svg?style=social&label=Star)
- üåü **YuLan-Mini: An Open Data-efficient Language Model**, `arXiv, 2412.17743`, [arxiv](http://arxiv.org/abs/2412.17743v2), [pdf](http://arxiv.org/pdf/2412.17743v2.pdf), cication: [**-1**](None) 

	 *Yiwen Hu, Huatong Song, Jia Deng, ..., Wayne Xin Zhao, Ji-Rong Wen* ¬∑ ([YuLan-Mini](https://github.com/RUC-GSAI/YuLan-Mini) - RUC-GSAI) ![Star](https://img.shields.io/github/stars/RUC-GSAI/YuLan-Mini.svg?style=social&label=Star)
- üåü **DeepSeek-V3 Technical Report**, `arXiv, 2412.19437`, [arxiv](http://arxiv.org/abs/2412.19437v1), [pdf](http://arxiv.org/pdf/2412.19437v1.pdf), cication: [**-1**](None) 

	 *DeepSeek-AI, Aixin Liu, Bei Feng, ..., Ziyi Gao, Zizheng Pan* ¬∑ ([DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) - deepseek-ai) ![Star](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3.svg?style=social&label=Star)
- [deepseek-ai  / DeepSeek-V3-Base](https://huggingface.co/deepseek-ai/DeepSeek-V3-Base/tree/main)  ü§ó 
- üåü [**DeepSeek-V3**](https://github.com/deepseek-ai/DeepSeek-V3) - deepseek-ai ![Star](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3.svg?style=social&label=Star) 
- **Qwen2.5 Technical Report**, `arXiv, 2412.15115`, [arxiv](http://arxiv.org/abs/2412.15115v1), [pdf](http://arxiv.org/pdf/2412.15115v1.pdf), cication: [**-1**](None) 

	 *Qwen, :, An Yang, ..., Zhenru Zhang, Zihan Qiu*
- [T-lite-it-1.0 is a model built upon the Qwen 2.5 model family and incorporates both continual pre-training and alignment techniques.](https://huggingface.co/t-tech/T-lite-it-1.0)  ü§ó 
- [DeepSeek-V2.5-1210 is an upgraded version of DeepSeek-V2.5, with improvements across various capabilities:](https://huggingface.co/deepseek-ai/DeepSeek-V2.5-1210)  ü§ó 
- **BayLing 2: A Multilingual Large Language Model with Efficient Language 
  Alignment**, `arXiv, 2411.16300`, [arxiv](http://arxiv.org/abs/2411.16300v1), [pdf](http://arxiv.org/pdf/2411.16300v1.pdf), cication: [**-1**](None) 

	 *Shaolei Zhang, Kehao Zhang, Qingkai Fang, ..., Xiaodong Liu, Yang Feng* ¬∑ ([BayLing](https://github.com/ictnlp/BayLing) - ictnlp) ![Star](https://img.shields.io/github/stars/ictnlp/BayLing.svg?style=social&label=Star)
- **Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated 
  Parameters by Tencent**, `arXiv, 2411.02265`, [arxiv](http://arxiv.org/abs/2411.02265v3), [pdf](http://arxiv.org/pdf/2411.02265v3.pdf), cication: [**-1**](None) 

	 *Xingwu Sun, Yanfeng Chen, Yiqing Huang, ..., Di Wang, Jie Jiang* ¬∑ ([huggingface](https://huggingface.co/tencent/Tencent-Hunyuan-Large)) ¬∑ ([Hunyuan-Large](https://github.com/Tencent/Hunyuan-Large) - Tencent) ![Star](https://img.shields.io/github/stars/Tencent/Hunyuan-Large.svg?style=social&label=Star)

## Reasoning

- [This is a 32B reasoning model trained from Qwen2.5-32B-Instruct with 17K data.](https://huggingface.co/NovaSky-AI/Sky-T1-32B-Preview)  ü§ó

	 ¬∑ ([ùïè](https://x.com/NovaSkyAI/status/1877793041957933347))
- [Bespoke-Stratos-32B, our reasoning model distilled from DeepSeek-R1 using Berkeley NovaSky‚Äôs Sky-T1 recipe.](https://x.com/madiator/status/1882131706515525919)  ùïè
- [OpenThinker-32B](https://huggingface.co/open-thoughts/OpenThinker-32B)  ü§ó

	 ¬∑ ([huggingface](https://huggingface.co/datasets/open-thoughts/OpenThoughts-114k)) ¬∑ ([LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory) - hiyouga) ![Star](https://img.shields.io/github/stars/hiyouga/LLaMA-Factory.svg?style=social&label=Star)
## Small Language Models

- [Hybrid Small Language Models.](https://huggingface.co/collections/nvidia/hymba-673c35516c12c4b98b5e845f)  ü§ó 

	 ¬∑ ([ùïè](https://x.com/reach_vb/status/1861492854293459423))
- üåü [**smollm**](https://github.com/huggingface/smollm) - huggingface ![Star](https://img.shields.io/github/stars/huggingface/smollm.svg?style=social&label=Star) 

	 *135M, 360M, and 1.7B parameters.* ¬∑ ([smollm](https://github.com/huggingface/smollm) - huggingface) ![Star](https://img.shields.io/github/stars/huggingface/smollm.svg?style=social&label=Star) ¬∑ ([ùïè](https://x.com/_philschmid/status/1859598525723488478)) ¬∑ ([huggingface](https://huggingface.co/datasets/HuggingFaceTB/smoltalk))
- [State-of-the-art compact LLMs for on-device applications: 1.7B, 360M, 135M    Upvote 107   +97](https://huggingface.co/collections/HuggingFaceTB/smollm2-6723884218bcda64b34d7db9)  ü§ó 

	 ¬∑ ([x](https://x.com/loubnabenallal1/status/1852055582494294414?s=46&t=MGz8l5Z36lvN2cHgl1IVqA))
- [SmolLM2 is a family of compact language models available in three size: 135M, 360M, and 1.7B parameters.](https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct)  ü§ó 
- [MobileLLM is an auto-regressive language model leveraging an optimized transformer architecture](https://huggingface.co/facebook/MobileLLM-125M)  ü§ó 

	 ¬∑ ([arxiv](https://arxiv.org/abs/2402.14905))

## Multilingual Models

- **Marco-LLM: Bridging Languages via Massive Multilingual Training for 
  Cross-Lingual Enhancement**, `arXiv, 2412.04003`, [arxiv](http://arxiv.org/abs/2412.04003v1), [pdf](http://arxiv.org/pdf/2412.04003v1.pdf), cication: [**-1**](None) 

	 *Lingfeng Ming, Bo Zeng, Chenyang Lyu, ..., Weihua Luo, Kaifu Zhang*
- [Sailor2: Sailing in South-East Asia with Inclusive Multilingual LLMs](http://sea-sailor.github.io/blog/sailor2/) 

	 ¬∑ ([ùïè](https://x.com/sivil_taram/status/1863989432170484008)) ¬∑ ([sailor2](https://github.com/sail-sg/sailor2) - sail-sg) ![Star](https://img.shields.io/github/stars/sail-sg/sailor2.svg?style=social&label=Star)
- **Xmodel-1.5: An 1B-scale Multilingual LLM**, `arXiv, 2411.10083`, [arxiv](http://arxiv.org/abs/2411.10083v1), [pdf](http://arxiv.org/pdf/2411.10083v1.pdf), cication: [**-1**](None) 

	 *Wang Qun, Liu Yang, Lin Qingquan, ..., Jiang Ling* ¬∑ ([XmodelLM](https://github.com/XiaoduoAILab/XmodelLM) - XiaoduoAILab) ![Star](https://img.shields.io/github/stars/XiaoduoAILab/XmodelLM.svg?style=social&label=Star)
- **Bielik 7B v0.1: A Polish Language Model -- Development, Insights, and 
  Evaluation**, `arXiv, 2410.18565`, [arxiv](http://arxiv.org/abs/2410.18565v1), [pdf](http://arxiv.org/pdf/2410.18565v1.pdf), cication: [**-1**](None)

	 *Krzysztof Ociepa, ≈Åukasz Flis, Krzysztof Wr√≥bel, ..., Adrian Gwo≈∫dziej, Remigiusz Kinas* ¬∑ ([bielik](https://bielik.ai/)) ¬∑ ([huggingface](https://huggingface.co/speakleash/Bielik-11B-v2.3-Instruct))
- :clapper: [Multilinguality and LLMs Special Session](https://www.youtube.com/watch?v=aNPa00_-DbA) 
- **Pangea: A Fully Open Multilingual Multimodal LLM for 39 Languages**, `arXiv, 2410.16153`, [arxiv](http://arxiv.org/abs/2410.16153v1), [pdf](http://arxiv.org/pdf/2410.16153v1.pdf), cication: [**-1**](None) 

	 *Xiang Yue, Yueqi Song, Akari Asai, ..., Sathyanarayanan Ramamoorthy, Graham Neubig* ¬∑ ([Pangea](https://github.com/neulab/Pangea) - neulab) ![Star](https://img.shields.io/github/stars/neulab/Pangea.svg?style=social&label=Star) ¬∑ ([neulab.github](https://neulab.github.io/Pangea/)) ¬∑ ([x](https://x.com/xiangyue96/status/1848753709787795679))

## Toolkits


## Misc

- [OLMo leads on the secrets of training language models (w Dirk Groeneveld, Kyle Lo, & Luca Soldaini)](https://www.youtube.com/watch?v=dS7QI99uJVc)  :clapper: 
- [Phi-4 bug fixes](https://x.com/danielhanchen/status/1877781452818968615)  ùïè 