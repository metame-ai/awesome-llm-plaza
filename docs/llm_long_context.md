# LLM Long Context

- [LLM Long Context](#llm-long-context)
  - [Survey](#survey)
  - [Long Context](#long-context)
  - [Evaluation](#evaluation)
  - [Projects](#projects)
  - [Misc](#misc)


## Survey


## Long Context

- [Paper page - LOGO -- Long cOntext aliGnment via efficient preference Optimization](https://huggingface.co/papers/2410.18533)
- [How to Train Long-Context Language Models (Effectively)](https://arxiv.org/abs/2410.02660)

	 · ([prolong](https://github.com/princeton-nlp/prolong?tab=readme-ov-file) - princeton-nlp) ![Star](https://img.shields.io/github/stars/princeton-nlp/prolong.svg?style=social&label=Star)
- [Why Does the Effective Context Length of LLMs Fall Short?](https://arxiv.org/abs/2410.18745)

	 · ([STRING](https://github.com/HKUNLP/STRING) - HKUNLP) ![Star](https://img.shields.io/github/stars/HKUNLP/STRING.svg?style=social&label=Star)
- **LLM$\times$MapReduce: Simplified Long-Sequence Processing using Large
  Language Models**, `arXiv, 2410.09342`, [arxiv](http://arxiv.org/abs/2410.09342v1), [pdf](http://arxiv.org/pdf/2410.09342v1.pdf), cication: [**-1**](None)

	 *Zihan Zhou, Chong Li, Xinyi Chen, ..., Zhiyuan Liu, Maosong Sun*

## Evaluation


## Projects


## Misc
