# Awesom long-context llm

- [Awesom long-context llm](#awesom-long-context-llm)
	- [Survey](#survey)
	- [Papers](#papers)
	- [Projects](#projects)
	- [Other](#other)
	- [Extra reference](#extra-reference)

## Survey
- **Advancing Transformer Architecture in Long-Context Large Language
  Models: A Comprehensive Survey**, `arXiv, 2311.12351`, [arxiv](http://arxiv.org/abs/2311.12351v1), [pdf](http://arxiv.org/pdf/2311.12351v1.pdf), cication: [**-1**](None)

	 *Yunpeng Huang, Jingwei Xu, Zixu Jiang, Junyu Lai, Zenan Li, Yuan Yao, Taolue Chen, Lijuan Yang, Zhou Xin, Xiaoxing Ma* · ([long-llms-learning](https://github.com/Strivin0311/long-llms-learning) - Strivin0311) ![Star](https://img.shields.io/github/stars/Strivin0311/long-llms-learning.svg?style=social&label=Star)
- [The Transformer Family | Lil'Log](https://lilianweng.github.io/posts/2020-04-07-the-transformer-family/#adaptive-computation-time-act)
- [The Transformer Family Version 2.0 | Lil'Log](https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/)
- [Attention? Attention! | Lil'Log](https://lilianweng.github.io/posts/2018-06-24-attention/#a-family-of-attention-mechanisms)

## Papers
- **Transformers are Multi-State RNNs**, `arXiv, 2401.06104`, [arxiv](http://arxiv.org/abs/2401.06104v1), [pdf](http://arxiv.org/pdf/2401.06104v1.pdf), cication: [**-1**](None)

	 *Matanel Oren, Michael Hassid, Yossi Adi, Roy Schwartz* · ([TOVA](https://github.com/schwartz-lab-NLP/TOVA) - schwartz-lab-NLP) ![Star](https://img.shields.io/github/stars/schwartz-lab-NLP/TOVA.svg?style=social&label=Star)
- **Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence
  Lengths in Large Language Models**, `arXiv, 2401.04658`, [arxiv](http://arxiv.org/abs/2401.04658v1), [pdf](http://arxiv.org/pdf/2401.04658v1.pdf), cication: [**-1**](None)

	 *Zhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, Yiran Zhong* · ([lightning-attention](https://github.com/OpenNLPLab/lightning-attention) - OpenNLPLab) ![Star](https://img.shields.io/github/stars/OpenNLPLab/lightning-attention.svg?style=social&label=Star)
- **Infinite-LLM: Efficient LLM Service for Long Context with DistAttention
  and Distributed KVCache**, `arXiv, 2401.02669`, [arxiv](http://arxiv.org/abs/2401.02669v1), [pdf](http://arxiv.org/pdf/2401.02669v1.pdf), cication: [**-1**](None)

	 *Bin Lin, Tao Peng, Chen Zhang, Minmin Sun, Lanbo Li, Hanyu Zhao, Wencong Xiao, Qi Xu, Xiafei Qiu, Shen Li*
- **LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning**, `arXiv, 2401.01325`, [arxiv](http://arxiv.org/abs/2401.01325v1), [pdf](http://arxiv.org/pdf/2401.01325v1.pdf), cication: [**-1**](None)

	 *Hongye Jin, Xiaotian Han, Jingfeng Yang, Zhimeng Jiang, Zirui Liu, Chia-Yuan Chang, Huiyuan Chen, Xia Hu* · ([qbitai](https://www.qbitai.com/2024/01/112273.html))
- **Cached Transformers: Improving Transformers with Differentiable Memory
  Cache**, `arXiv, 2312.12742`, [arxiv](http://arxiv.org/abs/2312.12742v1), [pdf](http://arxiv.org/pdf/2312.12742v1.pdf), cication: [**-1**](None)

	 *Zhaoyang Zhang, Wenqi Shao, Yixiao Ge, Xiaogang Wang, Jinwei Gu, Ping Luo*
- **Extending Context Window of Large Language Models via Semantic
  Compression**, `arXiv, 2312.09571`, [arxiv](http://arxiv.org/abs/2312.09571v1), [pdf](http://arxiv.org/pdf/2312.09571v1.pdf), cication: [**-1**](None)

	 *Weizhi Fei, Xueyan Niu, Pingyi Zhou, Lu Hou, Bo Bai, Lei Deng, Wei Han*
- **Zebra: Extending Context Window with Layerwise Grouped Local-Global
  Attention**, `arXiv, 2312.08618`, [arxiv](http://arxiv.org/abs/2312.08618v1), [pdf](http://arxiv.org/pdf/2312.08618v1.pdf), cication: [**-1**](None)

	 *Kaiqiang Song, Xiaoyang Wang, Sangwoo Cho, Xiaoman Pan, Dong Yu*
- **Ultra-Long Sequence Distributed Transformer**, `arXiv, 2311.02382`, [arxiv](http://arxiv.org/abs/2311.02382v2), [pdf](http://arxiv.org/pdf/2311.02382v2.pdf), cication: [**-1**](None)

	 *Xiao Wang, Isaac Lyngaas, Aristeidis Tsaris, Peng Chen, Sajal Dash, Mayanka Chandra Shekar, Tao Luo, Hong-Jun Yoon, Mohamed Wahib, John Gouley*
- **HyperAttention: Long-context Attention in Near-Linear Time**, `arXiv, 2310.05869`, [arxiv](http://arxiv.org/abs/2310.05869v2), [pdf](http://arxiv.org/pdf/2310.05869v2.pdf), cication: [**2**](https://scholar.google.com/scholar?cites=12733927342294964560&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Insu Han, Rajesh Jayaram, Amin Karbasi, Vahab Mirrokni, David P. Woodruff, Amir Zandieh*
- **CLEX: Continuous Length Extrapolation for Large Language Models**, `arXiv, 2310.16450`, [arxiv](http://arxiv.org/abs/2310.16450v1), [pdf](http://arxiv.org/pdf/2310.16450v1.pdf), cication: [**-1**](None)

	 *Guanzheng Chen, Xin Li, Zaiqiao Meng, Shangsong Liang, Lidong Bing*
- **TRAMS: Training-free Memory Selection for Long-range Language Modeling**, `arXiv, 2310.15494`, [arxiv](http://arxiv.org/abs/2310.15494v2), [pdf](http://arxiv.org/pdf/2310.15494v2.pdf), cication: [**-1**](None)

	 *Haofei Yu, Cunxiang Wang, Yue Zhang, Wei Bi*
- **Walking Down the Memory Maze: Beyond Context Limit through Interactive
  Reading**, `arXiv, 2310.05029`, [arxiv](http://arxiv.org/abs/2310.05029v1), [pdf](http://arxiv.org/pdf/2310.05029v1.pdf), cication: [**-1**](None)

	 *Howard Chen, Ramakanth Pasunuru, Jason Weston, Asli Celikyilmaz* · ([mp.weixin.qq](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652394913&idx=5&sn=173aada0b5d667541f02c12ddda4a357))
- **Scaling Laws of RoPE-based Extrapolation**, `arXiv, 2310.05209`, [arxiv](http://arxiv.org/abs/2310.05209v1), [pdf](http://arxiv.org/pdf/2310.05209v1.pdf), cication: [**-1**](None)

	 *Xiaoran Liu, Hang Yan, Shuo Zhang, Chenxin An, Xipeng Qiu, Dahua Lin* · ([qbitai](https://www.qbitai.com/2023/10/91648.html))
- **Walking Down the Memory Maze: Beyond Context Limit through Interactive
  Reading**, `arXiv, 2310.05029`, [arxiv](http://arxiv.org/abs/2310.05029v1), [pdf](http://arxiv.org/pdf/2310.05029v1.pdf), cication: [**-1**](None)

	 *Howard Chen, Ramakanth Pasunuru, Jason Weston, Asli Celikyilmaz*
- **Ring Attention with Blockwise Transformers for Near-Infinite Context**, `arXiv, 2310.01889`, [arxiv](http://arxiv.org/abs/2310.01889v3), [pdf](http://arxiv.org/pdf/2310.01889v3.pdf), cication: [**-1**](None)

	 *Hao Liu, Matei Zaharia, Pieter Abbeel*
- **EIPE-text: Evaluation-Guided Iterative Plan Extraction for Long-Form
  Narrative Text Generation**, `arXiv, 2310.08185`, [arxiv](http://arxiv.org/abs/2310.08185v1), [pdf](http://arxiv.org/pdf/2310.08185v1.pdf), cication: [**-1**](None)

	 *Wang You, Wenshan Wu, Yaobo Liang, Shaoguang Mao, Chenfei Wu, Maosong Cao, Yuzhe Cai, Yiduo Guo, Yan Xia, Furu Wei*
- **CoCA: Fusing position embedding with Collinear Constrained Attention for
  fine-tuning free context window extending**, `arXiv, 2309.08646`, [arxiv](http://arxiv.org/abs/2309.08646v2), [pdf](http://arxiv.org/pdf/2309.08646v2.pdf), cication: [**-1**](None)

	 *Shiyi Zhu, Jing Ye, Wei Jiang, Qi Zhang, Yifan Wu, Jianguo Li* · ([Collinear-Constrained-Attention](https://github.com/codefuse-ai/Collinear-Constrained-Attention) - codefuse-ai) ![Star](https://img.shields.io/github/stars/codefuse-ai/Collinear-Constrained-Attention.svg?style=social&label=Star) · ([jiqizhixin](https://www.jiqizhixin.com/articles/2023-11-13-9))
- **Effective Long-Context Scaling of Foundation Models**, `arXiv, 2309.16039`, [arxiv](http://arxiv.org/abs/2309.16039v2), [pdf](http://arxiv.org/pdf/2309.16039v2.pdf), cication: [**1**](https://scholar.google.com/scholar?cites=11499544443437070837&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz* · ([qbitai](https://www.qbitai.com/2023/09/87178.html))
- **LM-Infinite: Simple On-the-Fly Length Generalization for Large Language
  Models**, `arXiv, 2308.16137`, [arxiv](http://arxiv.org/abs/2308.16137v4), [pdf](http://arxiv.org/pdf/2308.16137v4.pdf), cication: [**3**](https://scholar.google.com/scholar?cites=12333597550468686714&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, Sinong Wang*
- **DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme
  Long Sequence Transformer Models**, `arXiv, 2309.14509`, [arxiv](http://arxiv.org/abs/2309.14509v2), [pdf](http://arxiv.org/pdf/2309.14509v2.pdf), cication: [**-1**](None)

	 *Sam Ade Jacobs, Masahiro Tanaka, Chengming Zhang, Minjia Zhang, Shuaiwen Leon Song, Samyam Rajbhandari, Yuxiong He*
- **YaRN: Efficient Context Window Extension of Large Language Models**, `arXiv, 2309.00071`, [arxiv](http://arxiv.org/abs/2309.00071v2), [pdf](http://arxiv.org/pdf/2309.00071v2.pdf), cication: [**9**](https://scholar.google.com/scholar?cites=4031663212894418290&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Bowen Peng, Jeffrey Quesnelle, Honglu Fan, Enrico Shippole* · ([yarn](https://github.com/jquesnelle/yarn) - jquesnelle) ![Star](https://img.shields.io/github/stars/jquesnelle/yarn.svg?style=social&label=Star) · ([jiqizhixin](https://www.jiqizhixin.com/articles/2023-09-13-10))
- **In-context Autoencoder for Context Compression in a Large Language Model**, `arXiv, 2307.06945`, [arxiv](http://arxiv.org/abs/2307.06945v2), [pdf](http://arxiv.org/pdf/2307.06945v2.pdf), cication: [**4**](https://scholar.google.com/scholar?cites=16575040938931374371&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Tao Ge, Jing Hu, Lei Wang, Xun Wang, Si-Qing Chen, Furu Wei*
- **Focused Transformer: Contrastive Training for Context Scaling**, `arXiv, 2307.03170`, [arxiv](http://arxiv.org/abs/2307.03170v1), [pdf](http://arxiv.org/pdf/2307.03170v1.pdf), cication: [**12**](https://scholar.google.com/scholar?cites=9230341592198565561&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Szymon Tworkowski, Konrad Staniszewski, Mikołaj Pacek, Yuhuai Wu, Henryk Michalewski, Piotr Miłoś*
- **Lost in the Middle: How Language Models Use Long Contexts**, `arXiv, 2307.03172`, [arxiv](http://arxiv.org/abs/2307.03172v2), [pdf](http://arxiv.org/pdf/2307.03172v2.pdf), cication: [**64**](https://scholar.google.com/scholar?cites=9077647831317733932&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, Percy Liang*
- **LongNet: Scaling Transformers to 1,000,000,000 Tokens**, `arXiv, 2307.02486`, [arxiv](http://arxiv.org/abs/2307.02486v2), [pdf](http://arxiv.org/pdf/2307.02486v2.pdf), cication: [**15**](https://scholar.google.com/scholar?cites=9993250623533435628&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, Nanning Zheng, Furu Wei*
- **Extending Context Window of Large Language Models via Positional
  Interpolation**, `arXiv, 2306.15595`, [arxiv](http://arxiv.org/abs/2306.15595v2), [pdf](http://arxiv.org/pdf/2306.15595v2.pdf), cication: [**36**](https://scholar.google.com/scholar?cites=16329089917044170292&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Shouyuan Chen, Sherman Wong, Liangjian Chen, Yuandong Tian* · ([qbitai](https://www.qbitai.com/2023/06/64516.html))
- **The Impact of Positional Encoding on Length Generalization in
  Transformers**, `arXiv, 2305.19466`, [arxiv](http://arxiv.org/abs/2305.19466v2), [pdf](http://arxiv.org/pdf/2305.19466v2.pdf), cication: [**5**](https://scholar.google.com/scholar?cites=4006212005145195153&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Amirhossein Kazemnejad, Inkit Padhi, Karthikeyan Natesan Ramamurthy, Payel Das, Siva Reddy*
- **Long-range Language Modeling with Self-retrieval**, `arXiv, 2306.13421`, [arxiv](http://arxiv.org/abs/2306.13421v1), [pdf](http://arxiv.org/pdf/2306.13421v1.pdf), cication: [**3**](https://scholar.google.com/scholar?cites=13798183978039596966&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Ohad Rubin, Jonathan Berant*
- **Block-State Transformers**, `arXiv, 2306.09539`, [arxiv](http://arxiv.org/abs/2306.09539v4), [pdf](http://arxiv.org/pdf/2306.09539v4.pdf), cication: [**2**](https://scholar.google.com/scholar?cites=15343899864948628782&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Mahan Fathi, Jonathan Pilault, Orhan Firat, Christopher Pal, Pierre-Luc Bacon, Ross Goroshin*
- **LeanDojo: Theorem Proving with Retrieval-Augmented Language Models**, `arXiv, 2306.15626`, [arxiv](http://arxiv.org/abs/2306.15626v2), [pdf](http://arxiv.org/pdf/2306.15626v2.pdf), cication: [**14**](https://scholar.google.com/scholar?cites=16638561208416938954&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Kaiyu Yang, Aidan M. Swope, Alex Gu, Rahul Chalamala, Peiyang Song, Shixing Yu, Saad Godil, Ryan Prenger, Anima Anandkumar*
- **GLIMMER: generalized late-interaction memory reranker**, `arXiv, 2306.10231`, [arxiv](http://arxiv.org/abs/2306.10231v1), [pdf](http://arxiv.org/pdf/2306.10231v1.pdf), cication: [**1**](https://scholar.google.com/scholar?cites=13889321003607649821&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Michiel de Jong, Yury Zemlyanskiy, Nicholas FitzGerald, Sumit Sanghai, William W. Cohen, Joshua Ainslie*
- **Augmenting Language Models with Long-Term Memory**, `arXiv, 2306.07174`, [arxiv](http://arxiv.org/abs/2306.07174v1), [pdf](http://arxiv.org/pdf/2306.07174v1.pdf), cication: [**7**](https://scholar.google.com/scholar?cites=10032266285163330345&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Weizhi Wang, Li Dong, Hao Cheng, Xiaodong Liu, Xifeng Yan, Jianfeng Gao, Furu Wei* · ([aka](https://aka.ms/LongMem))
- **Sequence Parallelism: Long Sequence Training from System Perspective**, `arXiv, 2105.13120`, [arxiv](http://arxiv.org/abs/2105.13120v3), [pdf](http://arxiv.org/pdf/2105.13120v3.pdf), cication: [**2**](https://scholar.google.com/scholar?cites=1387350093305756276&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Shenggui Li, Fuzhao Xue, Chaitanya Baranwal, Yongbin Li, Yang You*

## Projects
- [**LLMLingua**](https://github.com/microsoft/LLMLingua) - microsoft ![Star](https://img.shields.io/github/stars/microsoft/LLMLingua.svg?style=social&label=Star)

	 *To speed up LLMs' inference and enhance LLM's perceive of key information, compress the prompt and KV-Cache, which achieves up to 20x compression with minimal performance loss.*
- [**long-context**](https://github.com/abacusai/long-context) - abacusai ![Star](https://img.shields.io/github/stars/abacusai/long-context.svg?style=social&label=Star)

	 *This repository contains code and tooling for the Abacus.AI LLM Context Expansion project. Also included are evaluation scripts and benchmark tasks that evaluate a model’s information retrieval capabilities with context expansion. We also include key experimental results and instructions for reproducing and building on them.*
- [LLaMA rope_scaling](https://huggingface.co/docs/transformers/main/en/model_doc/llama#transformers.LlamaConfig.rope_scaling)
- [**long_llama**](https://github.com/cstankonrad/long_llama) - cstankonrad ![Star](https://img.shields.io/github/stars/cstankonrad/long_llama.svg?style=social&label=Star)

	 *LongLLaMA is a large language model capable of handling long contexts. It is based on OpenLLaMA and fine-tuned with the Focused Transformer (FoT) method.*

## Other
- [Understanding data influence on context scaling: a close look at baseline solution](https://yaofu.notion.site/Understanding-data-influence-on-context-scaling-a-close-look-at-baseline-solution-eb17eab795dd4132b1a1ffe73f5e850a)
- [Anthropic \\ Long context prompting for Claude 2.1](https://www.anthropic.com/index/claude-2-1-prompting)
- [The Secret Sauce behind 100K context window in LLMs: all tricks in one place | by Galina Alperovich | May, 2023 | GoPenAI](https://blog.gopenai.com/how-to-speed-up-llms-and-use-100k-context-window-all-tricks-in-one-place-ffd40577b4c)
- [Extending Context is Hard | kaiokendev.github.io](https://kaiokendev.github.io/context)

- [一句话解锁100k+上下文大模型真实力，27分涨到98，GPT-4、Claude2.1适用 | 量子位](https://www.qbitai.com/2023/12/105450.html)
- [500万token巨兽，一次读完全套「哈利波特」！比ChatGPT长1000多倍](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652338537&idx=2&sn=8aaf7f6e61ca91fd65a4d42e845f77f5)

## Extra reference