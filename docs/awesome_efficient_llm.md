# Awesome efficient llm

- **Prompt Cache: Modular Attention Reuse for Low-Latency Inference**, `arXiv, 2311.04934`, [arxiv](http://arxiv.org/abs/2311.04934v1), [pdf](http://arxiv.org/pdf/2311.04934v1.pdf), cication: [**-1**](None)

	 *In Gim, Guojun Chen, Seung-seob Lee, Nikhil Sarda, Anurag Khandelwal, Lin Zhong*

## Efficient transformer
- **Simplifying Transformer Blocks**, `arXiv, 2311.01906`, [arxiv](http://arxiv.org/abs/2311.01906v1), [pdf](http://arxiv.org/pdf/2311.01906v1.pdf), cication: [**-1**](None)

	 *Bobby He, Thomas Hofmann*
- **Alternating Updates for Efficient Transformers**, `arXiv, 2301.13310`, [arxiv](http://arxiv.org/abs/2301.13310v2), [pdf](http://arxiv.org/pdf/2301.13310v2.pdf), cication: [**-1**](None)

	 *Cenk Baykal, Dylan Cutler, Nishanth Dikkala, Nikhil Ghosh, Rina Panigrahy, Xin Wang*