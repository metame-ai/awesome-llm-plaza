# LLM Training

- [LLM Training](#llm-training)
  - [Survey](#survey)
  - [LLM Training](#llm-training)
  - [Pretraining](#pretraining)
  - [Finetuning](#finetuning)
  - [Optimization](#optimization)
  - [Architecture](#architecture)
  - [Mixture Of Experts](#mixture-of-experts)
  - [Merge](#merge)
  - [Online Learning](#online-learning)
  - [Toolkits](#toolkits)
  - [Misc](#misc)


## Survey


## LLM Training


## Pretraining

- [Paper page - MiniPLM: Knowledge Distillation for Pre-Training Language Models](https://huggingface.co/papers/2410.17215)
- **Pre-training Distillation for Large Language Models: A Design Space
  Exploration**, `arXiv, 2410.16215`, [arxiv](http://arxiv.org/abs/2410.16215v1), [pdf](http://arxiv.org/pdf/2410.16215v1.pdf), cication: [**-1**](None)

	 *Hao Peng, Xin Lv, Yushi Bai, ..., Lei Hou, Juanzi Li*

## Finetuning

- **SemiEvol: Semi-supervised Fine-tuning for LLM Adaptation**, `arXiv, 2410.14745`, [arxiv](http://arxiv.org/abs/2410.14745v1), [pdf](http://arxiv.org/pdf/2410.14745v1.pdf), cication: [**-1**](None)

	 *Junyu Luo, Xiao Luo, Xiusi Chen, ..., Wei Ju, Ming Zhang*

## Optimization

- [Hacks to Make LLM Training Faster - Daniel Han, Unsloth AI](https://www.youtube.com/watch?v=PdtKkc5jB4g)

## Architecture


## Mixture Of Experts


## Merge

- **Exploring Model Kinship for Merging Large Language Models**, `arXiv, 2410.12613`, [arxiv](http://arxiv.org/abs/2410.12613v1), [pdf](http://arxiv.org/pdf/2410.12613v1.pdf), cication: [**-1**](None)

	 *Yedi Hu, Yunzhi Yao, Ningyu Zhang, ..., Shumin Deng, Huajun Chen* · ([ModelKinship](https://github.com/zjunlp/ModelKinship) - zjunlp) ![Star](https://img.shields.io/github/stars/zjunlp/ModelKinship.svg?style=social&label=Star)

## Online Learning


## Toolkits

- [torchtune: Easy and Accessible Finetuning in Native PyTorch - Evan Smothers, Meta](https://www.youtube.com/watch?v=43X9E25-Qg0)
- [Fixing Gradient Accumulation](https://huggingface.co/blog/gradient_accumulation)
- [Fixed a bug which caused all training losses to diverge for large gradient accumulation sizes.](https://x.com/danielhanchen/status/1846235913443262891)
- **AutoTrain: No-code training for state-of-the-art models**, `arXiv, 2410.15735`, [arxiv](http://arxiv.org/abs/2410.15735v1), [pdf](http://arxiv.org/pdf/2410.15735v1.pdf), cication: [**-1**](None)

	 *Abhishek Thakur*

	 · ([autotrain-advanced](https://github.com/huggingface/autotrain-advanced) - huggingface) ![Star](https://img.shields.io/github/stars/huggingface/autotrain-advanced.svg?style=social&label=Star)

## Misc
