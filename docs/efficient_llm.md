# Efficient LLM

- [Efficient LLM](#efficient-llm)
  - [Survey](#survey)
  - [Efficient LLM](#efficient-llm-1)
  - [Finetune](#finetune)
  - [Quantization](#quantization)
  - [Distillation](#distillation)
  - [Pruning](#pruning)
  - [Inference](#inference)
  - [Mobile](#mobile)
  - [Transformer](#transformer)
  - [Hardware](#hardware)
  - [Tutorials](#tutorials)
  - [Projects](#projects)
  - [Products](#products)
  - [Misc](#misc)


## Survey


## Efficient LLM


## Finetune


## Quantization


## Distillation


## Pruning

- [What Matters in Transformers? Not All Attention is Needed](https://arxiv.org/abs/2406.15786)

## Inference


## Mobile


## Transformer

- [Paper page - SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs](https://huggingface.co/papers/2410.13276)
- [Paper page - MoH: Multi-Head Attention as Mixture-of-Head Attention](https://huggingface.co/papers/2410.11842)
   - [arxiv.org](https://arxiv.org/pdf/2410.11842)
   - [github.com](https://github.com/SkyworkAI/MoH)
   - [huggingface.co](https://huggingface.co/collections/Chat-UniVi/moh-66f4277375c1c1b2ad61a2c1)

## Hardware


## Tutorials

- [Lecture 32: Unsloth](https://www.youtube.com/watch?v=hfb_AIhDYnA)

## Projects


## Products


## Misc

- [k-mktr / gpu-poor-llm-arena](https://huggingface.co/spaces/k-mktr/gpu-poor-llm-arena/tree/main)

