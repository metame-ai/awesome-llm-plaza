# Awesome-robotics-llm

- [Awesome-robotics-llm](#awesome-robotics-llm)
	- [Papers](#papers)
	- [Projects](#projects)
	- [Other](#other)
	- [Reference](#reference)

## Survey
- **A Survey of Embodied Learning for Object-Centric Robotic Manipulation**, `arXiv, 2408.11537`, [arxiv](http://arxiv.org/abs/2408.11537v1), [pdf](http://arxiv.org/pdf/2408.11537v1.pdf), cication: [**-1**](None)

	 *Ying Zheng, Lei Yao, Yuejiao Su, Yi Zhang, Yi Wang, Sicheng Zhao, Yiyi Zhang, Lap-Pui Chau* · ([ocrm_survey](https://github.com/rayyoh/ocrm_survey) - rayyoh) ![Star](https://img.shields.io/github/stars/rayyoh/ocrm_survey.svg?style=social&label=Star)
- **When LLMs step into the 3D World: A Survey and Meta-Analysis of 3D Tasks
  via Multi-modal Large Language Models**, `arXiv, 2405.10255`, [arxiv](http://arxiv.org/abs/2405.10255v1), [pdf](http://arxiv.org/pdf/2405.10255v1.pdf), cication: [**3**](https://scholar.google.com/scholar?cites=3066496559366068505&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Xianzheng Ma, Yash Bhalgat, Brandon Smart, Shuai Chen, Xinghui Li, Jian Ding, Jindong Gu, Dave Zhenyu Chen, Songyou Peng, Jia-Wang Bian*
- **Aligning Cyber Space with Physical World: A Comprehensive Survey on
  Embodied AI**, `arXiv, 2407.06886`, [arxiv](http://arxiv.org/abs/2407.06886v6), [pdf](http://arxiv.org/pdf/2407.06886v6.pdf), cication: [**2**](https://scholar.google.com/scholar?cites=5047172009300036805&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Yang Liu, Weixing Chen, Yongjie Bai, Guanbin Li, Wen Gao, Liang Lin* · ([embodied_ai_paper_list](https://github.com/hcplab-sysu/embodied_ai_paper_list) - hcplab-sysu) ![Star](https://img.shields.io/github/stars/hcplab-sysu/embodied_ai_paper_list.svg?style=social&label=Star)
- **A Survey on Integration of Large Language Models with Intelligent Robots**, `arXiv, 2404.09228`, [arxiv](http://arxiv.org/abs/2404.09228v1), [pdf](http://arxiv.org/pdf/2404.09228v1.pdf), cication: [**-1**](None)

	 *Yeseung Kim, Dohyun Kim, Jieun Choi, Jisang Park, Nayoung Oh, Daehyung Park*

## Papers
- **Diffusion Models Are Real-Time Game Engines**, `arXiv, 2408.14837`, [arxiv](http://arxiv.org/abs/2408.14837v1), [pdf](http://arxiv.org/pdf/2408.14837v1.pdf), cication: [**-1**](None)

	 *Dani Valevski, Yaniv Leviathan, Moab Arar, Shlomi Fruchter* · ([gamengen.github](https://gamengen.github.io/))
- **Scaling Cross-Embodied Learning: One Policy for Manipulation,
  Navigation, Locomotion and Aviation**, `arXiv, 2408.11812`, [arxiv](http://arxiv.org/abs/2408.11812v1), [pdf](http://arxiv.org/pdf/2408.11812v1.pdf), cication: [**1**](https://scholar.google.com/scholar?cites=1288216298584926140&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Ria Doshi, Homer Walke, Oier Mees, Sudeep Dasari, Sergey Levine* · ([crossformer-model.github](https://crossformer-model.github.io/))
- **All Robots in One: A New Standard and Unified Dataset for Versatile,
  General-Purpose Embodied Agents**, `arXiv, 2408.10899`, [arxiv](http://arxiv.org/abs/2408.10899v1), [pdf](http://arxiv.org/pdf/2408.10899v1.pdf), cication: [**-1**](None)

	 *Zhiqiang Wang, Hao Zheng, Yunshuang Nie, Wenjun Xu, Qingwei Wang, Hua Ye, Zhe Li, Kaidong Zhang, Xuewen Cheng, Wanxi Dong* · ([imaei.github](https://imaei.github.io/project_pages/ario/)) · ([jiqizhixin](https://www.jiqizhixin.com/articles/2024-08-23-2))
- **Diffusion Augmented Agents: A Framework for Efficient Exploration and
  Transfer Learning**, `arXiv, 2407.20798`, [arxiv](http://arxiv.org/abs/2407.20798v1), [pdf](http://arxiv.org/pdf/2407.20798v1.pdf), cication: [**-1**](None)

	 *Norman Di Palo, Leonard Hasenclever, Jan Humplik, Arunkumar Byravan* · ([sites.google](https://sites.google.com/view/diffusion-augmented-agents/)) · ([mp.weixin.qq](https://mp.weixin.qq.com/s/P-x8EDrfd1ydCnPP8MYu6g))
- **ROS-LLM: A ROS framework for embodied AI with task feedback and
  structured reasoning**, `arXiv, 2406.19741`, [arxiv](http://arxiv.org/abs/2406.19741v2), [pdf](http://arxiv.org/pdf/2406.19741v2.pdf), cication: [**-1**](None)

	 *Christopher E. Mower, Yuhui Wan, Hongzhan Yu, Antoine Grosnit, Jonas Gonzalez-Billandon, Matthieu Zimmer, Jinlong Wang, Xinyu Zhang, Yao Zhao, Anbang Zhai*

	 · ([HEBO](https://github.com/huawei-noah/HEBO/tree/rosllm/ROSLLM) - huawei-noah) ![Star](https://img.shields.io/github/stars/huawei-noah/HEBO.svg?style=social&label=Star)
- **LLaRA: Supercharging Robot Learning Data for Vision-Language Policy**, `arXiv, 2406.20095`, [arxiv](http://arxiv.org/abs/2406.20095v1), [pdf](http://arxiv.org/pdf/2406.20095v1.pdf), cication: [**-1**](None)

	 *Xiang Li, Cristina Mata, Jongwoo Park, Kumara Kahatapitiya, Yoo Sung Jang, Jinghuan Shang, Kanchana Ranasinghe, Ryan Burgert, Mu Cai, Yong Jae Lee*

	 · ([LLaRA](https://github.com/LostXine/LLaRA) - LostXine) ![Star](https://img.shields.io/github/stars/LostXine/LLaRA.svg?style=social&label=Star)
- **PoliFormer: Scaling On-Policy RL with Transformers Results in Masterful
  Navigators**, `arXiv, 2406.20083`, [arxiv](http://arxiv.org/abs/2406.20083v1), [pdf](http://arxiv.org/pdf/2406.20083v1.pdf), cication: [**-1**](None)

	 *Kuo-Hao Zeng, Zichen Zhang, Kiana Ehsani, Rose Hendrix, Jordi Salvador, Alvaro Herrasti, Ross Girshick, Aniruddha Kembhavi, Luca Weihs*
- **OmniJARVIS: Unified Vision-Language-Action Tokenization Enables
  Open-World Instruction Following Agents**, `arXiv, 2407.00114`, [arxiv](http://arxiv.org/abs/2407.00114v1), [pdf](http://arxiv.org/pdf/2407.00114v1.pdf), cication: [**-1**](None)

	 *Zihao Wang, Shaofei Cai, Zhancun Mu, Haowei Lin, Ceyao Zhang, Xuejie Liu, Qing Li, Anji Liu, Xiaojian Ma, Yitao Liang*

	 · ([OmniJarvis](https://github.com/CraftJarvis/OmniJarvis) - CraftJarvis) ![Star](https://img.shields.io/github/stars/CraftJarvis/OmniJarvis.svg?style=social&label=Star) · ([omnijarvis.github](https://omnijarvis.github.io/))
- **Octo-planner: On-device Language Model for Planner-Action Agents**, `arXiv, 2406.18082`, [arxiv](http://arxiv.org/abs/2406.18082v1), [pdf](http://arxiv.org/pdf/2406.18082v1.pdf), cication: [**-1**](None)

	 *Wei Chen, Zhiyuan Li, Zhen Guo, Yikang Shen* · ([huggingface](https://huggingface.co/NexaAIDev/octopus-planning)) · ([nexa4ai](https://www.nexa4ai.com/octo-planner))
- **OpenVLA: An Open-Source Vision-Language-Action Model**, `arXiv, 2406.09246`, [arxiv](http://arxiv.org/abs/2406.09246v1), [pdf](http://arxiv.org/pdf/2406.09246v1.pdf), cication: [**-1**](None)

	 *Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi* · ([openvla.github](https://openvla.github.io/)) · ([openvla](https://github.com/openvla/openvla) - openvla) ![Star](https://img.shields.io/github/stars/openvla/openvla.svg?style=social&label=Star)
- **Octo: An Open-Source Generalist Robot Policy**, `arXiv, 2405.12213`, [arxiv](http://arxiv.org/abs/2405.12213v1), [pdf](http://arxiv.org/pdf/2405.12213v1.pdf), cication: [**26**](https://scholar.google.com/scholar?cites=577688306634111052&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu* · ([octo-models.github](https://octo-models.github.io/)) · ([octo](https://github.com/octo-models/octo) - octo-models) ![Star](https://img.shields.io/github/stars/octo-models/octo.svg?style=social&label=Star)
- **From LLMs to Actions: Latent Codes as Bridges in Hierarchical Robot
  Control**, `arXiv, 2405.04798`, [arxiv](http://arxiv.org/abs/2405.04798v1), [pdf](http://arxiv.org/pdf/2405.04798v1.pdf), cication: [**-1**](None)

	 *Yide Shentu, Philipp Wu, Aravind Rajeswaran, Pieter Abbeel*
- **LEGENT: Open Platform for Embodied Agents**, `arXiv, 2404.18243`, [arxiv](http://arxiv.org/abs/2404.18243v1), [pdf](http://arxiv.org/pdf/2404.18243v1.pdf), cication: [**-1**](None)

	 *Zhili Cheng, Zhitong Wang, Jinyi Hu, Shengding Hu, An Liu, Yuge Tu, Pengkai Li, Lei Shi, Zhiyuan Liu, Maosong Sun*
- [DrEureka | Language Model Guided Sim-To-Real Transfer](https://eureka-research.github.io/dr-eureka/)

	 · ([eureka-research.github](https://eureka-research.github.io/dr-eureka/assets/dreureka-paper.pdf)) · ([DrEureka](https://github.com/eureka-research/DrEureka) - eureka-research) ![Star](https://img.shields.io/github/stars/eureka-research/DrEureka.svg?style=social&label=Star)
- [OpenEQA: Embodied Question Answering in the Era of Foundation Models](https://open-eqa.github.io/assets/pdfs/paper.pdf)

	 · ([open-eqa.github](https://open-eqa.github.io/)) · ([open-eqa](https://github.com/facebookresearch/open-eqa) - facebookresearch) ![Star](https://img.shields.io/github/stars/facebookresearch/open-eqa.svg?style=social&label=Star)
- **Yell At Your Robot: Improving On-the-Fly from Language Corrections**, `arXiv, 2403.12910`, [arxiv](http://arxiv.org/abs/2403.12910v1), [pdf](http://arxiv.org/pdf/2403.12910v1.pdf), cication: [**-1**](None)

	 *Lucy Xiaoyang Shi, Zheyuan Hu, Tony Z. Zhao, Archit Sharma, Karl Pertsch, Jianlan Luo, Sergey Levine, Chelsea Finn* · ([qbitai](https://www.qbitai.com/2024/04/133699.html))
- **ShapeGrasp: Zero-Shot Task-Oriented Grasping with Large Language Models
  through Geometric Decomposition**, `arXiv, 2403.18062`, [arxiv](http://arxiv.org/abs/2403.18062v1), [pdf](http://arxiv.org/pdf/2403.18062v1.pdf), cication: [**-1**](None)

	 *Samuel Li, Sarthak Bhagat, Joseph Campbell, Yaqi Xie, Woojun Kim, Katia Sycara, Simon Stepputtis* · ([shapegrasp.github](https://shapegrasp.github.io/))
- **3D-VLA: A 3D Vision-Language-Action Generative World Model**, `arXiv, 2403.09631`, [arxiv](http://arxiv.org/abs/2403.09631v1), [pdf](http://arxiv.org/pdf/2403.09631v1.pdf), cication: [**-1**](None)

	 *Haoyu Zhen, Xiaowen Qiu, Peihao Chen, Jincheng Yang, Xin Yan, Yilun Du, Yining Hong, Chuang Gan*

	 · ([mp.weixin.qq](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652459636&idx=3&sn=702d841f9ad17b4dd42705569bfed8b6))
- **EnvGen: Generating and Adapting Environments via LLMs for Training
  Embodied Agents**, `arXiv, 2403.12014`, [arxiv](http://arxiv.org/abs/2403.12014v1), [pdf](http://arxiv.org/pdf/2403.12014v1.pdf), cication: [**-1**](None)

	 *Abhay Zala, Jaemin Cho, Han Lin, Jaehong Yoon, Mohit Bansal* · ([envgen-llm.github](https://envgen-llm.github.io/))
- [CoPa](https://copa-2024.github.io/)
- [SIMA generalist AI agent for 3D virtual environments - Google DeepMind](https://deepmind.google/discover/blog/sima-generalist-ai-agent-for-3d-virtual-environments/)

	 · ([storage.googleapis](https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/sima-generalist-ai-agent-for-3d-virtual-environments/Scaling%20Instructable%20Agents%20Across%20Many%20Simulated%20Worlds.pdf))
- **RT-H: Action Hierarchies Using Language**, `arXiv, 2403.01823`, [arxiv](http://arxiv.org/abs/2403.01823v1), [pdf](http://arxiv.org/pdf/2403.01823v1.pdf), cication: [**-1**](None)

	 *Suneel Belkhale, Tianli Ding, Ted Xiao, Pierre Sermanet, Quon Vuong, Jonathan Tompson, Yevgen Chebotar, Debidatta Dwibedi, Dorsa Sadigh* · ([rt-hierarchy.github](https://rt-hierarchy.github.io/))
- **Empowering Large Language Model Agents through Action Learning**, `arXiv, 2402.15809`, [arxiv](http://arxiv.org/abs/2402.15809v1), [pdf](http://arxiv.org/pdf/2402.15809v1.pdf), cication: [**-1**](None)

	 *Haiteng Zhao, Chang Ma, Guoyin Wang, Jing Su, Lingpeng Kong, Jingjing Xu, Zhi-Hong Deng, Hongxia Yang*
- **Learning to Learn Faster from Human Feedback with Language Model
  Predictive Control**, `arXiv, 2402.11450`, [arxiv](http://arxiv.org/abs/2402.11450v1), [pdf](http://arxiv.org/pdf/2402.11450v1.pdf), cication: [**-1**](None)

	 *Jacky Liang, Fei Xia, Wenhao Yu, Andy Zeng, Montserrat Gonzalez Arenas, Maria Attarian, Maria Bauza, Matthew Bennice, Alex Bewley, Adil Dostmohamed* · ([robot-teaching.github](https://robot-teaching.github.io/))
- **Generative Expressive Robot Behaviors using Large Language Models**, `arXiv, 2401.14673`, [arxiv](http://arxiv.org/abs/2401.14673v1), [pdf](http://arxiv.org/pdf/2401.14673v1.pdf), cication: [**-1**](None)

	 *Karthik Mahadevan, Jonathan Chien, Noah Brown, Zhuo Xu, Carolina Parada, Fei Xia, Andy Zeng, Leila Takayama, Dorsa Sadigh* · ([generative-expressive-motion.github](https://generative-expressive-motion.github.io/))
- **Adaptive Mobile Manipulation for Articulated Objects In the Open World**, `arXiv, 2401.14403`, [arxiv](http://arxiv.org/abs/2401.14403v1), [pdf](http://arxiv.org/pdf/2401.14403v1.pdf), cication: [**-1**](None)

	 *Haoyu Xiong, Russell Mendonca, Kenneth Shaw, Deepak Pathak* · ([open-world-mobilemanip.github](https://open-world-mobilemanip.github.io))
- **OK-Robot: What Really Matters in Integrating Open-Knowledge Models for
  Robotics**, `arXiv, 2401.12202`, [arxiv](http://arxiv.org/abs/2401.12202v1), [pdf](http://arxiv.org/pdf/2401.12202v1.pdf), cication: [**-1**](None)

	 *Peiqi Liu, Yaswanth Orru, Chris Paxton, Nur Muhammad Mahi Shafiullah, Lerrel Pinto* · ([ok-robot.github](https://ok-robot.github.io))
- [AutoRT: Embodied Foundation Models for Large Scale Orchestration of Robotic Agents](https://auto-rt.github.io/)

	 · ([auto-rt.github](https://auto-rt.github.io/static/pdf/AutoRT.pdf))
- **Toward General-Purpose Robots via Foundation Models: A Survey and
  Meta-Analysis**, `arXiv, 2312.08782`, [arxiv](http://arxiv.org/abs/2312.08782v2), [pdf](http://arxiv.org/pdf/2312.08782v2.pdf), cication: [**-1**](None)

	 *Yafei Hu, Quanting Xie, Vidhi Jain, Jonathan Francis, Jay Patrikar, Nikhil Keetha, Seungchan Kim, Yaqi Xie, Tianyi Zhang, Shibo Zhao*
- **ReST meets ReAct: Self-Improvement for Multi-Step Reasoning LLM Agent**, `arXiv, 2312.10003`, [arxiv](http://arxiv.org/abs/2312.10003v1), [pdf](http://arxiv.org/pdf/2312.10003v1.pdf), cication: [**-1**](None)

	 *Renat Aksitov, Sobhan Miryoosefi, Zonglin Li, Daliang Li, Sheila Babayan, Kavya Kopparapu, Zachary Fisher, Ruiqi Guo, Sushant Prakash, Pranesh Srinivasan*
- **ManipLLM: Embodied Multimodal Large Language Model for Object-Centric
  Robotic Manipulation**, `arXiv, 2312.16217`, [arxiv](http://arxiv.org/abs/2312.16217v1), [pdf](http://arxiv.org/pdf/2312.16217v1.pdf), cication: [**1**](https://scholar.google.com/scholar?cites=7882854002413266821&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Xiaoqi Li, Mingxu Zhang, Yiran Geng, Haoran Geng, Yuxing Long, Yan Shen, Renrui Zhang, Jiaming Liu, Hao Dong* · ([qbitai](https://www.qbitai.com/2024/03/124996.html)) · ([sites.google](https://sites.google.com/view/manipllm))
- **Vision-Language Models as a Source of Rewards**, `arXiv, 2312.09187`, [arxiv](http://arxiv.org/abs/2312.09187v1), [pdf](http://arxiv.org/pdf/2312.09187v1.pdf), cication: [**-1**](None)

	 *Kate Baumli, Satinder Baveja, Feryal Behbahani, Harris Chan, Gheorghe Comanici, Sebastian Flennerhag, Maxime Gazeau, Kristian Holsheimer, Dan Horgan, Michael Laskin*
- **Foundation Models in Robotics: Applications, Challenges, and the Future**, `arXiv, 2312.07843`, [arxiv](http://arxiv.org/abs/2312.07843v1), [pdf](http://arxiv.org/pdf/2312.07843v1.pdf), cication: [**-1**](None)

	 *Roya Firoozi, Johnathan Tucker, Stephen Tian, Anirudha Majumdar, Jiankai Sun, Weiyu Liu, Yuke Zhu, Shuran Song, Ashish Kapoor, Karol Hausman*

	 · ([Awesome-Robotics-Foundation-Models](https://github.com/robotics-survey/Awesome-Robotics-Foundation-Models) - robotics-survey) ![Star](https://img.shields.io/github/stars/robotics-survey/Awesome-Robotics-Foundation-Models.svg?style=social&label=Star)
- **SAGE: Bridging Semantic and Actionable Parts for GEneralizable
  Articulated-Object Manipulation under Language Instructions**, `arXiv, 2312.01307`, [arxiv](http://arxiv.org/abs/2312.01307v1), [pdf](http://arxiv.org/pdf/2312.01307v1.pdf), cication: [**-1**](None)

	 *Haoran Geng, Songlin Wei, Congyue Deng, Bokui Shen, He Wang, Leonidas Guibas* · ([geometry.stanford](https://geometry.stanford.edu/projects/sage/)) · ([SAGE](https://github.com/geng-haoran/SAGE) - geng-haoran) ![Star](https://img.shields.io/github/stars/geng-haoran/SAGE.svg?style=social&label=Star)
- **Agent as Cerebrum, Controller as Cerebellum: Implementing an Embodied
  LMM-based Agent on Drones**, `arXiv, 2311.15033`, [arxiv](http://arxiv.org/abs/2311.15033v1), [pdf](http://arxiv.org/pdf/2311.15033v1.pdf), cication: [**-1**](None)

	 *Haoran Zhao, Fengxing Pan, Huqiuyue Ping, Yaoming Zhou* · ([qbitai](https://www.qbitai.com/2023/12/106007.html))
- **From Text to Motion: Grounding GPT-4 in a Humanoid Robot "Alter3"**, `arXiv, 2312.06571`, [arxiv](http://arxiv.org/abs/2312.06571v1), [pdf](http://arxiv.org/pdf/2312.06571v1.pdf), cication: [**-1**](None)

	 *Takahide Yoshida, Atsushi Masumori, Takashi Ikegami* · ([tnoinkwms.github](https://tnoinkwms.github.io/ALTER-LLM/))
- **Controllable Human-Object Interaction Synthesis**, `arXiv, 2312.03913`, [arxiv](http://arxiv.org/abs/2312.03913v1), [pdf](http://arxiv.org/pdf/2312.03913v1.pdf), cication: [**-1**](None)

	 *Jiaman Li, Alexander Clegg, Roozbeh Mottaghi, Jiajun Wu, Xavier Puig, C. Karen Liu*
- **Generative agent-based modeling with actions grounded in physical,
  social, or digital space using Concordia**, `arXiv, 2312.03664`, [arxiv](http://arxiv.org/abs/2312.03664v1), [pdf](http://arxiv.org/pdf/2312.03664v1.pdf), cication: [**-1**](None)

	 *Alexander Sasha Vezhnevets, John P. Agapiou, Avia Aharon, Ron Ziv, Jayd Matyas, Edgar A. Duéñez-Guzmán, William A. Cunningham, Simon Osindero, Danny Karmon, Joel Z. Leibo*

	 · ([concordia](https://github.com/google-deepmind/concordia) - google-deepmind) ![Star](https://img.shields.io/github/stars/google-deepmind/concordia.svg?style=social&label=Star)
- **Vision-Language Foundation Models as Effective Robot Imitators**, `arXiv, 2311.01378`, [arxiv](http://arxiv.org/abs/2311.01378v2), [pdf](http://arxiv.org/pdf/2311.01378v2.pdf), cication: [**-1**](None)

	 *Xinghang Li, Minghuan Liu, Hanbo Zhang, Cunjun Yu, Jie Xu, Hongtao Wu, Chilam Cheang, Ya Jing, Weinan Zhang, Huaping Liu* · ([RoboFlamingo](https://github.com/RoboFlamingo/RoboFlamingo) - RoboFlamingo) ![Star](https://img.shields.io/github/stars/RoboFlamingo/RoboFlamingo.svg?style=social&label=Star) · ([jiqizhixin](https://www.jiqizhixin.com/articles/2024-01-17-3))
- **GPT-4V(ision) for Robotics: Multimodal Task Planning from Human
  Demonstration**, `arXiv, 2311.12015`, [arxiv](http://arxiv.org/abs/2311.12015v1), [pdf](http://arxiv.org/pdf/2311.12015v1.pdf), cication: [**-1**](None)

	 *Naoki Wake, Atsushi Kanehira, Kazuhiro Sasabuchi, Jun Takamatsu, Katsushi Ikeuchi* · ([microsoft.github](https://microsoft.github.io/GPT4Vision-Robot-Manipulation-Prompts))

	 · ([jiqizhixin](https://www.jiqizhixin.com/articles/2023-12-04-3))
- **Distilling and Retrieving Generalizable Knowledge for Robot Manipulation
  via Language Corrections**, `arXiv, 2311.10678`, [arxiv](http://arxiv.org/abs/2311.10678v1), [pdf](http://arxiv.org/pdf/2311.10678v1.pdf), cication: [**-1**](None)

	 *Lihan Zha, Yuchen Cui, Li-Heng Lin, Minae Kwon, Montserrat Gonzalez Arenas, Andy Zeng, Fei Xia, Dorsa Sadigh*
- **GOAT: GO to Any Thing**, `arXiv, 2311.06430`, [arxiv](http://arxiv.org/abs/2311.06430v1), [pdf](http://arxiv.org/pdf/2311.06430v1.pdf), cication: [**-1**](None)

	 *Matthew Chang, Theophile Gervet, Mukul Khanna, Sriram Yenamandra, Dhruv Shah, So Yeon Min, Kavit Shah, Chris Paxton, Saurabh Gupta, Dhruv Batra*
- **LLaMA Rider: Spurring Large Language Models to Explore the Open World**, arXiv, 2310.08922, [arxiv](http://arxiv.org/abs/2310.08922v1), [pdf](http://arxiv.org/pdf/2310.08922v1.pdf), cication: [**-1**](None)

	 *Yicheng Feng, Yuxuan Wang, Jiazheng Liu, Sipeng Zheng, Zongqing Lu*
	 · [[jiqizhixin](https://www.jiqizhixin.com/articles/2023-11-07-4)]
- **RoboVQA: Multimodal Long-Horizon Reasoning for Robotics**, arXiv, 2311.00899, [arxiv](http://arxiv.org/abs/2311.00899v1), [pdf](http://arxiv.org/pdf/2311.00899v1.pdf), cication: [**-1**](None)

	 *Pierre Sermanet, Tianli Ding, Jeffrey Zhao, Fei Xia, Debidatta Dwibedi, Keerthana Gopalakrishnan, Christine Chan, Gabriel Dulac-Arnold, Sharath Maddineni, Nikhil J Joshi*
- **Unleashing the Power of Pre-trained Language Models for Offline
  Reinforcement Learning**, arXiv, 2310.20587, [arxiv](http://arxiv.org/abs/2310.20587v3), [pdf](http://arxiv.org/pdf/2310.20587v3.pdf), cication: [**-1**](None)

	 *Ruizhe Shi, Yuyao Liu, Yanjie Ze, Simon S. Du, Huazhe Xu*
- **Large Language Models as Generalizable Policies for Embodied Tasks**, arXiv, 2310.17722, [arxiv](http://arxiv.org/abs/2310.17722v1), [pdf](http://arxiv.org/pdf/2310.17722v1.pdf), cication: [**-1**](None)

	 *Andrew Szot, Max Schwarzer, Harsh Agrawal, Bogdan Mazoure, Walter Talbott, Katherine Metcalf, Natalie Mackraz, Devon Hjelm, Alexander Toshev*
- **Creative Robot Tool Use with Large Language Models**, arXiv, 2310.13065, [arxiv](http://arxiv.org/abs/2310.13065v1), [pdf](http://arxiv.org/pdf/2310.13065v1.pdf), cication: [**-1**](None)

	 *Mengdi Xu, Peide Huang, Wenhao Yu, Shiqi Liu, Xilun Zhang, Yaru Niu, Tingnan Zhang, Fei Xia, Jie Tan, Ding Zhao*
- **Vision-Language Models are Zero-Shot Reward Models for Reinforcement
  Learning**, arXiv, 2310.12921, [arxiv](http://arxiv.org/abs/2310.12921v1), [pdf](http://arxiv.org/pdf/2310.12921v1.pdf), cication: [**-1**](None)

	 *Juan Rocamonde, Victoriano Montesinos, Elvis Nava, Ethan Perez, David Lindner*
- **Eureka: Human-Level Reward Design via Coding Large Language Models**, arXiv, 2310.12931, [arxiv](http://arxiv.org/abs/2310.12931v1), [pdf](http://arxiv.org/pdf/2310.12931v1.pdf), cication: [**1**](https://scholar.google.com/scholar?cites=455546167728485817&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Yecheng Jason Ma, William Liang, Guanzhi Wang, De-An Huang, Osbert Bastani, Dinesh Jayaraman, Yuke Zhu, Linxi Fan, Anima Anandkumar*
	 · [[Eureka](https://github.com/eureka-research/Eureka) - eureka-research] ![Star](https://img.shields.io/github/stars/eureka-research/Eureka.svg?style=social&label=Star) · [[qbitai](https://www.qbitai.com/2023/10/91741.html)]
- **Interactive Task Planning with Language Models**, arXiv, 2310.10645, [arxiv](http://arxiv.org/abs/2310.10645v1), [pdf](http://arxiv.org/pdf/2310.10645v1.pdf), cication: [**-1**](None)

	 *Boyi Li, Philipp Wu, Pieter Abbeel, Jitendra Malik*
- **Goal Representations for Instruction Following: A Semi-Supervised
  Language Interface to Control**, arXiv, 2307.00117, [arxiv](http://arxiv.org/abs/2307.00117v2), [pdf](http://arxiv.org/pdf/2307.00117v2.pdf), cication: [**3**](https://scholar.google.com/scholar?cites=9646982711872255783&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Vivek Myers, Andre He, Kuan Fang, Homer Walke, Philippe Hansen-Estruch, Ching-An Cheng, Mihai Jalobeanu, Andrey Kolobov, Anca Dragan, Sergey Levine*
	 · [[bair.berkeley](https://bair.berkeley.edu/blog/2023/10/17/grif/)]
- **Reason for Future, Act for Now: A Principled Framework for Autonomous
  LLM Agents with Provable Sample Efficiency**, arXiv, 2309.17382, [arxiv](http://arxiv.org/abs/2309.17382v2), [pdf](http://arxiv.org/pdf/2309.17382v2.pdf), cication: [**-1**](None)

	 *Zhihan Liu, Hao Hu, Shenao Zhang, Hongyi Guo, Shuqi Ke, Boyi Liu, Zhaoran Wang*
	 · [[RAFA_code](https://github.com/agentification/RAFA_code) - agentification] ![Star](https://img.shields.io/github/stars/agentification/RAFA_code.svg?style=social&label=Star)
- **Video Language Planning**, arXiv, 2310.10625, [arxiv](http://arxiv.org/abs/2310.10625v1), [pdf](http://arxiv.org/pdf/2310.10625v1.pdf), cication: [**-1**](None)

	 *Yilun Du, Mengjiao Yang, Pete Florence, Fei Xia, Ayzaan Wahid, Brian Ichter, Pierre Sermanet, Tianhe Yu, Pieter Abbeel, Joshua B. Tenenbaum*
- **Interactive Task Planning with Language Models**, arXiv, 2310.10645, [arxiv](http://arxiv.org/abs/2310.10645v1), [pdf](http://arxiv.org/pdf/2310.10645v1.pdf), cication: [**-1**](None)

	 *Boyi Li, Philipp Wu, Pieter Abbeel, Jitendra Malik*
- **FireAct: Toward Language Agent Fine-tuning**, arXiv, 2310.05915, [arxiv](http://arxiv.org/abs/2310.05915v1), [pdf](http://arxiv.org/pdf/2310.05915v1.pdf), cication: [**-1**](None)

	 *Baian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier, Karthik Narasimhan, Shunyu Yao*
- **LangNav: Language as a Perceptual Representation for Navigation**, arXiv, 2310.07889, [arxiv](http://arxiv.org/abs/2310.07889v1), [pdf](http://arxiv.org/pdf/2310.07889v1.pdf), cication: [**-1**](None)

	 *Bowen Pan, Rameswar Panda, SouYoung Jin, Rogerio Feris, Aude Oliva, Phillip Isola, Yoon Kim*
- **Learning Interactive Real-World Simulators**, arXiv, 2310.06114, [arxiv](http://arxiv.org/abs/2310.06114v1), [pdf](http://arxiv.org/pdf/2310.06114v1.pdf), cication: [**-1**](None)

	 *Mengjiao Yang, Yilun Du, Kamyar Ghasemipour, Jonathan Tompson, Dale Schuurmans, Pieter Abbeel*
	 · [[universal-simulator.github](https://universal-simulator.github.io/unisim/)]
- **GenSim: Generating Robotic Simulation Tasks via Large Language Models**, arXiv, 2310.01361, [arxiv](http://arxiv.org/abs/2310.01361v1), [pdf](http://arxiv.org/pdf/2310.01361v1.pdf), cication: [**-1**](None)

	 *Lirui Wang, Yiyang Ling, Zhecheng Yuan, Mohit Shridhar, Chen Bao, Yuzhe Qin, Bailin Wang, Huazhe Xu, Xiaolong Wang*
- **A Data Source for Reasoning Embodied Agents**, AAAI, 2023, [arxiv](http://arxiv.org/abs/2309.07974v1), [pdf](http://arxiv.org/pdf/2309.07974v1.pdf), cication: [**1**](https://scholar.google.com/scholar?cites=3832761011527185747&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Jack Lanchantin, Sainbayar Sukhbaatar, Gabriel Synnaeve, Yuxuan Sun, Kavya Srinet, Arthur Szlam*
- **Language Embedded Radiance Fields for Zero-Shot Task-Oriented Grasping**, CoRL, 2023, [arxiv](http://arxiv.org/abs/2309.07970v2), [pdf](http://arxiv.org/pdf/2309.07970v2.pdf), cication: [**3**](https://scholar.google.com/scholar?cites=10160617997986232830&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Adam Rashid, Satvik Sharma, Chung Min Kim, Justin Kerr, Lawrence Chen, Angjoo Kanazawa, Ken Goldberg*
- **Thought Cloning: Learning to Think while Acting by Imitating Human
  Thinking**, arXiv, 2306.00323, [arxiv](http://arxiv.org/abs/2306.00323v2), [pdf](http://arxiv.org/pdf/2306.00323v2.pdf), cication: [**4**](https://scholar.google.com/scholar?cites=10518182798701484533&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Shengran Hu, Jeff Clune*
- **Physically Grounded Vision-Language Models for Robotic Manipulation**, arXiv, 2309.02561, [arxiv](http://arxiv.org/abs/2309.02561v2), [pdf](http://arxiv.org/pdf/2309.02561v2.pdf), cication: [**1**](https://scholar.google.com/scholar?cites=16637425932865130825&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Jensen Gao, Bidipta Sarkar, Fei Xia, Ted Xiao, Jiajun Wu, Brian Ichter, Anirudha Majumdar, Dorsa Sadigh*
- **BEVBert: Multimodal Map Pre-training for Language-guided Navigation**, arXiv, 2212.04385, [arxiv](http://arxiv.org/abs/2212.04385v2), [pdf](http://arxiv.org/pdf/2212.04385v2.pdf), cication: [**11**](https://scholar.google.com/scholar?cites=941742455375698903&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Dong An, Yuankai Qi, Yangguang Li, Yan Huang, Liang Wang, Tieniu Tan, Jing Shao*
	 · [[vln-bevbert](https://github.com/marsaki/vln-bevbert) - marsaki] ![Star](https://img.shields.io/github/stars/marsaki/vln-bevbert.svg?style=social&label=Star)
- **Distilled Feature Fields Enable Few-Shot Language-Guided Manipulation**, arXiv, 2308.07931, [arxiv](http://arxiv.org/abs/2308.07931v1), [pdf](http://arxiv.org/pdf/2308.07931v1.pdf), cication: [**3**](https://scholar.google.com/scholar?cites=2243532271563662553&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *William Shen, Ge Yang, Alan Yu, Jansen Wong, Leslie Pack Kaelbling, Phillip Isola*
	 · [[qbitai](https://www.qbitai.com/2023/08/78033.html)] · [[jiqizhixin](https://www.jiqizhixin.com/articles/2023-08-21-12)]
- **Foundation Model based Open Vocabulary Task Planning and Executive
  System for General Purpose Service Robots**, arXiv, 2308.03357, [arxiv](http://arxiv.org/abs/2308.03357v1), [pdf](http://arxiv.org/pdf/2308.03357v1.pdf), cication: [**1**](https://scholar.google.com/scholar?cites=14423470033929024896&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Yoshiki Obinata, Naoaki Kanazawa, Kento Kawaharazuka, Iori Yanokura, Soonhyo Kim, Kei Okada, Masayuki Inaba*
- **Learning to Model the World with Language**, arXiv, 2308.01399, [arxiv](http://arxiv.org/abs/2308.01399v1), [pdf](http://arxiv.org/pdf/2308.01399v1.pdf), cication: [**4**](https://scholar.google.com/scholar?cites=18079008737666460104&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Jessy Lin, Yuqing Du, Olivia Watkins, Danijar Hafner, Pieter Abbeel, Dan Klein, Anca Dragan*
	 · [[mp.weixin.qq](https://mp.weixin.qq.com/s__biz=MzI1MjQ2OTQ3Ng==&mid=2247612552&idx=1&sn=0cbc4a6937ebbbef7abc8f721079569c)]
- **Alexa, play with robot: Introducing the First Alexa Prize SimBot
  Challenge on Embodied AI**, arXiv, 2308.05221, [arxiv](http://arxiv.org/abs/2308.05221v1), [pdf](http://arxiv.org/pdf/2308.05221v1.pdf), cication: [**1**](https://scholar.google.com/scholar?cites=14963404334913592173&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Hangjie Shi, Leslie Ball, Govind Thattai, Desheng Zhang, Lucy Hu, Qiaozi Gao, Suhaila Shakiah, Xiaofeng Gao, Aishwarya Padmakumar, Bofei Yang*
- **Scaling Up and Distilling Down: Language-Guided Robot Skill Acquisition**, arXiv, 2307.14535, [arxiv](http://arxiv.org/abs/2307.14535v2), [pdf](http://arxiv.org/pdf/2307.14535v2.pdf), cication: [**9**](https://scholar.google.com/scholar?cites=8641471144667957157&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Huy Ha, Pete Florence, Shuran Song*
- [RT-2: Vision-Language-Action Models](https://robotics-transformer2.github.io/)

	 · [[qbitai](https://www.qbitai.com/2023/07/72187.html)] · [[robotics-transformer2.github](https://robotics-transformer2.github.io/assets/rt2.pdf)]
- **Towards A Unified Agent with Foundation Models**, workshop on reincarnating reinforcement learning at iclr 2023, 2023, [arxiv](http://arxiv.org/abs/2307.09668v1), [pdf](http://arxiv.org/pdf/2307.09668v1.pdf), cication: [**9**](https://scholar.google.com/scholar?cites=18117373638079827954&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Norman Di Palo, Arunkumar Byravan, Leonard Hasenclever, Markus Wulfmeier, Nicolas Heess, Martin Riedmiller*

- **SayPlan: Grounding Large Language Models using 3D Scene Graphs for
  Scalable Robot Task Planning**, arXiv, 2307.06135, [arxiv](http://arxiv.org/abs/2307.06135v2), [pdf](http://arxiv.org/pdf/2307.06135v2.pdf), cication: [**13**](https://scholar.google.com/scholar?cites=15144668228497064232&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Krishan Rana, Jesse Haviland, Sourav Garg, Jad Abou-Chakra, Ian Reid, Niko Suenderhauf*
- **VoxPoser: Composable 3D Value Maps for Robotic Manipulation with
  Language Models**, arXiv, 2307.05973, [arxiv](http://arxiv.org/abs/2307.05973v2), [pdf](http://arxiv.org/pdf/2307.05973v2.pdf), cication: [**35**](https://scholar.google.com/scholar?cites=7844573100140075704&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun Wu, Li Fei-Fei*
	 · [[voxposer.github](https://voxposer.github.io/)] · [[mp.weixin.qq](https://mp.weixin.qq.com/s?__biz=MzIyMzk1MDE3Nw==&mid=2247605604&idx=1&sn=afacf517dcc5936de0a8175388ad9dca)]
- **Decomposing the Generalization Gap in Imitation Learning for Visual
  Robotic Manipulation**, arXiv, 2307.03659, [arxiv](http://arxiv.org/abs/2307.03659v1), [pdf](http://arxiv.org/pdf/2307.03659v1.pdf), cication: [**3**](https://scholar.google.com/scholar?cites=5003045834434490468&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Annie Xie, Lisa Lee, Ted Xiao, Chelsea Finn*
- **Robots That Ask For Help: Uncertainty Alignment for Large Language Model
  Planners**, arXiv, 2307.01928, [arxiv](http://arxiv.org/abs/2307.01928v2), [pdf](http://arxiv.org/pdf/2307.01928v2.pdf), cication: [**24**](https://scholar.google.com/scholar?cites=751643712331399494&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Allen Z. Ren, Anushri Dixit, Alexandra Bodrova, Sumeet Singh, Stephen Tu, Noah Brown, Peng Xu, Leila Takayama, Fei Xia, Jake Varley*
- **Building Cooperative Embodied Agents Modularly with Large Language
  Models**, arXiv, 2307.02485, [arxiv](http://arxiv.org/abs/2307.02485v1), [pdf](http://arxiv.org/pdf/2307.02485v1.pdf), cication: [**5**](https://scholar.google.com/scholar?cites=1624045542134367526&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Hongxin Zhang, Weihua Du, Jiaming Shan, Qinhong Zhou, Yilun Du, Joshua B. Tenenbaum, Tianmin Shu, Chuang Gan*
- **ChatGPT for Robotics: Design Principles and Model Abilities**, microsoft auton. syst. robot. res, 2023, [arxiv](http://arxiv.org/abs/2306.17582v2), [pdf](http://arxiv.org/pdf/2306.17582v2.pdf), cication: [**111**](https://scholar.google.com/scholar?cites=3223909598681380499&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Sai Vemprala, Rogerio Bonatti, Arthur Bucker, Ashish Kapoor*
	 · [[PromptCraft-Robotics](https://github.com/microsoft/PromptCraft-Robotics) - microsoft] ![Star](https://img.shields.io/github/stars/microsoft/PromptCraft-Robotics.svg?style=social&label=Star)
- **Statler: State-Maintaining Language Models for Embodied Reasoning**, arXiv, 2306.17840, [arxiv](http://arxiv.org/abs/2306.17840v2), [pdf](http://arxiv.org/pdf/2306.17840v2.pdf), cication: [**5**](https://scholar.google.com/scholar?cites=5225945479617936212&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Takuma Yoneda, Jiading Fang, Peng Li, Huanyu Zhang, Tianchong Jiang, Shengjie Lin, Ben Picker, David Yunis, Hongyuan Mei, Matthew R. Walter*
- **REFLECT: Summarizing Robot Experiences for Failure Explanation and
  Correction**, arXiv, 2306.15724, [arxiv](http://arxiv.org/abs/2306.15724v4), [pdf](http://arxiv.org/pdf/2306.15724v4.pdf), cication: [**13**](https://scholar.google.com/scholar?cites=13568943050059915533&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Zeyi Liu, Arpit Bahety, Shuran Song*
- **ViNT: A Foundation Model for Visual Navigation**, arXiv, 2306.14846, [arxiv](http://arxiv.org/abs/2306.14846v2), [pdf](http://arxiv.org/pdf/2306.14846v2.pdf), cication: [**10**](https://scholar.google.com/scholar?cites=7196741364817045136&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Dhruv Shah, Ajay Sridhar, Nitish Dashora, Kyle Stachowicz, Kevin Black, Noriaki Hirose, Sergey Levine*
	 · [[visualnav-transformer.github](https://visualnav-transformer.github.io)]
- **HomeRobot: Open-Vocabulary Mobile Manipulation**, arXiv, 2306.11565, [arxiv](http://arxiv.org/abs/2306.11565v1), [pdf](http://arxiv.org/pdf/2306.11565v1.pdf), cication: [**5**](https://scholar.google.com/scholar?cites=9955154956387142167&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Sriram Yenamandra, Arun Ramachandran, Karmesh Yadav, Austin Wang, Mukul Khanna, Theophile Gervet, Tsung-Yen Yang, Vidhi Jain, Alexander William Clegg, John Turner*
- **Language to Rewards for Robotic Skill Synthesis**, arXiv, 2306.08647, [arxiv](http://arxiv.org/abs/2306.08647v2), [pdf](http://arxiv.org/pdf/2306.08647v2.pdf), cication: [**27**](https://scholar.google.com/scholar?cites=18126079343828260538&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Wenhao Yu, Nimrod Gileadi, Chuyuan Fu, Sean Kirmani, Kuang-Huei Lee, Montse Gonzalez Arenas, Hao-Tien Lewis Chiang, Tom Erez, Leonard Hasenclever, Jan Humplik*
- **SayTap: Language to Quadrupedal Locomotion**, arXiv, 2306.07580, [arxiv](http://arxiv.org/abs/2306.07580v3), [pdf](http://arxiv.org/pdf/2306.07580v3.pdf), cication: [**4**](https://scholar.google.com/scholar?cites=13823207030350397172&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Yujin Tang, Wenhao Yu, Jie Tan, Heiga Zen, Aleksandra Faust, Tatsuya Harada*
- **ChessGPT: Bridging Policy Learning and Language Modeling**, arXiv, 2306.09200, [arxiv](http://arxiv.org/abs/2306.09200v1), [pdf](http://arxiv.org/pdf/2306.09200v1.pdf), cication: [**2**](https://scholar.google.com/scholar?cites=11832492475153297831&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Xidong Feng, Yicheng Luo, Ziyan Wang, Hongrui Tang, Mengyue Yang, Kun Shao, David Mguni, Yali Du, Jun Wang*
- **Language to Rewards for Robotic Skill Synthesis**, arXiv, 2306.08647, [arxiv](http://arxiv.org/abs/2306.08647v2), [pdf](http://arxiv.org/pdf/2306.08647v2.pdf), cication: [**27**](https://scholar.google.com/scholar?cites=18126079343828260538&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Wenhao Yu, Nimrod Gileadi, Chuyuan Fu, Sean Kirmani, Kuang-Huei Lee, Montse Gonzalez Arenas, Hao-Tien Lewis Chiang, Tom Erez, Leonard Hasenclever, Jan Humplik*
- **Embodied Executable Policy Learning with Language-based Scene
  Summarization**, arXiv, 2306.05696, [arxiv](http://arxiv.org/abs/2306.05696v1), [pdf](http://arxiv.org/pdf/2306.05696v1.pdf), cication: [**-1**](None)

	 *Jielin Qiu, Mengdi Xu, William Han, Seungwhan Moon, Ding Zhao*
- **GPT Models Meet Robotic Applications: Co-Speech Gesturing Chat System**, arXiv, 2306.01741, [arxiv](http://arxiv.org/abs/2306.01741v1), [pdf](http://arxiv.org/pdf/2306.01741v1.pdf), cication: [**-1**](None)

	 *Naoki Wake, Atsushi Kanehira, Kazuhiro Sasabuchi, Jun Takamatsu, Katsushi Ikeuchi*

- **ReAct: Synergizing Reasoning and Acting in Language Models**, `arXiv, 2210.03629`, [arxiv](http://arxiv.org/abs/2210.03629v3), [pdf](http://arxiv.org/pdf/2210.03629v3.pdf), cication: [**293**](https://scholar.google.com/scholar?cites=15164492138064021676&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao*

## Robotics
- **ACE: A Cross-Platform Visual-Exoskeletons System for Low-Cost Dexterous
  Teleoperation**, `arXiv, 2408.11805`, [arxiv](http://arxiv.org/abs/2408.11805v1), [pdf](http://arxiv.org/pdf/2408.11805v1.pdf), cication: [**-1**](None)

	 *Shiqi Yang, Minghuan Liu, Yuzhe Qin, Runyu Ding, Jialong Li, Xuxin Cheng, Ruihan Yang, Sha Yi, Xiaolong Wang* · ([ACETeleop](https://github.com/ACETeleop/ACETeleop) - ACETeleop) ![Star](https://img.shields.io/github/stars/ACETeleop/ACETeleop.svg?style=social&label=Star) · ([ace-teleop.github](https://ace-teleop.github.io/))
- **Berkeley Humanoid: A Research Platform for Learning-based Control**, `arXiv, 2407.21781`, [arxiv](http://arxiv.org/abs/2407.21781v1), [pdf](http://arxiv.org/pdf/2407.21781v1.pdf), cication: [**-1**](None)

	 *Qiayuan Liao, Bike Zhang, Xuanyu Huang, Xiaoyu Huang, Zhongyu Li, Koushil Sreenath* · ([berkeley-humanoid](https://berkeley-humanoid.com/))
- **Theia: Distilling Diverse Vision Foundation Models for Robot Learning**, `arXiv, 2407.20179`, [arxiv](http://arxiv.org/abs/2407.20179v1), [pdf](http://arxiv.org/pdf/2407.20179v1.pdf), cication: [**-1**](None)

	 *Jinghuan Shang, Karl Schmeckpeper, Brandon B. May, Maria Vittoria Minniti, Tarik Kelestemur, David Watkins, Laura Herlant* · ([theia.theaiinstitute](http://theia.theaiinstitute.com/))
- **Achieving Human Level Competitive Robot Table Tennis**, `arXiv, 2408.03906`, [arxiv](http://arxiv.org/abs/2408.03906v1), [pdf](http://arxiv.org/pdf/2408.03906v1.pdf), cication: [**-1**](None)

	 *David B. D'Ambrosio, Saminda Abeyruwan, Laura Graesser, Atil Iscen, Heni Ben Amor, Alex Bewley, Barney J. Reed, Krista Reymann, Leila Takayama, Yuval Tassa* · ([sites.google](https://sites.google.com/view/competitive-robot-table-tennis))
- [Berkeley Humanoid: A Research Platform for Learning-based Control](https://berkeley-humanoid.com/)
- **Theia: Distilling Diverse Vision Foundation Models for Robot Learning**, `arXiv, 2407.20179`, [arxiv](http://arxiv.org/abs/2407.20179v1), [pdf](http://arxiv.org/pdf/2407.20179v1.pdf), cication: [**-1**](None)

	 *Jinghuan Shang, Karl Schmeckpeper, Brandon B. May, Maria Vittoria Minniti, Tarik Kelestemur, David Watkins, Laura Herlant*
- **Cross Anything: General Quadruped Robot Navigation through Complex
  Terrains**, `arXiv, 2407.16412`, [arxiv](http://arxiv.org/abs/2407.16412v1), [pdf](http://arxiv.org/pdf/2407.16412v1.pdf), cication: [**-1**](None)

	 *Shaoting Zhu, Derun Li, Yong Liu, Ningyi Xu, Hang Zhao* · ([cross-anything.github](https://cross-anything.github.io/))
- [robocasa\_rss24.pdf](https://robocasa.ai/assets/robocasa_rss24.pdf)

	 · ([robocasa](https://robocasa.ai/))
- **RoboDreamer: Learning Compositional World Models for Robot Imagination**, `arXiv, 2404.12377`, [arxiv](http://arxiv.org/abs/2404.12377v1), [pdf](http://arxiv.org/pdf/2404.12377v1.pdf), cication: [**-1**](None)

	 *Siyuan Zhou, Yilun Du, Jiaben Chen, Yandong Li, Dit-Yan Yeung, Chuang Gan* · ([robovideo.github](https://robovideo.github.io/))
- **TRANSIC: Sim-to-Real Policy Transfer by Learning from Online Correction**, `arXiv, 2405.10315`, [arxiv](http://arxiv.org/abs/2405.10315v1), [pdf](http://arxiv.org/pdf/2405.10315v1.pdf), cication: [**-1**](None)

	 *Yunfan Jiang, Chen Wang, Ruohan Zhang, Jiajun Wu, Li Fei-Fei* · ([transic-robot.github](https://transic-robot.github.io/))
- [**lerobot**](https://github.com/huggingface/lerobot) - huggingface ![Star](https://img.shields.io/github/stars/huggingface/lerobot.svg?style=social&label=Star)
- **Learning H-Infinity Locomotion Control**, `arXiv, 2404.14405`, [arxiv](http://arxiv.org/abs/2404.14405v1), [pdf](http://arxiv.org/pdf/2404.14405v1.pdf), cication: [**-1**](None)

	 *Junfeng Long, Wenye Yu, Quanyi Li, Zirui Wang, Dahua Lin, Jiangmiao Pang*
- **Body Design and Gait Generation of Chair-Type Asymmetrical Tripedal
  Low-rigidity Robot**, `arXiv, 2404.05932`, [arxiv](http://arxiv.org/abs/2404.05932v1), [pdf](http://arxiv.org/pdf/2404.05932v1.pdf), cication: [**-1**](None)

	 *Shintaro Inoue, Kento Kawaharazuka, Kei Okada, Masayuki Inaba* · ([shin0805.github](https://shin0805.github.io/chair-type-tripedal-robot/))
- [Vid2Robot: End-to-end Video-conditioned Policy Learning with Cross-Attention Transformers](https://vid2robot.github.io/)

	 · ([vid2robot.github](https://vid2robot.github.io/vid2robot.pdf))
- **DROID: A Large-Scale In-The-Wild Robot Manipulation Dataset**, `arXiv, 2403.12945`, [arxiv](http://arxiv.org/abs/2403.12945v1), [pdf](http://arxiv.org/pdf/2403.12945v1.pdf), cication: [**-1**](None)

	 *Alexander Khazatsky, Karl Pertsch, Suraj Nair, Ashwin Balakrishna, Sudeep Dasari, Siddharth Karamcheti, Soroush Nasiriany, Mohan Kumar Srirama, Lawrence Yunliang Chen, Kirsty Ellis* · ([droid-dataset.github](https://droid-dataset.github.io/))
- **DexCap: Scalable and Portable Mocap Data Collection System for Dexterous
  Manipulation**, `arXiv, 2403.07788`, [arxiv](http://arxiv.org/abs/2403.07788v1), [pdf](http://arxiv.org/pdf/2403.07788v1.pdf), cication: [**-1**](None)

	 *Chen Wang, Haochen Shi, Weizhuo Wang, Ruohan Zhang, Li Fei-Fei, C. Karen Liu*
- [DexCap | Scalable and Portable Mocap Data Collection System for Dexterous Manipulation](https://dex-cap.github.io/)

	 · ([DexCap](https://github.com/j96w/DexCap) - j96w) ![Star](https://img.shields.io/github/stars/j96w/DexCap.svg?style=social&label=Star)
- **Learning Generalizable Feature Fields for Mobile Manipulation**, `arXiv, 2403.07563`, [arxiv](http://arxiv.org/abs/2403.07563v1), [pdf](http://arxiv.org/pdf/2403.07563v1.pdf), cication: [**-1**](None)

	 *Ri-Zhao Qiu, Yafei Hu, Ge Yang, Yuchen Song, Yang Fu, Jianglong Ye, Jiteng Mu, Ruihan Yang, Nikolay Atanasov, Sebastian Scherer*
- **Humanoid Locomotion as Next Token Prediction**, `arXiv, 2402.19469`, [arxiv](http://arxiv.org/abs/2402.19469v1), [pdf](http://arxiv.org/pdf/2402.19469v1.pdf), cication: [**1**](https://scholar.google.com/scholar?cites=10824280382918994834&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Ilija Radosavovic, Bike Zhang, Baifeng Shi, Jathushan Rajasegaran, Sarthak Kamat, Trevor Darrell, Koushil Sreenath, Jitendra Malik* · ([mp.weixin.qq](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652460909&idx=4&sn=35870451c2c9c90bbbfc7be2bebe75d5))
### Demos
- [Reachy-1’s hardware is open-source ](https://x.com/Thom_Wolf/status/1799008162772836355)
- [Dino Robotics showed off a new video of its robot chef making schnitzel](https://x.com/adcock_brett/status/1797297988567449675)
- [Robot dogs armed with AI-aimed rifles undergo US Marines Special Ops evaluation](https://arstechnica.com/gadgets/2024/05/robot-dogs-armed-with-ai-targeting-rifles-undergo-us-marines-special-ops-evaluation/)
- [twitter.com/heyBarsee/status/1788222961989701878](https://twitter.com/heyBarsee/status/1788222961989701878)
- [Barsee 🐶 on X: "4/ Hong Kong AI and robotics scientists create robots that can "swarm" to work collectively and achieve tasks that would be impossible individually. https://t.co/KOisEyCu4D" / X](https://twitter.com/heyBarsee/status/1788222902539546720)
- [**low_cost_robot**](https://github.com/AlexanderKoch-Koch/low_cost_robot) - AlexanderKoch-Koch ![Star](https://img.shields.io/github/stars/AlexanderKoch-Koch/low_cost_robot.svg?style=social&label=Star)
- [twitter.com/\_akhaliq/status/1770108427735314813](https://twitter.com/_akhaliq/status/1770108427735314813)
- [Barsee 🐶 on X: "This year is going to be the year of robotics 🤖 UC San Diego introduces Expressive Humanoid. Now robots can easily perform rich, diverse, and expressive motions in the real world. A THREAD: 👇🧵 https://t.co/sHlVnoOjhL" / X](https://twitter.com/heyBarsee/status/1770110514045104460)

## Projects

- [**Co-LLM-Agents**](https://github.com/UMass-Foundation-Model/Co-LLM-Agents) - UMass-Foundation-Model ![Star](https://img.shields.io/github/stars/UMass-Foundation-Model/Co-LLM-Agents.svg?style=social&label=Star)

	 *Source codes for the paper "Building Cooperative Embodied Agents Modularly with Large Language Models"*
	 · [[qbitai](https://www.qbitai.com/2023/07/69054.html)]

## Other
- [Scaling robotics datasets with video encoding](https://huggingface.co/blog/video-encoding)
- [HumanPlus and MobileAloha: LeRobot Tech Talk #6 by Zipeng Fu - YouTube](https://www.youtube.com/watch?v=tPz0gc6f-W8)
- [The Road to Embodied AI - Wayve](https://wayve.ai/thinking/road-to-embodied-ai/)
- [Pollen-Vision: Unified interface for Zero-Shot vision models in robotics](https://huggingface.co/blog/pollen-vision)
- [A team trained Mistral 7B playing DOOM](https://twitter.com/reach_vb/status/1772008460122509525)
- talk: [Foundation Agent: a roadmap to build generally capable embodied AI that acts skillfully across many worlds, virtual or real.](https://twitter.com/DrJimFan/status/1770848955519107345)
- [Covariant | Powering the Future of Automation, Today](https://covariant.ai/insights/introducing-rfm-1-giving-robots-human-like-reasoning-capabilities/)
- [ glimpse of the future foundation models and platforms for robots:](https://twitter.com/DrJimFan/status/1740041712184246314)

- [具身智能的Scaling Law在哪里？](https://mp.weixin.qq.com/s?__biz=MzAxMTk4NDkwNw==&mid=2247495674&idx=1&sn=c80af6006fcc17fe751707fabfa8ccb5)
- [一文读懂具身智能：方法、进展及挑战](https://mp.weixin.qq.com/s?__biz=MzI1MjQ2OTQ3Ng==&mid=2247627902&idx=1&sn=9e68f399edf61e25fd766f8be74c016e)
- [仅用7500条轨迹数据训练，CMU、Meta就让机器人「上得厅堂、下得厨房」 | 机器之心](https://www.jiqizhixin.com/articles/2023-08-22-5)
- [EmbodiedGPT｜具身智能或将成为实现AGI的最后一公里](https://mp.weixin.qq.com/s?__biz=MzAxMTk4NDkwNw==&mid=2247494040&idx=1&sn=c6ada8db1bdf84a1026fd78a636b27c5)

## Reference
- [**Awesome-Robotics-3D**](https://github.com/zubair-irshad/Awesome-Robotics-3D) - zubair-irshad ![Star](https://img.shields.io/github/stars/zubair-irshad/Awesome-Robotics-3D.svg?style=social&label=Star)

	 *A curated list of 3D Vision papers relating to Robotics domain in the era of large models i.e. LLMs/VLMs, inspired by awesome-computer-vision, including papers, codes, and related websites*
- [**Awesome-Robotics-Foundation-Models**](https://github.com/robotics-survey/Awesome-Robotics-Foundation-Models) - robotics-survey ![Star](https://img.shields.io/github/stars/robotics-survey/Awesome-Robotics-Foundation-Models.svg?style=social&label=Star)