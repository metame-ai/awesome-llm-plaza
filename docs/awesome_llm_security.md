#  Awesome llm security

- [Awesome llm security](#awesome-llm-security)
	- [Survey](#survey)
	- [Papers](#papers)
	- [Projects](#projects)
	- [Other](#other)
	- [Extra reference](#extra-reference)


## Survey
- **Open-Sourcing Highly Capable Foundation Models: An evaluation of risks,
  benefits, and alternative methods for pursuing open-source objectives**, `arXiv, 2311.09227`, [arxiv](http://arxiv.org/abs/2311.09227v1), [pdf](http://arxiv.org/pdf/2311.09227v1.pdf), cication: [**-1**](None)

	 *Elizabeth Seger, Noemi Dreksler, Richard Moulange, Emily Dardaman, Jonas Schuett, K. Wei, Christoph Winter, Mackenzie Arnold, Seán Ó hÉigeartaigh, Anton Korinek*
- [**Adversarial Attacks on LLMs | Lil'Log**](https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/)

## Papers
- **DeepInception: Hypnotize Large Language Model to Be Jailbreaker**, `arXiv, 2311.03191`, [arxiv](http://arxiv.org/abs/2311.03191v1), [pdf](http://arxiv.org/pdf/2311.03191v1.pdf), cication: [**-1**](None)

	 *Xuan Li, Zhanke Zhou, Jianing Zhu, Jiangchao Yao, Tongliang Liu, Bo Han* · ([DeepInception](https://github.com/tmlr-group/DeepInception) - tmlr-group) ![Star](https://img.shields.io/github/stars/tmlr-group/DeepInception.svg?style=social&label=Star) · ([deepinception.github](https://deepinception.github.io/))
- **Removing RLHF Protections in GPT-4 via Fine-Tuning**, `arXiv, 2311.05553`, [arxiv](http://arxiv.org/abs/2311.05553v2), [pdf](http://arxiv.org/pdf/2311.05553v2.pdf), cication: [**-1**](None)

	 *Qiusi Zhan, Richard Fang, Rohan Bindu, Akul Gupta, Tatsunori Hashimoto, Daniel Kang* · ([mp.weixin.qq](https://mp.weixin.qq.com/s?__biz=Mzg4OTEwNjMzMA==&mid=2247534723&idx=3&sn=5d812e7f65334a9dbbdaf052c21a58ce))
- **Frontier Language Models are not Robust to Adversarial Arithmetic, or
  "What do I need to say so you agree 2+2=5?**, `arXiv, 2311.07587`, [arxiv](http://arxiv.org/abs/2311.07587v1), [pdf](http://arxiv.org/pdf/2311.07587v1.pdf), cication: [**-1**](None)

	 *C. Daniel Freeman, Laura Culp, Aaron Parisi, Maxwell L Bileschi, Gamaleldin F Elsayed, Alex Rizkowsky, Isabelle Simpson, Alex Alemi, Azade Nova, Ben Adlam*
- **Unveiling Safety Vulnerabilities of Large Language Models**, `arXiv, 2311.04124`, [arxiv](http://arxiv.org/abs/2311.04124v1), [pdf](http://arxiv.org/pdf/2311.04124v1.pdf), cication: [**-1**](None)

	 *George Kour, Marcel Zalmanovici, Naama Zwerdling, Esther Goldbraich, Ora Nova Fandina, Ateret Anaby-Tavor, Orna Raz, Eitan Farchi*
- **Managing AI Risks in an Era of Rapid Progress**, `arXiv, 2310.17688`, [arxiv](http://arxiv.org/abs/2310.17688v1), [pdf](http://arxiv.org/pdf/2310.17688v1.pdf), cication: [**3**](https://scholar.google.com/scholar?cites=8498230073239105680&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Yoshua Bengio, Geoffrey Hinton, Andrew Yao, Dawn Song, Pieter Abbeel, Yuval Noah Harari, Ya-Qin Zhang, Lan Xue, Shai Shalev-Shwartz, Gillian Hadfield* · ([managing-ai-risks](https://managing-ai-risks.com/))
- **Jailbreaking Black Box Large Language Models in Twenty Queries**, `arXiv, 2310.08419`, [arxiv](http://arxiv.org/abs/2310.08419v2), [pdf](http://arxiv.org/pdf/2310.08419v2.pdf), cication: [**3**](https://scholar.google.com/scholar?cites=1618697255803227028&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George J. Pappas, Eric Wong* · ([qbitai](https://www.qbitai.com/2023/11/95723.html))
- **How Robust is Google's Bard to Adversarial Image Attacks?**, `arXiv, 2309.11751`, [arxiv](http://arxiv.org/abs/2309.11751v2), [pdf](http://arxiv.org/pdf/2309.11751v2.pdf), cication: [**1**](https://scholar.google.com/scholar?cites=10841564815936420494&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Yinpeng Dong, Huanran Chen, Jiawei Chen, Zhengwei Fang, Xiao Yang, Yichi Zhang, Yu Tian, Hang Su, Jun Zhu* · ([jiqizhixin](https://www.jiqizhixin.com/articles/2023-10-17-5))
- **Fine-tuning Aligned Language Models Compromises Safety, Even When Users
  Do Not Intend To!**, `arXiv, 2310.03693`, [arxiv](http://arxiv.org/abs/2310.03693v1), [pdf](http://arxiv.org/pdf/2310.03693v1.pdf), cication: [**3**](https://scholar.google.com/scholar?cites=14332933435628840179&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, Peter Henderson* · ([mp.weixin.qq](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652389679&idx=4&sn=a8860988614a66fab78392cbdbbd65fc&poc_token=HKlELGWjp2JYi7j0UAJ9dNwBEC2cxSIKZST7h0YT))
- **GPTFUZZER: Red Teaming Large Language Models with Auto-Generated
  Jailbreak Prompts**, `arXiv, 2309.10253`, [arxiv](http://arxiv.org/abs/2309.10253v2), [pdf](http://arxiv.org/pdf/2309.10253v2.pdf), cication: [**5**](https://scholar.google.com/scholar?cites=734711173091882842&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Jiahao Yu, Xingwei Lin, Zheng Yu, Xinyu Xing* · ([gptfuzz](https://github.com/sherdencooper/gptfuzz) - sherdencooper) ![Star](https://img.shields.io/github/stars/sherdencooper/gptfuzz.svg?style=social&label=Star)
- **"Do Anything Now": Characterizing and Evaluating In-The-Wild Jailbreak
  Prompts on Large Language Models**, `arXiv, 2308.03825`, [arxiv](http://arxiv.org/abs/2308.03825v1), [pdf](http://arxiv.org/pdf/2308.03825v1.pdf), cication: [**25**](https://scholar.google.com/scholar?cites=2730450320382131594&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Xinyue Shen, Zeyuan Chen, Michael Backes, Yun Shen, Yang Zhang*
- **MasterKey: Automated Jailbreak Across Multiple Large Language Model
  Chatbots**, `arXiv, 2307.08715`, [arxiv](http://arxiv.org/abs/2307.08715v2), [pdf](http://arxiv.org/pdf/2307.08715v2.pdf), cication: [**13**](https://scholar.google.com/scholar?cites=3831799754403709253&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Gelei Deng, Yi Liu, Yuekang Li, Kailong Wang, Ying Zhang, Zefeng Li, Haoyu Wang, Tianwei Zhang, Yang Liu* · ([qbitai](https://www.qbitai.com/2023/11/97487.html))
- **Universal and Transferable Adversarial Attacks on Aligned Language
  Models**, `arXiv, 2307.15043`, [arxiv](http://arxiv.org/abs/2307.15043v1), [pdf](http://arxiv.org/pdf/2307.15043v1.pdf), cication: [**58**](https://scholar.google.com/scholar?cites=18196466655097322708&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Andy Zou, Zifan Wang, J. Zico Kolter, Matt Fredrikson* · ([llm-attacks](https://github.com/llm-attacks/llm-attacks) - llm-attacks) ![Star](https://img.shields.io/github/stars/llm-attacks/llm-attacks.svg?style=social&label=Star) · ([qbitai](https://www.qbitai.com/2023/07/72014.html))
- **PUMA: Secure Inference of LLaMA-7B in Five Minutes**, `arXiv, 2307.12533`, [arxiv](http://arxiv.org/abs/2307.12533v3), [pdf](http://arxiv.org/pdf/2307.12533v3.pdf), cication: [**3**](https://scholar.google.com/scholar?cites=15180019305746166242&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Ye Dong, Wen-jie Lu, Yancheng Zheng, Haoqi Wu, Derun Zhao, Jin Tan, Zhicong Huang, Cheng Hong, Tao Wei, Wenguang Chen*
- **International Institutions for Advanced AI**, `arXiv, 2307.04699`, [arxiv](http://arxiv.org/abs/2307.04699v2), [pdf](http://arxiv.org/pdf/2307.04699v2.pdf), cication: [**9**](https://scholar.google.com/scholar?cites=1522776054133119216&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Lewis Ho, Joslyn Barnhart, Robert Trager, Yoshua Bengio, Miles Brundage, Allison Carnegie, Rumman Chowdhury, Allan Dafoe, Gillian Hadfield, Margaret Levi*
- **An Overview of Catastrophic AI Risks**, `arXiv, 2306.12001`, [arxiv](http://arxiv.org/abs/2306.12001v6), [pdf](http://arxiv.org/pdf/2306.12001v6.pdf), cication: [**20**](https://scholar.google.com/scholar?cites=13657604724019544721&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Dan Hendrycks, Mantas Mazeika, Thomas Woodside* · ([mp.weixin.qq](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652349067&idx=5&sn=8fabac5f652ec41a0dfe86a078def978))
- **Jailbroken: How Does LLM Safety Training Fail?**, `arXiv, 2307.02483`, [arxiv](http://arxiv.org/abs/2307.02483v1), [pdf](http://arxiv.org/pdf/2307.02483v1.pdf), cication: [**54**](https://scholar.google.com/scholar?cites=14029412962367612376&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Alexander Wei, Nika Haghtalab, Jacob Steinhardt*
- **PromptBench: Towards Evaluating the Robustness of Large Language Models
  on Adversarial Prompts**, `arXiv, 2306.04528`, [arxiv](http://arxiv.org/abs/2306.04528v4), [pdf](http://arxiv.org/pdf/2306.04528v4.pdf), cication: [**32**](https://scholar.google.com/scholar?cites=6727691362756502405&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong Wang, Linyi Yang, Wei Ye, Yue Zhang, Neil Zhenqiang Gong*
## Projects
- [**ai-exploits**](https://github.com/protectai/ai-exploits) - protectai ![Star](https://img.shields.io/github/stars/protectai/ai-exploits.svg?style=social&label=Star)

	 *A collection of real world AI/ML exploits for responsibly disclosed vulnerabilities*
- [**chatgpt_system_prompt**](https://github.com/LouisShark/chatgpt_system_prompt) - LouisShark ![Star](https://img.shields.io/github/stars/LouisShark/chatgpt_system_prompt.svg?style=social&label=Star)

	 *store all chatgpt's system prompt*
- [**cipherchat**](https://github.com/robustnlp/cipherchat) - robustnlp ![Star](https://img.shields.io/github/stars/robustnlp/cipherchat.svg?style=social&label=Star)

	 *A framework to evaluate the generalization capability of safety alignment for LLMs*

## Other
- [ChatGPT最近被微软内部禁用！GPTs新bug：数据两句话就能套走 | 量子位](https://www.qbitai.com/2023/11/97309.html)
- [一段话让模型自曝「系统提示词」！ChatGPT、Bing无一幸免 | 量子位](https://www.qbitai.com/2023/10/91673.html)
- [一段乱码，竟让ChatGPT越狱！乱序prompt让LLM火速生成勒索软件，Jim Fan惊了](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652367811&idx=4&sn=5737319e04c9ea80a506854350c438fa)
- [fast.ai - AI Safety and the Age of Dislightenment](https://www.fast.ai/posts/2023-11-07-dislightenment.html)
- [PoisonGPT: How We Hid a Lobotomized LLM on Hugging Face to Spread Fake News](https://blog.mithrilsecurity.io/poisongpt-how-we-hid-a-lobotomized-llm-on-hugging-face-to-spread-fake-news/)

	 · ([mp.weixin.qq](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652349939&idx=3&sn=54b2ca4d0ddf092c8883ad3625e13dd8))

## Extra reference