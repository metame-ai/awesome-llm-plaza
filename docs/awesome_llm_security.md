#  Awesome llm security

- [Awesome llm security](#awesome-llm-security)
	- [Survey](#survey)
	- [Papers](#papers)
	- [Projects](#projects)
	- [Other](#other)
	- [Extra reference](#extra-reference)


## Survey
- **Open-Sourcing Highly Capable Foundation Models: An evaluation of risks,
  benefits, and alternative methods for pursuing open-source objectives**, `arXiv, 2311.09227`, [arxiv](http://arxiv.org/abs/2311.09227v1), [pdf](http://arxiv.org/pdf/2311.09227v1.pdf), cication: [**-1**](None)

	 *Elizabeth Seger, Noemi Dreksler, Richard Moulange, Emily Dardaman, Jonas Schuett, K. Wei, Christoph Winter, Mackenzie Arnold, Seán Ó hÉigeartaigh, Anton Korinek*
- [**Adversarial Attacks on LLMs | Lil'Log**](https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/)

## Papers
- **How Easy is It to Fool Your Multimodal LLMs? An Empirical Analysis on
  Deceptive Prompts**, `arXiv, 2402.13220`, [arxiv](http://arxiv.org/abs/2402.13220v1), [pdf](http://arxiv.org/pdf/2402.13220v1.pdf), cication: [**-1**](None)

	 *Yusu Qian, Haotian Zhang, Yinfei Yang, Zhe Gan*
- **LLM Agents can Autonomously Hack Websites**, `arXiv, 2402.06664`, [arxiv](http://arxiv.org/abs/2402.06664v3), [pdf](http://arxiv.org/pdf/2402.06664v3.pdf), cication: [**-1**](None)

	 *Richard Fang, Rohan Bindu, Akul Gupta, Qiusi Zhan, Daniel Kang*
- **Robust Prompt Optimization for Defending Language Models Against
  Jailbreaking Attacks**, `arXiv, 2401.17263`, [arxiv](http://arxiv.org/abs/2401.17263v1), [pdf](http://arxiv.org/pdf/2401.17263v1.pdf), cication: [**-1**](None)

	 *Andy Zhou, Bo Li, Haohan Wang* · ([rpo](https://github.com/andyz245/rpo) - andyz245) ![Star](https://img.shields.io/github/stars/andyz245/rpo.svg?style=social&label=Star)
- **A Cross-Language Investigation into Jailbreak Attacks in Large Language
  Models**, `arXiv, 2401.16765`, [arxiv](http://arxiv.org/abs/2401.16765v1), [pdf](http://arxiv.org/pdf/2401.16765v1.pdf), cication: [**-1**](None)

	 *Jie Li, Yi Liu, Chongyang Liu, Ling Shi, Xiaoning Ren, Yaowen Zheng, Yang Liu, Yinxing Xue*
- **Weak-to-Strong Jailbreaking on Large Language Models**, `arXiv, 2401.17256`, [arxiv](http://arxiv.org/abs/2401.17256v1), [pdf](http://arxiv.org/pdf/2401.17256v1.pdf), cication: [**-1**](None)

	 *Xuandong Zhao, Xianjun Yang, Tianyu Pang, Chao Du, Lei Li, Yu-Xiang Wang, William Yang Wang* · ([weak-to-strong](https://github.com/XuandongZhao/weak-to-strong) - XuandongZhao) ![Star](https://img.shields.io/github/stars/XuandongZhao/weak-to-strong.svg?style=social&label=Star)
- **Red Teaming Visual Language Models**, `arXiv, 2401.12915`, [arxiv](http://arxiv.org/abs/2401.12915v1), [pdf](http://arxiv.org/pdf/2401.12915v1.pdf), cication: [**-1**](None)

	 *Mukai Li, Lei Li, Yuwei Yin, Masood Ahmed, Zhenguang Liu, Qi Liu*
- **AttackEval: How to Evaluate the Effectiveness of Jailbreak Attacking on
  Large Language Models**, `arXiv, 2401.09002`, [arxiv](http://arxiv.org/abs/2401.09002v1), [pdf](http://arxiv.org/pdf/2401.09002v1.pdf), cication: [**-1**](None)

	 *Dong shu, Mingyu Jin, Suiyuan Zhu, Beichen Wang, Zihao Zhou, Chong Zhang, Yongfeng Zhang*
- **Open the Pandora's Box of LLMs: Jailbreaking LLMs through Representation
  Engineering**, `arXiv, 2401.06824`, [arxiv](http://arxiv.org/abs/2401.06824v1), [pdf](http://arxiv.org/pdf/2401.06824v1.pdf), cication: [**-1**](None)

	 *Tianlong Li, Xiaoqing Zheng, Xuanjing Huang*
- **How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to
  Challenge AI Safety by Humanizing LLMs**, `arXiv, 2401.06373`, [arxiv](http://arxiv.org/abs/2401.06373v1), [pdf](http://arxiv.org/pdf/2401.06373v1.pdf), cication: [**-1**](None)

	 *Yi Zeng, Hongpeng Lin, Jingwen Zhang, Diyi Yang, Ruoxi Jia, Weiyan Shi* · ([chats-lab.github](https://chats-lab.github.io/persuasive_jailbreaker/)) · ([persuasive_jailbreaker](https://github.com/CHATS-lab/persuasive_jailbreaker) - CHATS-lab) ![Star](https://img.shields.io/github/stars/CHATS-lab/persuasive_jailbreaker.svg?style=social&label=Star)
- **Sleeper Agents: Training Deceptive LLMs that Persist Through Safety
  Training**, `arXiv, 2401.05566`, [arxiv](http://arxiv.org/abs/2401.05566v1), [pdf](http://arxiv.org/pdf/2401.05566v1.pdf), cication: [**-1**](None)

	 *Evan Hubinger, Carson Denison, Jesse Mu, Mike Lambert, Meg Tong, Monte MacDiarmid, Tamera Lanham, Daniel M. Ziegler, Tim Maxwell, Newton Cheng*

	 · ([qbitai](https://www.qbitai.com/2024/01/113745.html))
- **Exploiting Novel GPT-4 APIs**, `arXiv, 2312.14302`, [arxiv](http://arxiv.org/abs/2312.14302v1), [pdf](http://arxiv.org/pdf/2312.14302v1.pdf), cication: [**-1**](None)

	 *Kellin Pelrine, Mohammad Taufeeque, Michał Zając, Euan McLean, Adam Gleave*
	- 
- [**adversarial-random-search-gpt4**](https://github.com/max-andr/adversarial-random-search-gpt4) - max-andr ![Star](https://img.shields.io/github/stars/max-andr/adversarial-random-search-gpt4.svg?style=social&label=Star)

	 *Adversarial Attacks on GPT-4 via Simple Random Search [Dec 2023]* · ([andriushchenko](https://www.andriushchenko.me/gpt4adv.pdf))
- **Control Risk for Potential Misuse of Artificial Intelligence in Science**, `arXiv, 2312.06632`, [arxiv](http://arxiv.org/abs/2312.06632v1), [pdf](http://arxiv.org/pdf/2312.06632v1.pdf), cication: [**-1**](None)

	 *Jiyan He, Weitao Feng, Yaosen Min, Jingwei Yi, Kunsheng Tang, Shuai Li, Jie Zhang, Kejiang Chen, Wenbo Zhou, Xing Xie* · ([mp.weixin.qq](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652419855&idx=4&sn=21f7f3da5aedff2af360057d555eb29f))
- **Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations**, `arXiv, 2312.06674`, [arxiv](http://arxiv.org/abs/2312.06674v1), [pdf](http://arxiv.org/pdf/2312.06674v1.pdf), cication: [**-1**](None)

	 *Hakan Inan, Kartikeya Upasani, Jianfeng Chi, Rashi Rungta, Krithika Iyer, Yuning Mao, Michael Tontchev, Qing Hu, Brian Fuller, Davide Testuggine*

	 · ([ai.meta](https://ai.meta.com/research/publications/llama-guard-llm-based-input-output-safeguard-for-human-ai-conversations/))
	 · ([pdf](https://scontent-xsp1-3.xx.fbcdn.net/v/t39.2365-6/408725049_3688557441468029_8103913771964668529_n.pdf?_nc_cat=100&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=_gWcFD1K96gAX_s7udO&_nc_ht=scontent-xsp1-3.xx&oh=00_AfAihkcrUuum2SXpzSJ3bpLugpBbzNsVTkOoV7TcSeer2w&oe=65773519))
- **Scalable and Transferable Black-Box Jailbreaks for Language Models via
  Persona Modulation**, `arXiv, 2311.03348`, [arxiv](http://arxiv.org/abs/2311.03348v2), [pdf](http://arxiv.org/pdf/2311.03348v2.pdf), cication: [**-1**](None)

	 *Rusheb Shah, Quentin Feuillade--Montixi, Soroush Pour, Arush Tagade, Stephen Casper, Javier Rando*
- **Scalable Extraction of Training Data from (Production) Language Models**, `arXiv, 2311.17035`, [arxiv](http://arxiv.org/abs/2311.17035v1), [pdf](http://arxiv.org/pdf/2311.17035v1.pdf), cication: [**-1**](None)

	 *Milad Nasr, Nicholas Carlini, Jonathan Hayase, Matthew Jagielski, A. Feder Cooper, Daphne Ippolito, Christopher A. Choquette-Choo, Eric Wallace, Florian Tramèr, Katherine Lee* · ([qbitai](https://www.qbitai.com/2023/11/102130.html))
- **Scalable AI Safety via Doubly-Efficient Debate**, `arXiv, 2311.14125`, [arxiv](http://arxiv.org/abs/2311.14125v1), [pdf](http://arxiv.org/pdf/2311.14125v1.pdf), cication: [**-1**](None)

	 *Jonah Brown-Cohen, Geoffrey Irving, Georgios Piliouras* · ([debate](https://github.com/google-deepmind/debate) - google-deepmind) ![Star](https://img.shields.io/github/stars/google-deepmind/debate.svg?style=social&label=Star)
- **DeepInception: Hypnotize Large Language Model to Be Jailbreaker**, `arXiv, 2311.03191`, [arxiv](http://arxiv.org/abs/2311.03191v1), [pdf](http://arxiv.org/pdf/2311.03191v1.pdf), cication: [**-1**](None)

	 *Xuan Li, Zhanke Zhou, Jianing Zhu, Jiangchao Yao, Tongliang Liu, Bo Han* · ([DeepInception](https://github.com/tmlr-group/DeepInception) - tmlr-group) ![Star](https://img.shields.io/github/stars/tmlr-group/DeepInception.svg?style=social&label=Star) · ([deepinception.github](https://deepinception.github.io/))
- **Removing RLHF Protections in GPT-4 via Fine-Tuning**, `arXiv, 2311.05553`, [arxiv](http://arxiv.org/abs/2311.05553v2), [pdf](http://arxiv.org/pdf/2311.05553v2.pdf), cication: [**-1**](None)

	 *Qiusi Zhan, Richard Fang, Rohan Bindu, Akul Gupta, Tatsunori Hashimoto, Daniel Kang* · ([mp.weixin.qq](https://mp.weixin.qq.com/s?__biz=Mzg4OTEwNjMzMA==&mid=2247534723&idx=3&sn=5d812e7f65334a9dbbdaf052c21a58ce))
- **Frontier Language Models are not Robust to Adversarial Arithmetic, or
  "What do I need to say so you agree 2+2=5?**, `arXiv, 2311.07587`, [arxiv](http://arxiv.org/abs/2311.07587v1), [pdf](http://arxiv.org/pdf/2311.07587v1.pdf), cication: [**-1**](None)

	 *C. Daniel Freeman, Laura Culp, Aaron Parisi, Maxwell L Bileschi, Gamaleldin F Elsayed, Alex Rizkowsky, Isabelle Simpson, Alex Alemi, Azade Nova, Ben Adlam*
- **Unveiling Safety Vulnerabilities of Large Language Models**, `arXiv, 2311.04124`, [arxiv](http://arxiv.org/abs/2311.04124v1), [pdf](http://arxiv.org/pdf/2311.04124v1.pdf), cication: [**-1**](None)

	 *George Kour, Marcel Zalmanovici, Naama Zwerdling, Esther Goldbraich, Ora Nova Fandina, Ateret Anaby-Tavor, Orna Raz, Eitan Farchi*
- **Managing AI Risks in an Era of Rapid Progress**, `arXiv, 2310.17688`, [arxiv](http://arxiv.org/abs/2310.17688v1), [pdf](http://arxiv.org/pdf/2310.17688v1.pdf), cication: [**3**](https://scholar.google.com/scholar?cites=8498230073239105680&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Yoshua Bengio, Geoffrey Hinton, Andrew Yao, Dawn Song, Pieter Abbeel, Yuval Noah Harari, Ya-Qin Zhang, Lan Xue, Shai Shalev-Shwartz, Gillian Hadfield* · ([managing-ai-risks](https://managing-ai-risks.com/))
- **Jailbreaking Black Box Large Language Models in Twenty Queries**, `arXiv, 2310.08419`, [arxiv](http://arxiv.org/abs/2310.08419v2), [pdf](http://arxiv.org/pdf/2310.08419v2.pdf), cication: [**3**](https://scholar.google.com/scholar?cites=1618697255803227028&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George J. Pappas, Eric Wong* · ([qbitai](https://www.qbitai.com/2023/11/95723.html))
- **How Robust is Google's Bard to Adversarial Image Attacks?**, `arXiv, 2309.11751`, [arxiv](http://arxiv.org/abs/2309.11751v2), [pdf](http://arxiv.org/pdf/2309.11751v2.pdf), cication: [**1**](https://scholar.google.com/scholar?cites=10841564815936420494&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Yinpeng Dong, Huanran Chen, Jiawei Chen, Zhengwei Fang, Xiao Yang, Yichi Zhang, Yu Tian, Hang Su, Jun Zhu* · ([jiqizhixin](https://www.jiqizhixin.com/articles/2023-10-17-5))
- **Fine-tuning Aligned Language Models Compromises Safety, Even When Users
  Do Not Intend To!**, `arXiv, 2310.03693`, [arxiv](http://arxiv.org/abs/2310.03693v1), [pdf](http://arxiv.org/pdf/2310.03693v1.pdf), cication: [**3**](https://scholar.google.com/scholar?cites=14332933435628840179&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, Peter Henderson* · ([mp.weixin.qq](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652389679&idx=4&sn=a8860988614a66fab78392cbdbbd65fc&poc_token=HKlELGWjp2JYi7j0UAJ9dNwBEC2cxSIKZST7h0YT))
- **GPTFUZZER: Red Teaming Large Language Models with Auto-Generated
  Jailbreak Prompts**, `arXiv, 2309.10253`, [arxiv](http://arxiv.org/abs/2309.10253v2), [pdf](http://arxiv.org/pdf/2309.10253v2.pdf), cication: [**5**](https://scholar.google.com/scholar?cites=734711173091882842&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Jiahao Yu, Xingwei Lin, Zheng Yu, Xinyu Xing* · ([gptfuzz](https://github.com/sherdencooper/gptfuzz) - sherdencooper) ![Star](https://img.shields.io/github/stars/sherdencooper/gptfuzz.svg?style=social&label=Star)
- **"Do Anything Now": Characterizing and Evaluating In-The-Wild Jailbreak
  Prompts on Large Language Models**, `arXiv, 2308.03825`, [arxiv](http://arxiv.org/abs/2308.03825v1), [pdf](http://arxiv.org/pdf/2308.03825v1.pdf), cication: [**25**](https://scholar.google.com/scholar?cites=2730450320382131594&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Xinyue Shen, Zeyuan Chen, Michael Backes, Yun Shen, Yang Zhang*
- **MasterKey: Automated Jailbreak Across Multiple Large Language Model
  Chatbots**, `arXiv, 2307.08715`, [arxiv](http://arxiv.org/abs/2307.08715v2), [pdf](http://arxiv.org/pdf/2307.08715v2.pdf), cication: [**13**](https://scholar.google.com/scholar?cites=3831799754403709253&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Gelei Deng, Yi Liu, Yuekang Li, Kailong Wang, Ying Zhang, Zefeng Li, Haoyu Wang, Tianwei Zhang, Yang Liu* · ([qbitai](https://www.qbitai.com/2023/11/97487.html))
- **Universal and Transferable Adversarial Attacks on Aligned Language
  Models**, `arXiv, 2307.15043`, [arxiv](http://arxiv.org/abs/2307.15043v1), [pdf](http://arxiv.org/pdf/2307.15043v1.pdf), cication: [**58**](https://scholar.google.com/scholar?cites=18196466655097322708&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Andy Zou, Zifan Wang, J. Zico Kolter, Matt Fredrikson* · ([llm-attacks](https://github.com/llm-attacks/llm-attacks) - llm-attacks) ![Star](https://img.shields.io/github/stars/llm-attacks/llm-attacks.svg?style=social&label=Star) · ([qbitai](https://www.qbitai.com/2023/07/72014.html))
- **PUMA: Secure Inference of LLaMA-7B in Five Minutes**, `arXiv, 2307.12533`, [arxiv](http://arxiv.org/abs/2307.12533v3), [pdf](http://arxiv.org/pdf/2307.12533v3.pdf), cication: [**3**](https://scholar.google.com/scholar?cites=15180019305746166242&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Ye Dong, Wen-jie Lu, Yancheng Zheng, Haoqi Wu, Derun Zhao, Jin Tan, Zhicong Huang, Cheng Hong, Tao Wei, Wenguang Chen*
- **International Institutions for Advanced AI**, `arXiv, 2307.04699`, [arxiv](http://arxiv.org/abs/2307.04699v2), [pdf](http://arxiv.org/pdf/2307.04699v2.pdf), cication: [**9**](https://scholar.google.com/scholar?cites=1522776054133119216&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Lewis Ho, Joslyn Barnhart, Robert Trager, Yoshua Bengio, Miles Brundage, Allison Carnegie, Rumman Chowdhury, Allan Dafoe, Gillian Hadfield, Margaret Levi*
- **An Overview of Catastrophic AI Risks**, `arXiv, 2306.12001`, [arxiv](http://arxiv.org/abs/2306.12001v6), [pdf](http://arxiv.org/pdf/2306.12001v6.pdf), cication: [**20**](https://scholar.google.com/scholar?cites=13657604724019544721&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Dan Hendrycks, Mantas Mazeika, Thomas Woodside* · ([mp.weixin.qq](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652349067&idx=5&sn=8fabac5f652ec41a0dfe86a078def978))
- **Jailbroken: How Does LLM Safety Training Fail?**, `arXiv, 2307.02483`, [arxiv](http://arxiv.org/abs/2307.02483v1), [pdf](http://arxiv.org/pdf/2307.02483v1.pdf), cication: [**54**](https://scholar.google.com/scholar?cites=14029412962367612376&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Alexander Wei, Nika Haghtalab, Jacob Steinhardt*
- **PromptBench: Towards Evaluating the Robustness of Large Language Models
  on Adversarial Prompts**, `arXiv, 2306.04528`, [arxiv](http://arxiv.org/abs/2306.04528v4), [pdf](http://arxiv.org/pdf/2306.04528v4.pdf), cication: [**32**](https://scholar.google.com/scholar?cites=6727691362756502405&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong Wang, Linyi Yang, Wei Ye, Yue Zhang, Neil Zhenqiang Gong*
## Projects
- [**ai-exploits**](https://github.com/protectai/ai-exploits) - protectai ![Star](https://img.shields.io/github/stars/protectai/ai-exploits.svg?style=social&label=Star)

	 *A collection of real world AI/ML exploits for responsibly disclosed vulnerabilities*
- [**chatgpt_system_prompt**](https://github.com/LouisShark/chatgpt_system_prompt) - LouisShark ![Star](https://img.shields.io/github/stars/LouisShark/chatgpt_system_prompt.svg?style=social&label=Star)

	 *store all chatgpt's system prompt*
- [**cipherchat**](https://github.com/robustnlp/cipherchat) - robustnlp ![Star](https://img.shields.io/github/stars/robustnlp/cipherchat.svg?style=social&label=Star)

	 *A framework to evaluate the generalization capability of safety alignment for LLMs*

## Other
- [ChatGPT最近被微软内部禁用！GPTs新bug：数据两句话就能套走 | 量子位](https://www.qbitai.com/2023/11/97309.html)
- [一段话让模型自曝「系统提示词」！ChatGPT、Bing无一幸免 | 量子位](https://www.qbitai.com/2023/10/91673.html)
- [一段乱码，竟让ChatGPT越狱！乱序prompt让LLM火速生成勒索软件，Jim Fan惊了](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652367811&idx=4&sn=5737319e04c9ea80a506854350c438fa)
- [fast.ai - AI Safety and the Age of Dislightenment](https://www.fast.ai/posts/2023-11-07-dislightenment.html)
- [PoisonGPT: How We Hid a Lobotomized LLM on Hugging Face to Spread Fake News](https://blog.mithrilsecurity.io/poisongpt-how-we-hid-a-lobotomized-llm-on-hugging-face-to-spread-fake-news/)

	 · ([mp.weixin.qq](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652349939&idx=3&sn=54b2ca4d0ddf092c8883ad3625e13dd8))

## Extra reference