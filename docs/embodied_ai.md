# Embodied AI

- [Embodied AI](#embodied-ai) 
  - [Survey](#survey)
  - [Embodied AI](#embodied-ai-1)
  - [Robotics](#robotics)
  - [Humanoids](#humanoids)
  - [Projects](#projects)
  - [Misc](#misc)


## Survey

- **Neural Fields in Robotics: A Survey**, `arXiv, 2410.20220`, [arxiv](http://arxiv.org/abs/2410.20220v1), [pdf](http://arxiv.org/pdf/2410.20220v1.pdf), cication: [**-1**](None) 

	 *Muhammad Zubair Irshad, Mauro Comi, Yen-Chen Lin, ..., Zsolt Kira, Jonathan Tremblay* · ([robonerf.github](https://robonerf.github.io/))

## Embodied AI

- **LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large 
  Language Models**, `ICCV, 2023`, [arxiv](http://arxiv.org/abs/2212.04088v3), [pdf](http://arxiv.org/pdf/2212.04088v3.pdf), cication: [**-1**](None) 

	 *Chan Hee Song, Jiaman Wu, Clayton Washington, ..., Wei-Lun Chao, Yu Su* · ([dki-lab.github](https://dki-lab.github.io/LLM-Planner))
- **FAST: Efficient Action Tokenization for Vision-Language-Action Models**, `arXiv, 2501.09747`, [arxiv](http://arxiv.org/abs/2501.09747v1), [pdf](http://arxiv.org/pdf/2501.09747v1.pdf), cication: [**-1**](None) 

	 *Karl Pertsch, Kyle Stachowicz, Brian Ichter, ..., Chelsea Finn, Sergey Levine* · ([𝕏](https://x.com/KarlPertsch/status/1879960952042660063)) · ([pi](https://www.pi.website/research/fast))
- **OmniManip: Towards General Robotic Manipulation via Object-Centric 
  Interaction Primitives as Spatial Constraints**, `arXiv, 2501.03841`, [arxiv](http://arxiv.org/abs/2501.03841v1), [pdf](http://arxiv.org/pdf/2501.03841v1.pdf), cication: [**-1**](None) 

	 *Mingjie Pan, Jiyao Zhang, Tianshu Wu, ..., Wenlong Gao, Hao Dong* · ([omnimanip.github](https://omnimanip.github.io/))
- **Beyond Sight: Finetuning Generalist Robot Policies with Heterogeneous 
  Sensors via Language Grounding**, `arXiv, 2501.04693`, [arxiv](http://arxiv.org/abs/2501.04693v1), [pdf](http://arxiv.org/pdf/2501.04693v1.pdf), cication: [**-1**](None) 

	 *Joshua Jones, Oier Mees, Carmelo Sferrazza, ..., Pieter Abbeel, Sergey Levine*
- 🌟 **EnerVerse: Envisioning Embodied Future Space for Robotics Manipulation**, `arXiv, 2501.01895`, [arxiv](http://arxiv.org/abs/2501.01895v1), [pdf](http://arxiv.org/pdf/2501.01895v1.pdf), cication: [**-1**](None) 

	 *Siyuan Huang, Liliang Chen, Pengfei Zhou, ..., Maoqing Yao, Guanghui Ren* · ([sites.google](https://sites.google.com/view/enerverse))
- **Towards Generalist Robot Policies: What Matters in Building 
  Vision-Language-Action Models**, `arXiv, 2412.14058`, [arxiv](http://arxiv.org/abs/2412.14058v3), [pdf](http://arxiv.org/pdf/2412.14058v3.pdf), cication: [**-1**](None) 

	 *Xinghang Li, Peiyan Li, Minghuan Liu, ..., Hanbo Zhang, Huaping Liu*
- **Code-as-Monitor: Constraint-aware Visual Programming for Reactive and 
  Proactive Robotic Failure Detection**, `arXiv, 2412.04455`, [arxiv](http://arxiv.org/abs/2412.04455v2), [pdf](http://arxiv.org/pdf/2412.04455v2.pdf), cication: [**-1**](None) 

	 *Enshen Zhou, Qi Su, Cheng Chi, ..., Lu Sheng, He Wang* · ([zhoues.github](https://zhoues.github.io/Code-as-Monitor/))
- **Moto: Latent Motion Token as the Bridging Language for Robot 
  Manipulation**, `arXiv, 2412.04445`, [arxiv](http://arxiv.org/abs/2412.04445v1), [pdf](http://arxiv.org/pdf/2412.04445v1.pdf), cication: [**-1**](None) 

	 *Yi Chen, Yuying Ge, Yizhuo Li, ..., Ying Shan, Xihui Liu* · ([chenyi99.github](https://chenyi99.github.io/moto/)) · ([Moto](https://github.com/TencentARC/Moto) - TencentARC) ![Star](https://img.shields.io/github/stars/TencentARC/Moto.svg?style=social&label=Star)
- 🌟 **Unraveling the Complexity of Memory in RL Agents: an Approach for 
  Classification and Evaluation**, `arXiv, 2412.06531`, [arxiv](http://arxiv.org/abs/2412.06531v1), [pdf](http://arxiv.org/pdf/2412.06531v1.pdf), cication: [**-1**](None) 

	 *Egor Cherepanov, Nikita Kachaev, Artem Zholus, ..., Alexey K. Kovalev, Aleksandr I. Panov*
- **Embodied Agent Interface: Benchmarking LLMs for Embodied Decision Making**, `arXiv, 2410.07166`, [arxiv](http://arxiv.org/abs/2410.07166v2), [pdf](http://arxiv.org/pdf/2410.07166v2.pdf), cication: [**1**](https://scholar.google.com/scholar?cites=2488405281591567279&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII) 

	 *Manling Li, Shiyu Zhao, Qineng Wang, ..., Jiayuan Mao, Jiajun Wu* · ([embodied-agent-interface.github](https://embodied-agent-interface.github.io/)) · ([embodied-agent-eval](https://github.com/embodied-agent-eval/embodied-agent-eval) - embodied-agent-eval) ![Star](https://img.shields.io/github/stars/embodied-agent-eval/embodied-agent-eval.svg?style=social&label=Star) · ([𝕏](https://x.com/manlingli_/status/1854041025146404897?s=46))
- [foundation models for the physical world.](https://perceptron.inc/) 
- **DeeR-VLA: Dynamic Inference of Multimodal Large Language Models for 
  Efficient Robot Execution**, `arXiv, 2411.02359`, [arxiv](http://arxiv.org/abs/2411.02359v1), [pdf](http://arxiv.org/pdf/2411.02359v1.pdf), cication: [**-1**](None) 

	 *Yang Yue, Yulin Wang, Bingyi Kang, ..., Jiashi Feng, Gao Huang* · ([DeeR-VLA](https://github.com/yueyang130/DeeR-VLA) - yueyang130) ![Star](https://img.shields.io/github/stars/yueyang130/DeeR-VLA.svg?style=social&label=Star)
- **DynaMem: Online Dynamic Spatio-Semantic Memory for Open World Mobile 
  Manipulation**, `arXiv, 2411.04999`, [arxiv](http://arxiv.org/abs/2411.04999v1), [pdf](http://arxiv.org/pdf/2411.04999v1.pdf), cication: [**-1**](None) 

	 *Peiqi Liu, Zhanqiu Guo, Mohit Warke, ..., Nur Muhammad Mahi Shafiullah, Lerrel Pinto* · ([dynamem.github](https://dynamem.github.io/))
- **A Large Recurrent Action Model: xLSTM enables Fast Inference for 
  Robotics Tasks**, `arXiv, 2410.22391`, [arxiv](http://arxiv.org/abs/2410.22391v1), [pdf](http://arxiv.org/pdf/2410.22391v1.pdf), cication: [**-1**](None)

	 *Thomas Schmied, Thomas Adler, Vihang Patil, ..., Razvan Pascanu, Sepp Hochreiter* · ([arxiv](https://arxiv.org/abs/2410.22391)) · ([huggingface](https://huggingface.co/ml-jku)) · ([LRAM](https://github.com/ml-jku/LRAM) - ml-jku) ![Star](https://img.shields.io/github/stars/ml-jku/LRAM.svg?style=social&label=Star)
- **VLMimic: Vision Language Models are Visual Imitation Learner for 
  Fine-grained Actions**, `arXiv, 2410.20927`, [arxiv](http://arxiv.org/abs/2410.20927v2), [pdf](http://arxiv.org/pdf/2410.20927v2.pdf), cication: [**-1**](None)

	 *Guanyan Chen, Meiling Wang, Te Cui, ..., Yi Yang, Yufeng Yue*
- **VidEgoThink: Assessing Egocentric Video Understanding Capabilities for 
  Embodied AI**, `arXiv, 2410.11623`, [arxiv](http://arxiv.org/abs/2410.11623v1), [pdf](http://arxiv.org/pdf/2410.11623v1.pdf), cication: [**-1**](None)

	 *Sijie Cheng, Kechen Fang, Yangyang Yu, ..., Lei Han, Yang Liu*

## Robotics

- [ASAP                    Aligning Simulation and Real-World Physics for Learning Agile Humanoid Whole-Body Skills                                                                                                                    Paper                                                                                                      arXiv                                                                                                                Video                                                                                                                Code                                                                            ASAP: Aligning Simulation and Real-World Physics for Learning Agile Humanoid Whole-Body Skills                                              Tairan He*                Jiawei Gao*                Wenli Xiao*                Yuanhang Zhang*                Zi Wang                Jiashun Wang                              Zhengyi Luo                Guanqi He                Nikhil Sobanbabu                Chaoyi Pan                Zeji Yi                Guannan Qu                              Kris Kitani                Jessica Hodgins                Linxi "Jim" Fan                Yuke Zhu                Changliu Liu                Guanya Shi                                                                                                                                                                                                               PDF                                                                                                                                                      ArXiv                                                                                                                                        Video                                                                                                                                                                                  Summary                                                                                                                                                    Code                                                                                            Agile Whole-body Skills                                                             Cristiano Ronaldo                                                            Kobe Bryant                                                                                                 LeBron James                                                            Side Jump (1.3m)                                                                                                 Jump Forward (0.85m)                                                            Jump Forward (1.5m)                                                                                                 Forward Kick                                                            Right Kick                                                                                                 APT Dance                                                            Leg Stretch                                                                                                 Squat                                                            Squat + Lean Forward                                                                                                         Before/After ASAP Fine-tuning                                                             Kick (Before ASAP)                                                            Kick (After ASAP)                                                                                                 LeBron James (Before ASAP)                                                            LeBron James (After ASAP)                                                                                                 Abstract                              Humanoid robots hold the potential for unparalleled versatility for performing human-like, whole-body skills. However, achieving agile and coordinated whole-body motions remains a significant challenge due to the dynamics mismatch between simulation and the real world. Existing approaches, such as system identification (SysID) and domain randomization (DR) methods, often rely on labor-intensive parameter tuning or result in overly conservative policies that sacrifice agility. In this paper, we present ASAP (Aligning Simulation and Real Physics), a two-stage framework designed to tackle the dynamics mismatch and enable agile humanoid whole-body skills.                            In the first stage, we pre-train motion tracking policies in simulation using retargeted human motion data. In the second stage, we deploy the policies in the real world and collect real-world data to train a delta (residual) action model that compensates for the dynamics mismatch. Then, ASAP fine-tunes pre-trained policies with the delta action model integrated into the simulator to align effectively with real-world dynamics. We evaluate ASAP across three transfer scenarios—IsaacGym to IsaacSim, IsaacGym to Genesis, and IsaacGym to the real-world Unitree G1 humanoid robot. Our approach significantly improves agility and whole-body coordination across various dynamic motions, reducing tracking error compared to SysID, DR, and delta dynamics learning baselines.                            ASAP enables highly agile motions that were previously difficult to achieve, demonstrating the potential of delta action learning in bridging simulation and real-world dynamics. These results suggest a promising sim-to-real direction for developing more expressive and agile humanoids.                                  Method                                                There are four steps within the ASAP framework:                              Motion Tracking Pre-training and Real Trajectory Collection:  With the humanoid motions retargeted from human videos, we pre-train multiple motion tracking policies to roll out real-world trajectories;                Delta Action Model Training Based on the real-world rollout data, we train the delta action model by minimizing the discrepancy between simulation state s_t and real-world state s^r_t;                Policy Fine-tuning We freeze the delta action model, incorporate it into the simulator to align the real-world physics and then fine-tune the pre-trained motion tracking policy;                Real-World Deployment Finally, we deploy the fine-tuned policy directly in the real world without the delta action model.                                                                                  BibTeX        @article{he2025asap,          title={ASAP: Aligning Simulation and Real-World Physics for Learning Agile Humanoid Whole-Body Skills},          author={He, Tairan and Gao, Jiawei and Xiao, Wenli and Zhang, Yuanhang and Wang, Zi and Wang, Jiashun and Luo, Zhengyi and He, Guanqi and Sobanbabu, Nikhil and Pan, Chaoyi and Yi, Zeji and Qu, Guannan and Kitani, Kris and Hodgins, Jessica and Fan, Linxi "Jim" and Zhu, Yuke and Liu, Changliu and Shi, Guanya},          journal={arXiv preprint arXiv:2502.01143},          year={2025}        }                                      Page template borrowed from Nerfies and UMI-On-Legs.](https://agile.human2humanoid.com/) 
- [Deep Robotics shared a video of its Lynx robot dog parkouring on snow and crossing streams](https://x.com/adcock_brett/status/1883561429963243849)  𝕏 
- [CS 294-277, Robots That Learn (Fall 2024)](http://robots-that-learn.github.io/) 
- [Introducing AgiBot World, the first large-scale robotic learning dataset designed to advance multi-purpose robotic policies.](https://agibot-world.com/) 

	 · ([Agibot-World](https://github.com/OpenDriveLab/Agibot-World) - OpenDriveLab) ![Star](https://img.shields.io/github/stars/OpenDriveLab/Agibot-World.svg?style=social&label=Star)
- [**openpilot**](https://github.com/commaai/openpilot) - commaai ![Star](https://img.shields.io/github/stars/commaai/openpilot.svg?style=social&label=Star) 
- [**flow_matching**](https://github.com/HRI-EU/flow_matching) - HRI-EU ![Star](https://img.shields.io/github/stars/HRI-EU/flow_matching.svg?style=social&label=Star) 
- [fastest, most precise, and most capable hand control setup](https://x.com/RemiCadene/status/1868210029985513959)  𝕏 
- **GRAPE: Generalizing Robot Policy via Preference Alignment**, `arXiv, 2411.19309`, [arxiv](http://arxiv.org/abs/2411.19309v1), [pdf](http://arxiv.org/pdf/2411.19309v1.pdf), cication: [**-1**](None) 

	 *Zijian Zhang, Kaiyuan Zheng, Zhaorun Chen, ..., Dieter Fox, Huaxiu Yao* · ([grape-vla.github](https://grape-vla.github.io/))
- **Soft Robotic Dynamic In-Hand Pen Spinning**, `arXiv, 2411.12734`, [arxiv](http://arxiv.org/abs/2411.12734v1), [pdf](http://arxiv.org/pdf/2411.12734v1.pdf), cication: [**-1**](None) 

	 *Yunchao Yao, Uksang Yoo, Jean Oh, ..., Christopher G. Atkeson, Jeffrey Ichnowski* · ([soft-spin.github](https://soft-spin.github.io/))
- [trained surgical robots through video-based imitation learning to match human surgeon skill levels](https://x.com/adcock_brett/status/1858194256680079674)  𝕏 
- [DEEP Robotics shared a new video of their new commercially available robot dog](https://x.com/adcock_brett/status/1858194279056744876)  𝕏 
- **HOVER: Versatile Neural Whole-Body Controller for Humanoid Robots**, `arXiv, 2410.21229`, [arxiv](http://arxiv.org/abs/2410.21229v1), [pdf](http://arxiv.org/pdf/2410.21229v1.pdf), cication: [**-1**](None) 

	 *Tairan He, Wenli Xiao, Toru Lin, ..., Linxi Fan, Yuke Zhu* · ([x](https://x.com/DrJimFan/status/1851643431803830551)) · ([hover-versatile-humanoid.github](https://hover-versatile-humanoid.github.io/))
- **DexMimicGen: Automated Data Generation for Bimanual Dexterous 
  Manipulation via Imitation Learning**, `arXiv, 2410.24185`, [arxiv](http://arxiv.org/abs/2410.24185v1), [pdf](http://arxiv.org/pdf/2410.24185v1.pdf), cication: [**-1**](None)

	 *Zhenyu Jiang, Yuqi Xie, Kevin Lin, ..., Linxi Fan, Yuke Zhu* · ([dexmimicgen.github](https://dexmimicgen.github.io/)) · ([twitter](https://twitter.com/SteveTod1998/status/1852365700372832707))
- [Advancing embodied AI through progress in touch perception, dexterity, and human-robot interaction](https://ai.meta.com/blog/fair-robotics-open-source/) 

	 · ([x](https://x.com/AIatMeta/status/1852019804292682200))
- [what is the largest public dataset of robot expert demonstrations across diverse manipulations tasks, constrained to a single form factor?](https://x.com/ericjang11/status/1851987666000101596)  𝕏 
- [AI-assisted multi-arm robot for apple picking](https://buttondown.com/ainews/archive/ainews-github-copilot-strikes-back-3402/) 

	 · ([v.redd](https://v.redd.it/552w8berqhxd1))
- [**moss-robot-arms**](https://github.com/jess-moss/moss-robot-arms) - jess-moss ![Star](https://img.shields.io/github/stars/jess-moss/moss-robot-arms.svg?style=social&label=Star) 
- **Language-Model-Assisted Bi-Level Programming for Reward Learning from 
  Internet Videos**, `arXiv, 2410.09286`, [arxiv](http://arxiv.org/abs/2410.09286v1), [pdf](http://arxiv.org/pdf/2410.09286v1.pdf), cication: [**-1**](None)

	 *Harsh Mahesheka, Zhixian Xie, Zhaoran Wang, ..., Wanxin Jin*

## Humanoids

- [**unitree_rl_gym**](https://github.com/unitreerobotics/unitree_rl_gym) - unitreerobotics ![Star](https://img.shields.io/github/stars/unitreerobotics/unitree_rl_gym.svg?style=social&label=Star) 
- [Boston Dynamics showcased Atlas autonomously transferring engine covers using ML vision for object detection and localization](https://x.com/adcock_brett/status/1853120940651024503)  𝕏 
- [EngineAI unveiled SE01, a humanoid robot achieving natural walking through advanced joint modules and neural networks.](https://x.com/adcock_brett/status/1850569193365676202)  𝕏 
- [Introducing Torso, a bimanual android actuated with artificial muscles.](https://x.com/clonerobotics/status/1849181515022053845)  𝕏 
- [CooHOI, a learning-based framework designed for the cooperative transportation of objects by multiple humanoid robots.](https://x.com/WinstonGu_/status/1848393460849799439)  𝕏 
- **CooHOI: Learning Cooperative Human-Object Interaction with Manipulated 
  Object Dynamics**, `arXiv, 2406.14558`, [arxiv](http://arxiv.org/abs/2406.14558v2), [pdf](http://arxiv.org/pdf/2406.14558v2.pdf), cication: [**-1**](None)

	 *Jiawei Gao, Ziqin Wang, Zeqi Xiao, ..., Jifeng Dai, Jiangmiao Pang* · ([gao-jiawei](https://gao-jiawei.com/Research/CooHOI/))

## Projects

- [**Genesis**](https://github.com/Genesis-Embodied-AI/Genesis) - Genesis-Embodied-AI ![Star](https://img.shields.io/github/stars/Genesis-Embodied-AI/Genesis.svg?style=social&label=Star) 

## Misc