# Awesome llm training

## Alignment
- **Zephyr: Direct Distillation of LM Alignment**, `arXiv, 2310.16944`, [arxiv](http://arxiv.org/abs/2310.16944v1), [pdf](http://arxiv.org/pdf/2310.16944v1.pdf), cication: [**1**](https://scholar.google.com/scholar?cites=5826276281263581161&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clémentine Fourrier, Nathan Habib* · ([alignment-handbook](https://github.com/huggingface/alignment-handbook) - huggingface) ![Star](https://img.shields.io/github/stars/huggingface/alignment-handbook.svg?style=social&label=Star)

## RLHF
- **Don't throw away your value model! Making PPO even better via
  Value-Guided Monte-Carlo Tree Search decoding**, `arXiv, 2309.15028`, [arxiv](http://arxiv.org/abs/2309.15028v2), [pdf](http://arxiv.org/pdf/2309.15028v2.pdf), cication: [**1**](https://scholar.google.com/scholar?cites=12825384516892808854&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Jiacheng Liu, Andrew Cohen, Ramakanth Pasunuru, Yejin Choi, Hannaneh Hajishirzi, Asli Celikyilmaz* · ([jiqizhixin](https://www.jiqizhixin.com/articles/2023-10-27-3))
- [The N Implementation Details of RLHF with PPO](https://huggingface.co/blog/the_n_implementation_details_of_rlhf_with_ppo)
- **Specific versus General Principles for Constitutional AI**, `arXiv, 2310.13798`, [arxiv](http://arxiv.org/abs/2310.13798v1), [pdf](http://arxiv.org/pdf/2310.13798v1.pdf), cication: [**1**](https://scholar.google.com/scholar?cites=10472606437287020754&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Sandipan Kundu, Yuntao Bai, Saurav Kadavath, Amanda Askell, Andrew Callahan, Anna Chen, Anna Goldie, Avital Balwit, Azalia Mirhoseini, Brayden McLean*
- **Contrastive Preference Learning: Learning from Human Feedback without RL**, `arXiv, 2310.13639`, [arxiv](http://arxiv.org/abs/2310.13639v2), [pdf](http://arxiv.org/pdf/2310.13639v2.pdf), cication: [**-1**](None)

	 *Joey Hejna, Rafael Rafailov, Harshit Sikchi, Chelsea Finn, Scott Niekum, W. Bradley Knox, Dorsa Sadigh*
- **Tuna: Instruction Tuning using Feedback from Large Language Models**, `arXiv, 2310.13385`, [arxiv](http://arxiv.org/abs/2310.13385v1), [pdf](http://arxiv.org/pdf/2310.13385v1.pdf), cication: [**-1**](None)

	 *Haoran Li, Yiran Liu, Xingxing Zhang, Wei Lu, Furu Wei*
- **Safe RLHF: Safe Reinforcement Learning from Human Feedback**, `arXiv, 2310.12773`, [arxiv](http://arxiv.org/abs/2310.12773v1), [pdf](http://arxiv.org/pdf/2310.12773v1.pdf), cication: [**1**](https://scholar.google.com/scholar?cites=10151978917046355982&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Josef Dai, Xuehai Pan, Ruiyang Sun, Jiaming Ji, Xinbo Xu, Mickel Liu, Yizhou Wang, Yaodong Yang*
- **ReMax: A Simple, Effective, and Efficient Reinforcement Learning Method
  for Aligning Large Language Models**, `arXiv, 2310.10505`, [arxiv](http://arxiv.org/abs/2310.10505v2), [pdf](http://arxiv.org/pdf/2310.10505v2.pdf), cication: [**-1**](None)

	 *Ziniu Li, Tian Xu, Yushun Zhang, Yang Yu, Ruoyu Sun, Zhi-Quan Luo* · ([jiqizhixin](https://www.jiqizhixin.com/articles/2023-10-20-9))
- [Rethinking the Role of PPO in RLHF – The Berkeley Artificial Intelligence Research Blog](https://bair.berkeley.edu/blog/2023/10/16/p3o/)
- **Reinforcement Learning in the Era of LLMs: What is Essential? What is
  needed? An RL Perspective on RLHF, Prompting, and Beyond**, `arXiv, 2310.06147`, [arxiv](http://arxiv.org/abs/2310.06147v1), [pdf](http://arxiv.org/pdf/2310.06147v1.pdf), cication: [**-1**](None)

	 *Hao Sun*
- **A Long Way to Go: Investigating Length Correlations in RLHF**, `arXiv, 2310.03716`, [arxiv](http://arxiv.org/abs/2310.03716v1), [pdf](http://arxiv.org/pdf/2310.03716v1.pdf), cication: [**3**](https://scholar.google.com/scholar?cites=17792312030938285213&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Prasann Singhal, Tanya Goyal, Jiacheng Xu, Greg Durrett*
- **Aligning Large Multimodal Models with Factually Augmented RLHF**, `arXiv, 2309.14525`, [arxiv](http://arxiv.org/abs/2309.14525v1), [pdf](http://arxiv.org/pdf/2309.14525v1.pdf), cication: [**4**](https://scholar.google.com/scholar?cites=17054470781093797244&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang*
- **Stabilizing RLHF through Advantage Model and Selective Rehearsal**, `arXiv, 2309.10202`, [arxiv](http://arxiv.org/abs/2309.10202v1), [pdf](http://arxiv.org/pdf/2309.10202v1.pdf), cication: [**1**](https://scholar.google.com/scholar?cites=16456025046699375201&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Baolin Peng, Linfeng Song, Ye Tian, Lifeng Jin, Haitao Mi, Dong Yu*
- **Statistical Rejection Sampling Improves Preference Optimization**, `arXiv, 2309.06657`, [arxiv](http://arxiv.org/abs/2309.06657v1), [pdf](http://arxiv.org/pdf/2309.06657v1.pdf), cication: [**-1**](None)

	 *Tianqi Liu, Yao Zhao, Rishabh Joshi, Misha Khalman, Mohammad Saleh, Peter J. Liu, Jialu Liu*
- **Efficient RLHF: Reducing the Memory Usage of PPO**, `arXiv, 2309.00754`, [arxiv](http://arxiv.org/abs/2309.00754v1), [pdf](http://arxiv.org/pdf/2309.00754v1.pdf), cication: [**1**](https://scholar.google.com/scholar?cites=6843908222161428317&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Michael Santacroce, Yadong Lu, Han Yu, Yuanzhi Li, Yelong Shen*
- **RLAIF: Scaling Reinforcement Learning from Human Feedback with AI
  Feedback**, `arXiv, 2309.00267`, [arxiv](http://arxiv.org/abs/2309.00267v1), [pdf](http://arxiv.org/pdf/2309.00267v1.pdf), cication: [**24**](https://scholar.google.com/scholar?cites=7995210232742152683&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune, Abhinav Rastogi* · ([mp.weixin.qq](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652372872&idx=1&sn=c1a543c792bf3dfc891729f04236c549))
- [**awesome-RLHF**](https://github.com/opendilab/awesome-RLHF) - opendilab ![Star](https://img.shields.io/github/stars/opendilab/awesome-RLHF.svg?style=social&label=Star)

	 *A curated list of reinforcement learning with human feedback resources (continually updated)*
- **Reinforced Self-Training (ReST) for Language Modeling**, `arXiv, 2308.08998`, [arxiv](http://arxiv.org/abs/2308.08998v2), [pdf](http://arxiv.org/pdf/2308.08998v2.pdf), cication: [**12**](https://scholar.google.com/scholar?cites=3263533902860525796&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu* · ([jiqizhixin](https://www.jiqizhixin.com/articles/2023-08-22-6))
- **DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like
  Models at All Scales**, `arXiv, 2308.01320`, [arxiv](http://arxiv.org/abs/2308.01320v1), [pdf](http://arxiv.org/pdf/2308.01320v1.pdf), cication: [**4**](https://scholar.google.com/scholar?cites=9524636698512222272&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Zhewei Yao, Reza Yazdani Aminabadi, Olatunji Ruwase, Samyam Rajbhandari, Xiaoxia Wu, Ammar Ahmad Awan, Jeff Rasley, Minjia Zhang, Conglong Li, Connor Holmes*
- **Open Problems and Fundamental Limitations of Reinforcement Learning from
  Human Feedback**, `arXiv, 2307.15217`, [arxiv](http://arxiv.org/abs/2307.15217v2), [pdf](http://arxiv.org/pdf/2307.15217v2.pdf), cication: [**36**](https://scholar.google.com/scholar?cites=2043453316558651204&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, Jérémy Scheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire* · ([jiqizhixin](https://www.jiqizhixin.com/articles/2023-08-01-9))
- [ICML '23 Tutorial on Reinforcement Learning from Human Feedback](https://docs.google.com/presentation/d/1b_ymNDU0WRQ1-rcQDK45_bH9F0giNyRmdi0iKso6G5E/edit#slide=id.g259dff58475_0_44)

	 · ([openlmlab.github](https://openlmlab.github.io/MOSS-RLHF/)) · ([mp.weixin.qq](https://mp.weixin.qq.com/s?__biz=MzAxMTk4NDkwNw==&mid=2247494225&idx=1&sn=7bae77595b4e2f43cc8a4b94ddb4646c))
- **Fine-Tuning Language Models with Advantage-Induced Policy Alignment**, `arXiv, 2306.02231`, [arxiv](http://arxiv.org/abs/2306.02231v3), [pdf](http://arxiv.org/pdf/2306.02231v3.pdf), cication: [**5**](https://scholar.google.com/scholar?cites=7726101249171983080&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Banghua Zhu, Hiteshi Sharma, Felipe Vieira Frujeri, Shi Dong, Chenguang Zhu, Michael I. Jordan, Jiantao Jiao*
- **System-Level Natural Language Feedback**, `arXiv, 2306.13588`, [arxiv](http://arxiv.org/abs/2306.13588v1), [pdf](http://arxiv.org/pdf/2306.13588v1.pdf), cication: [**1**](https://scholar.google.com/scholar?cites=5523428644029476523&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Weizhe Yuan, Kyunghyun Cho, Jason Weston*
- **Fine-Grained Human Feedback Gives Better Rewards for Language Model
  Training**, `arXiv, 2306.01693`, [arxiv](http://arxiv.org/abs/2306.01693v2), [pdf](http://arxiv.org/pdf/2306.01693v2.pdf), cication: [**7**](https://scholar.google.com/scholar?cites=9400790265193597011&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah A. Smith, Mari Ostendorf, Hannaneh Hajishirzi* · ([finegrainedrlhf.github](https://finegrainedrlhf.github.io/)) · ([qbitai](https://www.qbitai.com/2023/06/61691.html))

### Projects
- [**trl**](https://github.com/huggingface/trl) - huggingface ![Star](https://img.shields.io/github/stars/huggingface/trl.svg?style=social&label=Star)

	 *Train transformer language models with reinforcement learning.*

### Other
- [LLM Training: RLHF and Its Alternatives](https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives)

	 · ([mp.weixin.qq](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652379509&idx=5&sn=910a60449c276ac9d0f29b8f71a60327))
- [RLHF实践 - 知乎](https://zhuanlan.zhihu.com/p/635569455)

	 · ([mp.weixin.qq](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652371219&idx=5&sn=63c3294a9c08dde7e63444c6110e9c34))
- [ICML '23 Tutorial on Reinforcement Learning from Human Feedback](https://docs.google.com/presentation/d/1b_ymNDU0WRQ1-rcQDK45_bH9F0giNyRmdi0iKso6G5E/edit#slide=id.g259dff58475_0_44)
- [LLM成功不可或缺的基石：RLHF及其替代技术 | 机器之心](https://www.jiqizhixin.com/articles/2023-10-07-7)


## Extra reference