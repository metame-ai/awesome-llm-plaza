# LLM Data

- [LLM Data](#llm-data) 
  - [Survey](#survey)
  - [LLM Data](#llm-data-1)
  - [Multi Modal](#multi-modal)
  - [Alignment](#alignment)
  - [Synthetic](#synthetic)
  - [Reasoning](#reasoning)
  - [Action](#action)
  - [Toolkits](#toolkits)
  - [Misc](#misc)


## Survey

- **A Survey on Data Synthesis and Augmentation for Large Language Models**, `arXiv, 2410.12896`, [arxiv](http://arxiv.org/abs/2410.12896v1), [pdf](http://arxiv.org/pdf/2410.12896v1.pdf), cication: [**-1**](None) 

	 *Ke Wang, Jiahui Zhu, Minjie Ren, ..., Qingjie Liu, Yunhong Wang*

## LLM Data

- [**MultimodalUniverse**](https://github.com/MultimodalUniverse/MultimodalUniverse) - MultimodalUniverse ![Star](https://img.shields.io/github/stars/MultimodalUniverse/MultimodalUniverse.svg?style=social&label=Star) 

	 *Enabling Large-Scale Machine Learning with 100TBs of Astronomical Scientific Data* 路 ([](https://x.com/MilesCranmer/status/1863668192519839793))
- **Zyda-2: a 5 Trillion Token High-Quality Dataset**, `arXiv, 2411.06068`, [arxiv](http://arxiv.org/abs/2411.06068v1), [pdf](http://arxiv.org/pdf/2411.06068v1.pdf), cication: [**-1**](None) 

	 *Yury Tokpanov, Paolo Glorioso, Quentin Anthony, ..., Beren Millidge* 路 ([huggingface](https://huggingface.co/datasets/Zyphra/Zyda-2))
- [**dolma**](https://github.com/allenai/dolma) - allenai ![Star](https://img.shields.io/github/stars/allenai/dolma.svg?style=social&label=Star) 

	 *an open dataset of 3 trillion tokens from a diverse mix of web content, academic publications, code, books, and encyclopedic materials.*
-  **RedPajama: an Open Dataset for Training Large Language Models**, `arXiv, 2411.12372`, [arxiv](http://arxiv.org/abs/2411.12372v1), [pdf](http://arxiv.org/pdf/2411.12372v1.pdf), cication: [**-1**](None) 

	 *Maurice Weber, Daniel Fu, Quentin Anthony, ..., Irina Rish, Ce Zhang*
-  [**smollm**](https://github.com/huggingface/smollm) - huggingface ![Star](https://img.shields.io/github/stars/huggingface/smollm.svg?style=social&label=Star) 

	 *135M, 360M, and 1.7B parameters.* 路 ([smollm](https://github.com/huggingface/smollm) - huggingface) ![Star](https://img.shields.io/github/stars/huggingface/smollm.svg?style=social&label=Star) 路 ([](https://x.com/_philschmid/status/1859598525723488478)) 路 ([huggingface](https://huggingface.co/datasets/HuggingFaceTB/smoltalk))
- [Zyda-2 is a 5 trillion token language modeling dataset created by collecting open and high quality datasets and combining them and cross-deduplication and model-based quality filtering.](https://huggingface.co/datasets/Zyphra/Zyda-2)   
- [Common Corpus is the largest open and permissible licensed text dataset, comprising over 2 trillion tokens](https://huggingface.co/datasets/PleIAs/common_corpus)   
- [Releasing the largest multilingual open pretraining dataset](https://huggingface.co/blog/Pclanglais/two-trillion-tokens-open)   
-  **Both Text and Images Leaked! A Systematic Analysis of Multimodal LLM 
  Data Contamination**, `arXiv, 2411.03823`, [arxiv](http://arxiv.org/abs/2411.03823v1), [pdf](http://arxiv.org/pdf/2411.03823v1.pdf), cication: [**-1**](None) 

	 *Dingjie Song, Sicheng Lai, Shunian Chen, ..., Lichao Sun, Benyou Wang* 路 ([MM-Detect](https://github.com/MLLM-Data-Contamination/MM-Detect) - MLLM-Data-Contamination) ![Star](https://img.shields.io/github/stars/MLLM-Data-Contamination/MM-Detect.svg?style=social&label=Star)
- **Document Parsing Unveiled: Techniques, Challenges, and Prospects for 
  Structured Information Extraction**, `arXiv, 2410.21169`, [arxiv](http://arxiv.org/abs/2410.21169v2), [pdf](http://arxiv.org/pdf/2410.21169v2.pdf), cication: [**-1**](None)

	 *Qintong Zhang, Victor Shea-Jay Huang, Bin Wang, ..., Conghui He, Wentao Zhang*
- [Dolma is a dataset of 3 trillion tokens from a diverse mix of web content, academic publications, code, books, and encyclopedic materials.](https://huggingface.co/datasets/allenai/dolma)   

	 路 ([arxiv](https://arxiv.org/abs/2402.00159))
- [Arxiver consists of 63,357 arXiv papers converted to multi-markdown (.mmd) format.](https://huggingface.co/datasets/neuralwork/arxiver)   
- **Compute-Constrained Data Selection**, `arXiv, 2410.16208`, [arxiv](http://arxiv.org/abs/2410.16208v1), [pdf](http://arxiv.org/pdf/2410.16208v1.pdf), cication: [**-1**](None) 

	 *Junjie Oscar Yin, Alexander M. Rush*

## Multi Modal

-  [A set of vision-language datasets built by Ai2 and used to train the Molmo family of models.](https://huggingface.co/collections/allenai/pixmo-674746ea613028006285687b)   
- [PangeaIns is a 6M multilingual multicultural multimodal instruction tuning dataset spanning 39 languages.](https://huggingface.co/datasets/neulab/PangeaInstruct)   

	 路 ([Pangea](https://github.com/neulab/Pangea/tree/main) - neulab) ![Star](https://img.shields.io/github/stars/neulab/Pangea.svg?style=social&label=Star)

## Alignment

- [**Open-O1**](https://github.com/Open-Source-O1/Open-O1) - Open-Source-O1 ![Star](https://img.shields.io/github/stars/Open-Source-O1/Open-O1.svg?style=social&label=Star) 

	 *A Model Matching Proprietary Power with Open-Source Innovation*
-  **Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs 
  with Nothing**, `arXiv, 2406.08464`, [arxiv](http://arxiv.org/abs/2406.08464v2), [pdf](http://arxiv.org/pdf/2406.08464v2.pdf), cication: [**-1**](None) 

	 *Zhangchen Xu, Fengqing Jiang, Luyao Niu, ..., Yejin Choi, Bill Yuchen Lin* 路 ([huggingface](https://huggingface.co/Magpie-Align)) 路 ([magpie](https://github.com/magpie-align/magpie) - magpie-align) ![Star](https://img.shields.io/github/stars/magpie-align/magpie.svg?style=social&label=Star)
- [Tulu V2.5 Suite  updated 6 days ago A suite of models trained using DPO and PPO across a wide variety (up to 14) of preference datasets.](https://huggingface.co/collections/allenai/tulu-v25-suite-66676520fd578080e126f618)   
- [Llama 3.1 Tulu 3 70B Preference Mixture](https://huggingface.co/datasets/allenai/llama-3.1-tulu-3-70b-preference-mixture)   
- [Tulu 3 SFT Mixture](https://huggingface.co/datasets/allenai/tulu-3-sft-mixture)   
- **LongReward: Improving Long-context Large Language Models with AI 
  Feedback**, `arXiv, 2410.21252`, [arxiv](http://arxiv.org/abs/2410.21252v1), [pdf](http://arxiv.org/pdf/2410.21252v1.pdf), cication: [**-1**](None)

	 *Jiajie Zhang, Zhongni Hou, Xin Lv, ..., Ling Feng, Juanzi Li* 路 ([LongReward](https://github.com/THUDM/LongReward) - THUDM) ![Star](https://img.shields.io/github/stars/THUDM/LongReward.svg?style=social&label=Star) 路 ([huggingface](https://huggingface.co/datasets/THUDM/LongReward-10k))

## Synthetic

- [WizardArena: Post-training Large Language Models via Simulated Offline Chatbot Aren](https://openreview.net/forum?id=VHva3d836i) 

	 路 ([arxiv](https://arxiv.org/abs/2407.10627v1))
- [synthetic set of instruction pairs where both the prompts and the responses have been synthetically generated, using the AgentInstruct framework.](https://huggingface.co/datasets/microsoft/orca-agentinstruct-1M-v1)   
-  **WizardLM: Empowering Large Language Models to Follow Complex 
  Instructions**, `arXiv, 2304.12244`, [arxiv](http://arxiv.org/abs/2304.12244v2), [pdf](http://arxiv.org/pdf/2304.12244v2.pdf), cication: [**-1**](None) 

	 *Can Xu, Qingfeng Sun, Kai Zheng, ..., Chongyang Tao, Daxin Jiang*
- **Fictitious Synthetic Data Can Improve LLM Factuality via Prerequisite 
  Learning**, `arXiv, 2410.19290`, [arxiv](http://arxiv.org/abs/2410.19290v1), [pdf](http://arxiv.org/pdf/2410.19290v1.pdf), cication: [**-1**](None)

	 *Yujian Liu, Shiyu Chang, Tommi Jaakkola, ..., Yang Zhang* 路 ([Prereq_tune.git](https://github.com/UCSB-NLP-Chang/Prereq_tune.git) - UCSB-NLP-Chang) ![Star](https://img.shields.io/github/stars/UCSB-NLP-Chang/Prereq_tune.git.svg?style=social&label=Star)
- [**promptwright**](https://github.com/StacklokLabs/promptwright) - StacklokLabs ![Star](https://img.shields.io/github/stars/StacklokLabs/promptwright.svg?style=social&label=Star) 

	 路 ([reddit](https://www.reddit.com/r/LocalLLaMA/comments/1ge9192/we_just_open_sourced_promptwright_generate_large/))
- **Scaling Synthetic Data Creation with 1,000,000,000 Personas**, `arXiv, 2406.20094`, [arxiv](http://arxiv.org/abs/2406.20094v2), [pdf](http://arxiv.org/pdf/2406.20094v2.pdf), cication: [**17**](https://scholar.google.com/scholar?cites=15219174281717374109&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII) 

	 *Tao Ge, Xin Chan, Xiaoyang Wang, ..., Haitao Mi, Dong Yu*
- **Unleashing Reasoning Capability of LLMs via Scalable Question Synthesis 
  from Scratch**, `arXiv, 2410.18693`, [arxiv](http://arxiv.org/abs/2410.18693v1), [pdf](http://arxiv.org/pdf/2410.18693v1.pdf), cication: [**-1**](None)

	 *Yuyang Ding, Xinyu Shi, Xiaobo Liang, ..., Qiaoming Zhu, Min Zhang*

## Reasoning


## Action


## Toolkits

- [**dolma**](https://github.com/allenai/dolma) - allenai ![Star](https://img.shields.io/github/stars/allenai/dolma.svg?style=social&label=Star) 
- [**docling**](https://github.com/DS4SD/docling) - DS4SD ![Star](https://img.shields.io/github/stars/DS4SD/docling.svg?style=social&label=Star) 
- [**maxun**](https://github.com/getmaxun/maxun) - getmaxun ![Star](https://img.shields.io/github/stars/getmaxun/maxun.svg?style=social&label=Star) 

## Misc