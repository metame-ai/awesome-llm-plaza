# LLM Data

- [LLM Data](#llm-data) 
  - [Survey](#survey)
  - [LLM Data](#llm-data-1)
  - [Multi Modal](#multi-modal)
  - [Alignment](#alignment)
  - [Synthetic](#synthetic)
  - [Reasoning](#reasoning)
  - [Action](#action)
  - [Toolkits](#toolkits)
  - [Misc](#misc)


## Survey

- **A Survey on Data Synthesis and Augmentation for Large Language Models**, `arXiv, 2410.12896`, [arxiv](http://arxiv.org/abs/2410.12896v1), [pdf](http://arxiv.org/pdf/2410.12896v1.pdf), cication: [**-1**](None) 

	 *Ke Wang, Jiahui Zhu, Minjie Ren, ..., Qingjie Liu, Yunhong Wang*

## LLM Data

- **CLIMB: CLustering-based Iterative Data Mixture Bootstrapping for 
  Language Model Pre-training**, `arXiv, 2504.13161`, [arxiv](http://arxiv.org/abs/2504.13161v1), [pdf](http://arxiv.org/pdf/2504.13161v1.pdf), cication: [**-1**](None) 

	 *Shizhe Diao, Yu Yang, Yonggan Fu, ..., Jan Kautz, Pavlo Molchanov* · ([research.nvidia](https://research.nvidia.com/labs/lpr/climb/))
- 🌟 **MIG: Automatic Data Selection for Instruction Tuning by Maximizing 
  Information Gain in Semantic Space**, `arXiv, 2504.13835`, [arxiv](http://arxiv.org/abs/2504.13835v1), [pdf](http://arxiv.org/pdf/2504.13835v1.pdf), cication: [**-1**](None) 

	 *Yicheng Chen, Yining Li, Kai Hu, ..., Haochen Ye, Kai Chen* · ([yichengchen24.github](https://yichengchen24.github.io/projects/mig)) · ([MIG](https://github.com/yichengchen24/MIG) - yichengchen24) ![Star](https://img.shields.io/github/stars/yichengchen24/MIG.svg?style=social&label=Star)
- **Pre-DPO: Improving Data Utilization in Direct Preference Optimization 
  Using a Guiding Reference Model**, `arXiv, 2504.15843`, [arxiv](http://arxiv.org/abs/2504.15843v2), [pdf](http://arxiv.org/pdf/2504.15843v2.pdf), cication: [**-1**](None) 

	 *Junshu Pan, Wei Shen, Shulin Huang, ..., Qiji Zhou, Yue Zhang* · ([Pre-DPO](https://github.com/DtYXs/Pre-DPO) - DtYXs) ![Star](https://img.shields.io/github/stars/DtYXs/Pre-DPO.svg?style=social&label=Star)
- [filtered version of DCLM dataset using FineWeb-Edu educational quality classifier.](https://huggingface.co/datasets/HuggingFaceTB/dclm-edu)  🤗 
- **Predictive Data Selection: The Data That Predicts Is the Data That 
  Teaches**, `arXiv, 2503.00808`, [arxiv](http://arxiv.org/abs/2503.00808v2), [pdf](http://arxiv.org/pdf/2503.00808v2.pdf), cication: [**-1**](None) 

	 *Kashun Shum, Yuzhen Huang, Hongjian Zou, ..., Qian Liu, Junxian He* · ([PreSelect.](https://github.com/hkust-nlp/PreSelect.) - hkust-nlp) ![Star](https://img.shields.io/github/stars/hkust-nlp/PreSelect..svg?style=social&label=Star)
- **GneissWeb: Preparing High Quality Data for LLMs at Scale**, `arXiv, 2502.14907`, [arxiv](http://arxiv.org/abs/2502.14907v1), [pdf](http://arxiv.org/pdf/2502.14907v1.pdf), cication: [**-1**](None) 

	 *Hajar Emami Gohari, Swanand Ravindra Kadhe, Syed Yousaf Shah. Constantin Adam, ..., Yi Zhou, Bishwaranjan Bhattacharjee*
- **Craw4LLM: Efficient Web Crawling for LLM Pretraining**, `arXiv, 2502.13347`, [arxiv](http://arxiv.org/abs/2502.13347v2), [pdf](http://arxiv.org/pdf/2502.13347v2.pdf), cication: [**-1**](None) 

	 *Shi Yu, Zhiyuan Liu, Chenyan Xiong* · ([Craw4LLM.](https://github.com/cxcscmu/Craw4LLM.) - cxcscmu) ![Star](https://img.shields.io/github/stars/cxcscmu/Craw4LLM..svg?style=social&label=Star)
- [OpenR1-Math-220k is a large-scale dataset for mathematical reasoning.](https://huggingface.co/datasets/open-r1/OpenR1-Math-220k)  🤗 
- [**nv-ingest**](https://github.com/NVIDIA/nv-ingest) - NVIDIA ![Star](https://img.shields.io/github/stars/NVIDIA/nv-ingest.svg?style=social&label=Star) 

	 *Multi-modal data extraction*
- 🌟 **Towards Best Practices for Open Datasets for LLM Training**, `arXiv, 2501.08365`, [arxiv](http://arxiv.org/abs/2501.08365v1), [pdf](http://arxiv.org/pdf/2501.08365v1.pdf), cication: [**-1**](None) 

	 *Stefan Baack, Stella Biderman, Kasia Odrozek, ..., Lee White, Thomas Wolf*
- [Improving Foundation Models Using Expert Human Data](https://llm-class.github.io/speakers.html) 
- **Bridging the Data Provenance Gap Across Text, Speech and Video**, `arXiv, 2412.17847`, [arxiv](http://arxiv.org/abs/2412.17847v1), [pdf](http://arxiv.org/pdf/2412.17847v1.pdf), cication: [**-1**](None) 

	 *Shayne Longpre, Nikhil Singh, Manuel Cherep, ..., Sara Hooker, Jad Kabbara*
- [FineMath consists of 34B tokens (FineMath-3+) and 54B tokens (FineMath-3+ with InfiMM-WebMath-3+) of mathematical educational content filtered from CommonCrawl.](https://huggingface.co/datasets/HuggingFaceTB/finemath)  🤗 

	 · ([𝕏](https://x.com/anton_lozhkov/status/1869771053146464507))
- 🌟 [**fineweb-2**](https://github.com/huggingface/fineweb-2) - huggingface ![Star](https://img.shields.io/github/stars/huggingface/fineweb-2.svg?style=social&label=Star) 

	 · ([huggingface](https://huggingface.co/datasets/HuggingFaceFW/fineweb-2))
- [**MultimodalUniverse**](https://github.com/MultimodalUniverse/MultimodalUniverse) - MultimodalUniverse ![Star](https://img.shields.io/github/stars/MultimodalUniverse/MultimodalUniverse.svg?style=social&label=Star) 

	 *Enabling Large-Scale Machine Learning with 100TBs of Astronomical Scientific Data* · ([𝕏](https://x.com/MilesCranmer/status/1863668192519839793))
- **Zyda-2: a 5 Trillion Token High-Quality Dataset**, `arXiv, 2411.06068`, [arxiv](http://arxiv.org/abs/2411.06068v1), [pdf](http://arxiv.org/pdf/2411.06068v1.pdf), cication: [**-1**](None) 

	 *Yury Tokpanov, Paolo Glorioso, Quentin Anthony, ..., Beren Millidge* · ([huggingface](https://huggingface.co/datasets/Zyphra/Zyda-2))
- [**dolma**](https://github.com/allenai/dolma) - allenai ![Star](https://img.shields.io/github/stars/allenai/dolma.svg?style=social&label=Star) 

	 *an open dataset of 3 trillion tokens from a diverse mix of web content, academic publications, code, books, and encyclopedic materials.*
- 🌟 **RedPajama: an Open Dataset for Training Large Language Models**, `arXiv, 2411.12372`, [arxiv](http://arxiv.org/abs/2411.12372v1), [pdf](http://arxiv.org/pdf/2411.12372v1.pdf), cication: [**-1**](None) 

	 *Maurice Weber, Daniel Fu, Quentin Anthony, ..., Irina Rish, Ce Zhang*
- 🌟 [**smollm**](https://github.com/huggingface/smollm) - huggingface ![Star](https://img.shields.io/github/stars/huggingface/smollm.svg?style=social&label=Star) 

	 *135M, 360M, and 1.7B parameters.* · ([smollm](https://github.com/huggingface/smollm) - huggingface) ![Star](https://img.shields.io/github/stars/huggingface/smollm.svg?style=social&label=Star) · ([𝕏](https://x.com/_philschmid/status/1859598525723488478)) · ([huggingface](https://huggingface.co/datasets/HuggingFaceTB/smoltalk))
- [Zyda-2 is a 5 trillion token language modeling dataset created by collecting open and high quality datasets and combining them and cross-deduplication and model-based quality filtering.](https://huggingface.co/datasets/Zyphra/Zyda-2)  🤗 
- [Common Corpus is the largest open and permissible licensed text dataset, comprising over 2 trillion tokens](https://huggingface.co/datasets/PleIAs/common_corpus)  🤗 
- [Releasing the largest multilingual open pretraining dataset](https://huggingface.co/blog/Pclanglais/two-trillion-tokens-open)  🤗 
- 🌟 **Both Text and Images Leaked! A Systematic Analysis of Multimodal LLM 
  Data Contamination**, `arXiv, 2411.03823`, [arxiv](http://arxiv.org/abs/2411.03823v1), [pdf](http://arxiv.org/pdf/2411.03823v1.pdf), cication: [**-1**](None) 

	 *Dingjie Song, Sicheng Lai, Shunian Chen, ..., Lichao Sun, Benyou Wang* · ([MM-Detect](https://github.com/MLLM-Data-Contamination/MM-Detect) - MLLM-Data-Contamination) ![Star](https://img.shields.io/github/stars/MLLM-Data-Contamination/MM-Detect.svg?style=social&label=Star)
- **Document Parsing Unveiled: Techniques, Challenges, and Prospects for 
  Structured Information Extraction**, `arXiv, 2410.21169`, [arxiv](http://arxiv.org/abs/2410.21169v2), [pdf](http://arxiv.org/pdf/2410.21169v2.pdf), cication: [**-1**](None)

	 *Qintong Zhang, Victor Shea-Jay Huang, Bin Wang, ..., Conghui He, Wentao Zhang*
- [Dolma is a dataset of 3 trillion tokens from a diverse mix of web content, academic publications, code, books, and encyclopedic materials.](https://huggingface.co/datasets/allenai/dolma)  🤗 

	 · ([arxiv](https://arxiv.org/abs/2402.00159))
- [Arxiver consists of 63,357 arXiv papers converted to multi-markdown (.mmd) format.](https://huggingface.co/datasets/neuralwork/arxiver)  🤗 
- **Compute-Constrained Data Selection**, `arXiv, 2410.16208`, [arxiv](http://arxiv.org/abs/2410.16208v1), [pdf](http://arxiv.org/pdf/2410.16208v1.pdf), cication: [**-1**](None) 

	 *Junjie Oscar Yin, Alexander M. Rush*

## Multi Modal

- 🌟 [A set of vision-language datasets built by Ai2 and used to train the Molmo family of models.](https://huggingface.co/collections/allenai/pixmo-674746ea613028006285687b)  🤗 
- [PangeaIns is a 6M multilingual multicultural multimodal instruction tuning dataset spanning 39 languages.](https://huggingface.co/datasets/neulab/PangeaInstruct)  🤗 

	 · ([Pangea](https://github.com/neulab/Pangea/tree/main) - neulab) ![Star](https://img.shields.io/github/stars/neulab/Pangea.svg?style=social&label=Star)

## Alignment

- [**Open-O1**](https://github.com/Open-Source-O1/Open-O1) - Open-Source-O1 ![Star](https://img.shields.io/github/stars/Open-Source-O1/Open-O1.svg?style=social&label=Star) 

	 *A Model Matching Proprietary Power with Open-Source Innovation*
- 🌟 **Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs 
  with Nothing**, `arXiv, 2406.08464`, [arxiv](http://arxiv.org/abs/2406.08464v2), [pdf](http://arxiv.org/pdf/2406.08464v2.pdf), cication: [**-1**](None) 

	 *Zhangchen Xu, Fengqing Jiang, Luyao Niu, ..., Yejin Choi, Bill Yuchen Lin* · ([huggingface](https://huggingface.co/Magpie-Align)) · ([magpie](https://github.com/magpie-align/magpie) - magpie-align) ![Star](https://img.shields.io/github/stars/magpie-align/magpie.svg?style=social&label=Star)
- [Tulu V2.5 Suite  updated 6 days ago A suite of models trained using DPO and PPO across a wide variety (up to 14) of preference datasets.](https://huggingface.co/collections/allenai/tulu-v25-suite-66676520fd578080e126f618)  🤗 
- [Llama 3.1 Tulu 3 70B Preference Mixture](https://huggingface.co/datasets/allenai/llama-3.1-tulu-3-70b-preference-mixture)  🤗 
- [Tulu 3 SFT Mixture](https://huggingface.co/datasets/allenai/tulu-3-sft-mixture)  🤗 
- **LongReward: Improving Long-context Large Language Models with AI 
  Feedback**, `arXiv, 2410.21252`, [arxiv](http://arxiv.org/abs/2410.21252v1), [pdf](http://arxiv.org/pdf/2410.21252v1.pdf), cication: [**-1**](None)

	 *Jiajie Zhang, Zhongni Hou, Xin Lv, ..., Ling Feng, Juanzi Li* · ([LongReward](https://github.com/THUDM/LongReward) - THUDM) ![Star](https://img.shields.io/github/stars/THUDM/LongReward.svg?style=social&label=Star) · ([huggingface](https://huggingface.co/datasets/THUDM/LongReward-10k))

## Synthetic

- **Improving the Scaling Laws of Synthetic Data with Deliberate Practice**, `arXiv, 2502.15588`, [arxiv](http://arxiv.org/abs/2502.15588v1), [pdf](http://arxiv.org/pdf/2502.15588v1.pdf), cication: [**-1**](None) 

	 *Reyhane Askari-Hemmat, Mohammad Pezeshki, Elvis Dohmatob, ..., Michal Drozdzal, Adriana Romero-Soriano*
- **WILDCHAT-50M: A Deep Dive Into the Role of Synthetic Data in 
  Post-Training**, `arXiv, 2501.18511`, [arxiv](http://arxiv.org/abs/2501.18511v1), [pdf](http://arxiv.org/pdf/2501.18511v1.pdf), cication: [**-1**](None) 

	 *Benjamin Feuer, Chinmay Hegde*
- **Condor: Enhance LLM Alignment with Knowledge-Driven Data Synthesis and 
  Refinement**, `arXiv, 2501.12273`, [arxiv](http://arxiv.org/abs/2501.12273v1), [pdf](http://arxiv.org/pdf/2501.12273v1.pdf), cication: [**-1**](None) 

	 *Maosong Cao, Taolin Zhang, Mo Li, ..., Songyang Zhang, Kai Chen* · ([Condor](https://github.com/InternLM/Condor) - InternLM) ![Star](https://img.shields.io/github/stars/InternLM/Condor.svg?style=social&label=Star) · ([hf](https://hf.co/datasets/internlm/Condor-SFT-20K))
- [**curator**](https://github.com/bespokelabsai/curator) - bespokelabsai ![Star](https://img.shields.io/github/stars/bespokelabsai/curator.svg?style=social&label=Star) 

	 · ([𝕏](https://x.com/madiator/status/1879579213554147665))
- [Best of 2024: Synthetic Data / Smol Models, Loubna Ben Allal, HuggingFace [LS Live! @ NeurIPS 2024]](https://www.youtube.com/watch?v=AjmdDy7Rzx0)  :clapper: 
- **How to Synthesize Text Data without Model Collapse?**, `arXiv, 2412.14689`, [arxiv](http://arxiv.org/abs/2412.14689v1), [pdf](http://arxiv.org/pdf/2412.14689v1.pdf), cication: [**-1**](None) 

	 *Xuekai Zhu, Daixuan Cheng, Hengli Li, ..., Zilong Zheng, Bowen Zhou*
- **Evaluating Language Models as Synthetic Data Generators**, `arXiv, 2412.03679`, [arxiv](http://arxiv.org/abs/2412.03679v1), [pdf](http://arxiv.org/pdf/2412.03679v1.pdf), cication: [**-1**](None) 

	 *Seungone Kim, Juyoung Suk, Xiang Yue, ..., Sean Welleck, Graham Neubig*
- [WizardArena: Post-training Large Language Models via Simulated Offline Chatbot Aren](https://openreview.net/forum?id=VHva3d836i) 

	 · ([arxiv](https://arxiv.org/abs/2407.10627v1))
- [synthetic set of instruction pairs where both the prompts and the responses have been synthetically generated, using the AgentInstruct framework.](https://huggingface.co/datasets/microsoft/orca-agentinstruct-1M-v1)  🤗 
- 🌟 **WizardLM: Empowering Large Language Models to Follow Complex 
  Instructions**, `arXiv, 2304.12244`, [arxiv](http://arxiv.org/abs/2304.12244v2), [pdf](http://arxiv.org/pdf/2304.12244v2.pdf), cication: [**-1**](None) 

	 *Can Xu, Qingfeng Sun, Kai Zheng, ..., Chongyang Tao, Daxin Jiang*
- **Fictitious Synthetic Data Can Improve LLM Factuality via Prerequisite 
  Learning**, `arXiv, 2410.19290`, [arxiv](http://arxiv.org/abs/2410.19290v1), [pdf](http://arxiv.org/pdf/2410.19290v1.pdf), cication: [**-1**](None)

	 *Yujian Liu, Shiyu Chang, Tommi Jaakkola, ..., Yang Zhang* · ([Prereq_tune.git](https://github.com/UCSB-NLP-Chang/Prereq_tune.git) - UCSB-NLP-Chang) ![Star](https://img.shields.io/github/stars/UCSB-NLP-Chang/Prereq_tune.git.svg?style=social&label=Star)
- [**promptwright**](https://github.com/StacklokLabs/promptwright) - StacklokLabs ![Star](https://img.shields.io/github/stars/StacklokLabs/promptwright.svg?style=social&label=Star) 

	 · ([reddit](https://www.reddit.com/r/LocalLLaMA/comments/1ge9192/we_just_open_sourced_promptwright_generate_large/))
- **Scaling Synthetic Data Creation with 1,000,000,000 Personas**, `arXiv, 2406.20094`, [arxiv](http://arxiv.org/abs/2406.20094v2), [pdf](http://arxiv.org/pdf/2406.20094v2.pdf), cication: [**17**](https://scholar.google.com/scholar?cites=15219174281717374109&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII) 

	 *Tao Ge, Xin Chan, Xiaoyang Wang, ..., Haitao Mi, Dong Yu*
- **Unleashing Reasoning Capability of LLMs via Scalable Question Synthesis 
  from Scratch**, `arXiv, 2410.18693`, [arxiv](http://arxiv.org/abs/2410.18693v1), [pdf](http://arxiv.org/pdf/2410.18693v1.pdf), cication: [**-1**](None)

	 *Yuyang Ding, Xinyu Shi, Xiaobo Liang, ..., Qiaoming Zhu, Min Zhang*

## Reasoning

- **DeepMath-103K: A Large-Scale, Challenging, Decontaminated, and 
  Verifiable Mathematical Dataset for Advancing Reasoning**, `arXiv, 2504.11456`, [arxiv](http://arxiv.org/abs/2504.11456v1), [pdf](http://arxiv.org/pdf/2504.11456v1.pdf), cication: [**-1**](None) 

	 *Zhiwei He, Tian Liang, Jiahao Xu, ..., Haitao Mi, Dong Yu* · ([DeepMath.](https://github.com/zwhe99/DeepMath.) - zwhe99) ![Star](https://img.shields.io/github/stars/zwhe99/DeepMath..svg?style=social&label=Star)
- [**Chinese-Data-Distill-From-R1**](https://github.com/YunwenTechnology/Chinese-Data-Distill-From-R1) - YunwenTechnology ![Star](https://img.shields.io/github/stars/YunwenTechnology/Chinese-Data-Distill-From-R1.svg?style=social&label=Star) 
- **NaturalReasoning: Reasoning in the Wild with 2.8M Challenging Questions**, `arXiv, 2502.13124`, [arxiv](http://arxiv.org/abs/2502.13124v2), [pdf](http://arxiv.org/pdf/2502.13124v2.pdf), cication: [**-1**](None) 

	 *Weizhe Yuan, Jane Yu, Song Jiang, ..., Jason E Weston, Xian Li*
- [NuminaMath-1.5 by AI-MO](https://www.interconnects.ai/p/artifacts-7) 

	 · ([huggingface](https://huggingface.co/datasets/AI-MO/NuminaMath-1.5))
- [OpenR1-Math-220k by open-r1: A subset of NuminaMath 1.5 with reasoning traces generated by DeepSeek R1.](https://www.interconnects.ai/p/artifacts-7) 
- [stackexchange-question-answering by PrimeIntellect: Prime Intellect has released SYNTHETIC-1, a dataset collection of code and math problems with R1-generated answers.](https://www.interconnects.ai/p/artifacts-7) 
- [**open-thoughts**](https://github.com/open-thoughts/open-thoughts) - open-thoughts ![Star](https://img.shields.io/github/stars/open-thoughts/open-thoughts.svg?style=social&label=Star) 

	 · ([𝕏](https://x.com/madiator/status/1884284103354376283))

## Action


## Toolkits

- [**Dataset-Tools**](https://github.com/Ktiseos-Nyx/Dataset-Tools) - Ktiseos-Nyx ![Star](https://img.shields.io/github/stars/Ktiseos-Nyx/Dataset-Tools.svg?style=social&label=Star) 

	 *A Simple Viewer for EXIF and AI Metadata*
- [**fastText**](https://github.com/facebookresearch/fastText) - facebookresearch ![Star](https://img.shields.io/github/stars/facebookresearch/fastText.svg?style=social&label=Star) 
- [**dolma**](https://github.com/allenai/dolma) - allenai ![Star](https://img.shields.io/github/stars/allenai/dolma.svg?style=social&label=Star) 
- [**docling**](https://github.com/DS4SD/docling) - DS4SD ![Star](https://img.shields.io/github/stars/DS4SD/docling.svg?style=social&label=Star) 
- [**maxun**](https://github.com/getmaxun/maxun) - getmaxun ![Star](https://img.shields.io/github/stars/getmaxun/maxun.svg?style=social&label=Star) 

## Misc