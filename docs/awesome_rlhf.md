# Awesome RLHF

- [Awesome RLHF](#awesome-rlhf)
	- [Survey](#survey)
	- [Papers](#papers)
	- [Projects](#projects)
	- [Other](#other)
	- [Extra reference](#extra-reference)


## Survey


## Papers
- **LiPO: Listwise Preference Optimization through Learning-to-Rank**, `arXiv, 2402.01878`, [arxiv](http://arxiv.org/abs/2402.01878v1), [pdf](http://arxiv.org/pdf/2402.01878v1.pdf), cication: [**-1**](None)

	 *Tianqi Liu, Zhen Qin, Junru Wu, Jiaming Shen, Misha Khalman, Rishabh Joshi, Yao Zhao, Mohammad Saleh, Simon Baumgartner, Jialu Liu*
- **StepCoder: Improve Code Generation with Reinforcement Learning from
  Compiler Feedback**, `arXiv, 2402.01391`, [arxiv](http://arxiv.org/abs/2402.01391v2), [pdf](http://arxiv.org/pdf/2402.01391v2.pdf), cication: [**-1**](None)

	 *Shihan Dou, Yan Liu, Haoxiang Jia, Limao Xiong, Enyu Zhou, Wei Shen, Junjie Shan, Caishuang Huang, Xiao Wang, Xiaoran Fan*
- **Transforming and Combining Rewards for Aligning Large Language Models**, `arXiv, 2402.00742`, [arxiv](http://arxiv.org/abs/2402.00742v1), [pdf](http://arxiv.org/pdf/2402.00742v1.pdf), cication: [**-1**](None)

	 *Zihao Wang, Chirag Nagpal, Jonathan Berant, Jacob Eisenstein, Alex D'Amour, Sanmi Koyejo, Victor Veitch*
- **Aligning Large Language Models with Counterfactual DPO**, `arXiv, 2401.09566`, [arxiv](http://arxiv.org/abs/2401.09566v2), [pdf](http://arxiv.org/pdf/2401.09566v2.pdf), cication: [**-1**](None)

	 *Bradley Butcher*
- **WARM: On the Benefits of Weight Averaged Reward Models**, `arXiv, 2401.12187`, [arxiv](http://arxiv.org/abs/2401.12187v1), [pdf](http://arxiv.org/pdf/2401.12187v1.pdf), cication: [**-1**](None)

	 *Alexandre Ramé, Nino Vieillard, Léonard Hussenot, Robert Dadashi, Geoffrey Cideron, Olivier Bachem, Johan Ferret*
- **ReFT: Reasoning with Reinforced Fine-Tuning**, `arXiv, 2401.08967`, [arxiv](http://arxiv.org/abs/2401.08967v1), [pdf](http://arxiv.org/pdf/2401.08967v1.pdf), cication: [**-1**](None)

	 *Trung Quoc Luong, Xinbo Zhang, Zhanming Jie, Peng Sun, Xiaoran Jin, Hang Li*
- **Self-Rewarding Language Models**, `arXiv, 2401.10020`, [arxiv](http://arxiv.org/abs/2401.10020v1), [pdf](http://arxiv.org/pdf/2401.10020v1.pdf), cication: [**-1**](None)

	 *Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, Jason Weston*
- **Contrastive Preference Optimization: Pushing the Boundaries of LLM
  Performance in Machine Translation**, `arXiv, 2401.08417`, [arxiv](http://arxiv.org/abs/2401.08417v1), [pdf](http://arxiv.org/pdf/2401.08417v1.pdf), cication: [**-1**](None)

	 *Haoran Xu, Amr Sharaf, Yunmo Chen, Weiting Tan, Lingfeng Shen, Benjamin Van Durme, Kenton Murray, Young Jin Kim*
- **Secrets of RLHF in Large Language Models Part II: Reward Modeling**, `arXiv, 2401.06080`, [arxiv](http://arxiv.org/abs/2401.06080v1), [pdf](http://arxiv.org/pdf/2401.06080v1.pdf), cication: [**-1**](None)

	 *Binghai Wang, Rui Zheng, Lu Chen, Yan Liu, Shihan Dou, Caishuang Huang, Wei Shen, Senjie Jin, Enyu Zhou, Chenyu Shi*

	 · ([jiqizhixin](https://www.jiqizhixin.com/articles/2024-01-15-19))
- **ICE-GRT: Instruction Context Enhancement by Generative Reinforcement
  based Transformers**, `arXiv, 2401.02072`, [arxiv](http://arxiv.org/abs/2401.02072v1), [pdf](http://arxiv.org/pdf/2401.02072v1.pdf), cication: [**-1**](None)

	 *Chen Zheng, Ke Sun, Da Tang, Yukun Ma, Yuyu Zhang, Chenguang Xi, Xun Zhou*
- **InstructVideo: Instructing Video Diffusion Models with Human Feedback**, `arXiv, 2312.12490`, [arxiv](http://arxiv.org/abs/2312.12490v1), [pdf](http://arxiv.org/pdf/2312.12490v1.pdf), cication: [**-1**](None)

	 *Hangjie Yuan, Shiwei Zhang, Xiang Wang, Yujie Wei, Tao Feng, Yining Pan, Yingya Zhang, Ziwei Liu, Samuel Albanie, Dong Ni*
- **Silkie: Preference Distillation for Large Visual Language Models**, `arXiv, 2312.10665`, [arxiv](http://arxiv.org/abs/2312.10665v1), [pdf](http://arxiv.org/pdf/2312.10665v1.pdf), cication: [**-1**](None)

	 *Lei Li, Zhihui Xie, Mukai Li, Shunian Chen, Peiyi Wang, Liang Chen, Yazheng Yang, Benyou Wang, Lingpeng Kong*
- **Align on the Fly: Adapting Chatbot Behavior to Established Norms**, `arXiv, 2312.15907`, [arxiv](http://arxiv.org/abs/2312.15907v1), [pdf](http://arxiv.org/pdf/2312.15907v1.pdf), cication: [**-1**](None)

	 *Chunpu Xu, Steffi Chern, Ethan Chern, Ge Zhang, Zekun Wang, Ruibo Liu, Jing Li, Jie Fu, Pengfei Liu* · ([jiqizhixin](https://www.jiqizhixin.com/articles/2024-01-24)) · ([OPO](https://github.com/GAIR-NLP/OPO) - GAIR-NLP) ![Star](https://img.shields.io/github/stars/GAIR-NLP/OPO.svg?style=social&label=Star) · ([gair-nlp.github](https://gair-nlp.github.io/OPO/))
- **Helping or Herding? Reward Model Ensembles Mitigate but do not Eliminate
  Reward Hacking**, `arXiv, 2312.09244`, [arxiv](http://arxiv.org/abs/2312.09244v1), [pdf](http://arxiv.org/pdf/2312.09244v1.pdf), cication: [**-1**](None)

	 *Jacob Eisenstein, Chirag Nagpal, Alekh Agarwal, Ahmad Beirami, Alex D'Amour, DJ Dvijotham, Adam Fisch, Katherine Heller, Stephen Pfohl, Deepak Ramachandran*
- **Beyond Human Data: Scaling Self-Training for Problem-Solving with
  Language Models**, `arXiv, 2312.06585`, [arxiv](http://arxiv.org/abs/2312.06585v1), [pdf](http://arxiv.org/pdf/2312.06585v1.pdf), cication: [**-1**](None)

	 *Avi Singh, John D. Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil, Peter J. Liu, James Harrison, Jaehoon Lee, Kelvin Xu, Aaron Parisi*
- [**HALOs**](https://github.com/ContextualAI/HALOs?tab=readme-ov-file) - ContextualAI ![Star](https://img.shields.io/github/stars/ContextualAI/HALOs?tab=readme-ov-file.svg?style=social&label=Star)

	 *Human-Centered Loss Functions (HALOs)* · ([HALOs](https://github.com/ContextualAI/HALOs/blob/main/assets/report.pdf) - ContextualAI) ![Star](https://img.shields.io/github/stars/ContextualAI/HALOs.svg?style=social&label=Star)
- **Axiomatic Preference Modeling for Longform Question Answering**, `arXiv, 2312.02206`, [arxiv](http://arxiv.org/abs/2312.02206v1), [pdf](http://arxiv.org/pdf/2312.02206v1.pdf), cication: [**-1**](None)

	 *Corby Rosset, Guoqing Zheng, Victor Dibia, Ahmed Awadallah, Paul Bennett* · ([huggingface](https://huggingface.co/corbyrosset/axiomatic_preference_model))
- **Nash Learning from Human Feedback**, `arXiv, 2312.00886`, [arxiv](http://arxiv.org/abs/2312.00886v2), [pdf](http://arxiv.org/pdf/2312.00886v2.pdf), cication: [**-1**](None)

	 *Rémi Munos, Michal Valko, Daniele Calandriello, Mohammad Gheshlaghi Azar, Mark Rowland, Zhaohan Daniel Guo, Yunhao Tang, Matthieu Geist, Thomas Mesnard, Andrea Michi*
- **RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from
  Fine-grained Correctional Human Feedback**, `arXiv, 2312.00849`, [arxiv](http://arxiv.org/abs/2312.00849v1), [pdf](http://arxiv.org/pdf/2312.00849v1.pdf), cication: [**-1**](None)

	 *Tianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng Han, Ganqu Cui, Jinyi Hu, Zhiyuan Liu, Hai-Tao Zheng, Maosong Sun* · ([RLHF-V](https://github.com/RLHF-V/RLHF-V) - RLHF-V) ![Star](https://img.shields.io/github/stars/RLHF-V/RLHF-V.svg?style=social&label=Star)
- [Starling-7B: Increasing LLM Helpfulness & Harmlessness with RLAIF](https://starling.cs.berkeley.edu/)
- **Adversarial Preference Optimization**, `arXiv, 2311.08045`, [arxiv](http://arxiv.org/abs/2311.08045v1), [pdf](http://arxiv.org/pdf/2311.08045v1.pdf), cication: [**-1**](None)

	 *Pengyu Cheng, Yifan Yang, Jian Li, Yong Dai, Nan Du*

	 · ([mp.weixin.qq](https://mp.weixin.qq.com/s?__biz=MzAxMTk4NDkwNw==&mid=2247495391&idx=1&sn=51407a77e2d0277d958adaf533f6ca5b))
- **Diffusion Model Alignment Using Direct Preference Optimization**, `arXiv, 2311.12908`, [arxiv](http://arxiv.org/abs/2311.12908v1), [pdf](http://arxiv.org/pdf/2311.12908v1.pdf), cication: [**-1**](None)

	 *Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, Nikhil Naik*
- **Black-Box Prompt Optimization: Aligning Large Language Models without
  Model Training**, `arXiv, 2311.04155`, [arxiv](http://arxiv.org/abs/2311.04155v2), [pdf](http://arxiv.org/pdf/2311.04155v2.pdf), cication: [**-1**](None)

	 *Jiale Cheng, Xiao Liu, Kehan Zheng, Pei Ke, Hongning Wang, Yuxiao Dong, Jie Tang, Minlie Huang* · ([bpo](https://github.com/thu-coai/bpo) - thu-coai) ![Star](https://img.shields.io/github/stars/thu-coai/bpo.svg?style=social&label=Star)
- **Towards Understanding Sycophancy in Language Models**, `arXiv, 2310.13548`, [arxiv](http://arxiv.org/abs/2310.13548v3), [pdf](http://arxiv.org/pdf/2310.13548v3.pdf), cication: [**-1**](None)

	 *Mrinank Sharma, Meg Tong, Tomasz Korbak, David Duvenaud, Amanda Askell, Samuel R. Bowman, Newton Cheng, Esin Durmus, Zac Hatfield-Dodds, Scott R. Johnston* · ([jiqizhixin](https://www.jiqizhixin.com/articles/2023-10-25-4))
- **Contrastive Preference Learning: Learning from Human Feedback without RL**, `arXiv, 2310.13639`, [arxiv](http://arxiv.org/abs/2310.13639v2), [pdf](http://arxiv.org/pdf/2310.13639v2.pdf), cication: [**-1**](None)

	 *Joey Hejna, Rafael Rafailov, Harshit Sikchi, Chelsea Finn, Scott Niekum, W. Bradley Knox, Dorsa Sadigh* · ([jiqizhixin](https://www.jiqizhixin.com/articles/2023-11-13-6))
- **Don't throw away your value model! Making PPO even better via
  Value-Guided Monte-Carlo Tree Search decoding**, `arXiv, 2309.15028`, [arxiv](http://arxiv.org/abs/2309.15028v2), [pdf](http://arxiv.org/pdf/2309.15028v2.pdf), cication: [**1**](https://scholar.google.com/scholar?cites=12825384516892808854&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Jiacheng Liu, Andrew Cohen, Ramakanth Pasunuru, Yejin Choi, Hannaneh Hajishirzi, Asli Celikyilmaz* · ([jiqizhixin](https://www.jiqizhixin.com/articles/2023-10-27-3))
- [The N Implementation Details of RLHF with PPO](https://huggingface.co/blog/the_n_implementation_details_of_rlhf_with_ppo)
- **Specific versus General Principles for Constitutional AI**, `arXiv, 2310.13798`, [arxiv](http://arxiv.org/abs/2310.13798v1), [pdf](http://arxiv.org/pdf/2310.13798v1.pdf), cication: [**1**](https://scholar.google.com/scholar?cites=10472606437287020754&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Sandipan Kundu, Yuntao Bai, Saurav Kadavath, Amanda Askell, Andrew Callahan, Anna Chen, Anna Goldie, Avital Balwit, Azalia Mirhoseini, Brayden McLean*
- **Contrastive Preference Learning: Learning from Human Feedback without RL**, `arXiv, 2310.13639`, [arxiv](http://arxiv.org/abs/2310.13639v2), [pdf](http://arxiv.org/pdf/2310.13639v2.pdf), cication: [**-1**](None)

	 *Joey Hejna, Rafael Rafailov, Harshit Sikchi, Chelsea Finn, Scott Niekum, W. Bradley Knox, Dorsa Sadigh*
- **A General Theoretical Paradigm to Understand Learning from Human
  Preferences**, `arXiv, 2310.12036`, [arxiv](http://arxiv.org/abs/2310.12036v2), [pdf](http://arxiv.org/pdf/2310.12036v2.pdf), cication: [**1**](https://scholar.google.com/scholar?cites=16028671700226284164&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Mohammad Gheshlaghi Azar, Mark Rowland, Bilal Piot, Daniel Guo, Daniele Calandriello, Michal Valko, Rémi Munos*
- **Tuna: Instruction Tuning using Feedback from Large Language Models**, `arXiv, 2310.13385`, [arxiv](http://arxiv.org/abs/2310.13385v1), [pdf](http://arxiv.org/pdf/2310.13385v1.pdf), cication: [**-1**](None)

	 *Haoran Li, Yiran Liu, Xingxing Zhang, Wei Lu, Furu Wei*
- **Safe RLHF: Safe Reinforcement Learning from Human Feedback**, `arXiv, 2310.12773`, [arxiv](http://arxiv.org/abs/2310.12773v1), [pdf](http://arxiv.org/pdf/2310.12773v1.pdf), cication: [**1**](https://scholar.google.com/scholar?cites=10151978917046355982&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Josef Dai, Xuehai Pan, Ruiyang Sun, Jiaming Ji, Xinbo Xu, Mickel Liu, Yizhou Wang, Yaodong Yang*
- **ReMax: A Simple, Effective, and Efficient Reinforcement Learning Method
  for Aligning Large Language Models**, `arXiv, 2310.10505`, [arxiv](http://arxiv.org/abs/2310.10505v2), [pdf](http://arxiv.org/pdf/2310.10505v2.pdf), cication: [**-1**](None)

	 *Ziniu Li, Tian Xu, Yushun Zhang, Yang Yu, Ruoyu Sun, Zhi-Quan Luo* · ([jiqizhixin](https://www.jiqizhixin.com/articles/2023-10-20-9))
- [Rethinking the Role of PPO in RLHF – The Berkeley Artificial Intelligence Research Blog](https://bair.berkeley.edu/blog/2023/10/16/p3o/)
- **Reinforcement Learning in the Era of LLMs: What is Essential? What is
  needed? An RL Perspective on RLHF, Prompting, and Beyond**, `arXiv, 2310.06147`, [arxiv](http://arxiv.org/abs/2310.06147v1), [pdf](http://arxiv.org/pdf/2310.06147v1.pdf), cication: [**-1**](None)

	 *Hao Sun*
- **A Long Way to Go: Investigating Length Correlations in RLHF**, `arXiv, 2310.03716`, [arxiv](http://arxiv.org/abs/2310.03716v1), [pdf](http://arxiv.org/pdf/2310.03716v1.pdf), cication: [**3**](https://scholar.google.com/scholar?cites=17792312030938285213&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Prasann Singhal, Tanya Goyal, Jiacheng Xu, Greg Durrett*
- **Aligning Large Multimodal Models with Factually Augmented RLHF**, `arXiv, 2309.14525`, [arxiv](http://arxiv.org/abs/2309.14525v1), [pdf](http://arxiv.org/pdf/2309.14525v1.pdf), cication: [**4**](https://scholar.google.com/scholar?cites=17054470781093797244&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang*
- **Stabilizing RLHF through Advantage Model and Selective Rehearsal**, `arXiv, 2309.10202`, [arxiv](http://arxiv.org/abs/2309.10202v1), [pdf](http://arxiv.org/pdf/2309.10202v1.pdf), cication: [**1**](https://scholar.google.com/scholar?cites=16456025046699375201&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Baolin Peng, Linfeng Song, Ye Tian, Lifeng Jin, Haitao Mi, Dong Yu*
- **Statistical Rejection Sampling Improves Preference Optimization**, `arXiv, 2309.06657`, [arxiv](http://arxiv.org/abs/2309.06657v1), [pdf](http://arxiv.org/pdf/2309.06657v1.pdf), cication: [**-1**](None)

	 *Tianqi Liu, Yao Zhao, Rishabh Joshi, Misha Khalman, Mohammad Saleh, Peter J. Liu, Jialu Liu*
- **Efficient RLHF: Reducing the Memory Usage of PPO**, `arXiv, 2309.00754`, [arxiv](http://arxiv.org/abs/2309.00754v1), [pdf](http://arxiv.org/pdf/2309.00754v1.pdf), cication: [**1**](https://scholar.google.com/scholar?cites=6843908222161428317&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Michael Santacroce, Yadong Lu, Han Yu, Yuanzhi Li, Yelong Shen*
- **RLAIF: Scaling Reinforcement Learning from Human Feedback with AI
  Feedback**, `arXiv, 2309.00267`, [arxiv](http://arxiv.org/abs/2309.00267v1), [pdf](http://arxiv.org/pdf/2309.00267v1.pdf), cication: [**24**](https://scholar.google.com/scholar?cites=7995210232742152683&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune, Abhinav Rastogi* · ([mp.weixin.qq](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652372872&idx=1&sn=c1a543c792bf3dfc891729f04236c549))
- **Reinforced Self-Training (ReST) for Language Modeling**, `arXiv, 2308.08998`, [arxiv](http://arxiv.org/abs/2308.08998v2), [pdf](http://arxiv.org/pdf/2308.08998v2.pdf), cication: [**12**](https://scholar.google.com/scholar?cites=3263533902860525796&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu* · ([jiqizhixin](https://www.jiqizhixin.com/articles/2023-08-22-6))
- **DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like
  Models at All Scales**, `arXiv, 2308.01320`, [arxiv](http://arxiv.org/abs/2308.01320v1), [pdf](http://arxiv.org/pdf/2308.01320v1.pdf), cication: [**4**](https://scholar.google.com/scholar?cites=9524636698512222272&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Zhewei Yao, Reza Yazdani Aminabadi, Olatunji Ruwase, Samyam Rajbhandari, Xiaoxia Wu, Ammar Ahmad Awan, Jeff Rasley, Minjia Zhang, Conglong Li, Connor Holmes*
- **Open Problems and Fundamental Limitations of Reinforcement Learning from
  Human Feedback**, `arXiv, 2307.15217`, [arxiv](http://arxiv.org/abs/2307.15217v2), [pdf](http://arxiv.org/pdf/2307.15217v2.pdf), cication: [**36**](https://scholar.google.com/scholar?cites=2043453316558651204&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, Jérémy Scheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire* · ([jiqizhixin](https://www.jiqizhixin.com/articles/2023-08-01-9))
- [ICML '23 Tutorial on Reinforcement Learning from Human Feedback](https://docs.google.com/presentation/d/1b_ymNDU0WRQ1-rcQDK45_bH9F0giNyRmdi0iKso6G5E/edit#slide=id.g259dff58475_0_44)

	 · ([openlmlab.github](https://openlmlab.github.io/MOSS-RLHF/)) · ([mp.weixin.qq](https://mp.weixin.qq.com/s?__biz=MzAxMTk4NDkwNw==&mid=2247494225&idx=1&sn=7bae77595b4e2f43cc8a4b94ddb4646c))
- **Fine-Tuning Language Models with Advantage-Induced Policy Alignment**, `arXiv, 2306.02231`, [arxiv](http://arxiv.org/abs/2306.02231v3), [pdf](http://arxiv.org/pdf/2306.02231v3.pdf), cication: [**5**](https://scholar.google.com/scholar?cites=7726101249171983080&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Banghua Zhu, Hiteshi Sharma, Felipe Vieira Frujeri, Shi Dong, Chenguang Zhu, Michael I. Jordan, Jiantao Jiao*
- **System-Level Natural Language Feedback**, `arXiv, 2306.13588`, [arxiv](http://arxiv.org/abs/2306.13588v1), [pdf](http://arxiv.org/pdf/2306.13588v1.pdf), cication: [**1**](https://scholar.google.com/scholar?cites=5523428644029476523&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Weizhe Yuan, Kyunghyun Cho, Jason Weston*
- **Fine-Grained Human Feedback Gives Better Rewards for Language Model
  Training**, `arXiv, 2306.01693`, [arxiv](http://arxiv.org/abs/2306.01693v2), [pdf](http://arxiv.org/pdf/2306.01693v2.pdf), cication: [**7**](https://scholar.google.com/scholar?cites=9400790265193597011&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah A. Smith, Mari Ostendorf, Hannaneh Hajishirzi* · ([finegrainedrlhf.github](https://finegrainedrlhf.github.io/)) · ([qbitai](https://www.qbitai.com/2023/06/61691.html))
- **Direct Preference Optimization: Your Language Model is Secretly a Reward
  Model**, `arXiv, 2305.18290`, [arxiv](http://arxiv.org/abs/2305.18290v2), [pdf](http://arxiv.org/pdf/2305.18290v2.pdf), cication: [**-1**](None)

	 *Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, Chelsea Finn*
- **Let's Verify Step by Step**, `arXiv, 2305.20050`, [arxiv](http://arxiv.org/abs/2305.20050v1), [pdf](http://arxiv.org/pdf/2305.20050v1.pdf), cication: [**76**](https://scholar.google.com/scholar?cites=3594089577812846684&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, Karl Cobbe*
- **Training a Helpful and Harmless Assistant with Reinforcement Learning
  from Human Feedback**, `arXiv, 2204.05862`, [arxiv](http://arxiv.org/abs/2204.05862v1), [pdf](http://arxiv.org/pdf/2204.05862v1.pdf), cication: [**109**](https://scholar.google.com/scholar?cites=11199782510491151350&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan* · ([hh-rlhf](https://github.com/anthropics/hh-rlhf) - anthropics) ![Star](https://img.shields.io/github/stars/anthropics/hh-rlhf.svg?style=social&label=Star)
## Projects
- [**PairRM**](https://huggingface.co/llm-blender/PairRM) - llm-blender 🤗
- [**OpenRLHF**](https://github.com/OpenLLMAI/OpenRLHF) - OpenLLMAI ![Star](https://img.shields.io/github/stars/OpenLLMAI/OpenRLHF.svg?style=social&label=Star)

	 *A Ray-based High-performance RLHF framework (for 7B on RTX4090 and 34B on A100)*
- [**direct-preference-optimization**](https://github.com/eric-mitchell/direct-preference-optimization) - eric-mitchell ![Star](https://img.shields.io/github/stars/eric-mitchell/direct-preference-optimization.svg?style=social&label=Star)

	 *Reference implementation for DPO (Direct Preference Optimization)*
- [**trl**](https://github.com/huggingface/trl) - huggingface ![Star](https://img.shields.io/github/stars/huggingface/trl.svg?style=social&label=Star)

	 *Train transformer language models with reinforcement learning.*
- [**tril**](https://github.com/cornell-rl/tril) - cornell-rl ![Star](https://img.shields.io/github/stars/cornell-rl/tril.svg?style=social&label=Star)

## Other
- [Constitutional AI with Open LLMs](https://huggingface.co/blog/constitutional_ai)
- [Preference Tuning LLMs with Direct Preference Optimization Methods](https://huggingface.co/blog/pref-tuning)
- [Reinforcement Learning from Human Feedback - DeepLearning.AI](https://www.deeplearning.ai/short-courses/reinforcement-learning-from-human-feedback/)
- [**Reinforcement Learning for Language Models**](https://gist.github.com/yoavg/6bff0fecd65950898eba1bb321cfbd81) - yoavg ![Star](https://img.shields.io/github/stars/yoavg/6bff0fecd65950898eba1bb321cfbd81.svg?style=social&label=Star)
- [The Q\* hypothesis: Tree-of-thoughts reasoning, process reward models, and supercharging synthetic data](https://www.interconnects.ai/p/q-star)
- [reverse engineer the Q* fantasy](https://twitter.com/DrJimFan/status/1728100123862004105)
- [Fine-tune Llama 2 with DPO](https://huggingface.co/blog/dpo-trl)
- [LLM Training: RLHF and Its Alternatives](https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives)

	 · ([mp.weixin.qq](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652379509&idx=5&sn=910a60449c276ac9d0f29b8f71a60327))
- [ICML '23 Tutorial on Reinforcement Learning from Human Feedback](https://docs.google.com/presentation/d/1b_ymNDU0WRQ1-rcQDK45_bH9F0giNyRmdi0iKso6G5E/edit#slide=id.g259dff58475_0_44)

- [RLHF中Reward model的trick](https://mp.weixin.qq.com/s?__biz=MzAxMTk4NDkwNw==&mid=2247495465&idx=1&sn=b0690e841f36d154024c0f6a9e922daf)
- [怎样让 PPO 训练更稳定？早期人类征服 RLHF 的驯化经验](https://mp.weixin.qq.com/s?__biz=MzAxMTk4NDkwNw==&mid=2247495219&idx=2&sn=bcd6cf222963493fed29fd44838b5aa2)
- [RLHF实践 - 知乎](https://zhuanlan.zhihu.com/p/635569455)

	 · ([mp.weixin.qq](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652371219&idx=5&sn=63c3294a9c08dde7e63444c6110e9c34))
- [LLM成功不可或缺的基石：RLHF及其替代技术 | 机器之心](https://www.jiqizhixin.com/articles/2023-10-07-7)


## Extra reference

- [**awesome-RLHF**](https://github.com/opendilab/awesome-RLHF) - opendilab ![Star](https://img.shields.io/github/stars/opendilab/awesome-RLHF.svg?style=social&label=Star)

	 *A curated list of reinforcement learning with human feedback resources (continually updated)*