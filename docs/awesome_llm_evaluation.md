# Awesome llm evaluation

- [Awesome llm evaluation](#awesome-llm-evaluation)
	- [Survey](#survey)
	- [Papers](#papers)
	- [Projects](#projects)
	- [Other](#other)
	- [Extra reference](#extra-reference)


## Survey
- **Evaluating Large Language Models: A Comprehensive Survey**, `arXiv, 2310.19736`, [arxiv](http://arxiv.org/abs/2310.19736v2), [pdf](http://arxiv.org/pdf/2310.19736v2.pdf), cication: [**-1**](None)

	 *Zishan Guo, Renren Jin, Chuang Liu, Yufei Huang, Dan Shi, Supryadi, Linhao Yu, Yan Liu, Jiaxuan Li, Bojian Xiong* ¬∑ ([Awesome-LLMs-Evaluation-Papers](https://github.com/tjunlp-lab/Awesome-LLMs-Evaluation-Papers) - tjunlp-lab) ![Star](https://img.shields.io/github/stars/tjunlp-lab/Awesome-LLMs-Evaluation-Papers.svg?style=social&label=Star)
- **Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language
  Models' Alignment**, `arXiv, 2308.05374`, [arxiv](http://arxiv.org/abs/2308.05374v1), [pdf](http://arxiv.org/pdf/2308.05374v1.pdf), cication: [**12**](https://scholar.google.com/scholar?cites=9999906366581522663&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Yang Liu, Yuanshun Yao, Jean-Francois Ton, Xiaoying Zhang, Ruocheng Guo, Hao Cheng, Yegor Klochkov, Muhammad Faaiz Taufiq, Hang Li* ¬∑ [[jiqizhixin](https://www.jiqizhixin.com/articles/2023-10-03-7)]
- **A Survey on Evaluation of Large Language Models**, `arXiv, 2307.03109`, [arxiv](http://arxiv.org/abs/2307.03109v8), [pdf](http://arxiv.org/pdf/2307.03109v8.pdf), cication: [**-1**](None)

	 *Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang*

## Papers
- **MEGAVERSE: Benchmarking Large Language Models Across Languages,
  Modalities, Models and Tasks**, `arXiv, 2311.07463`, [arxiv](http://arxiv.org/abs/2311.07463v1), [pdf](http://arxiv.org/pdf/2311.07463v1.pdf), cication: [**-1**](None)

	 *Sanchit Ahuja, Divyanshu Aggarwal, Varun Gumma, Ishaan Watts, Ashutosh Sathe, Millicent Ochieng, Rishav Hada, Prachi Jain, Maxamed Axmed, Kalika Bali*
- **Generative Judge for Evaluating Alignment**, `arXiv, 2310.05470`, [arxiv](http://arxiv.org/abs/2310.05470v1), [pdf](http://arxiv.org/pdf/2310.05470v1.pdf), cication: [**-1**](None)

	 *Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, Hai Zhao, Pengfei Liu* ¬∑ ([auto-j](https://github.com/GAIR-NLP/auto-j) - GAIR-NLP) ![Star](https://img.shields.io/github/stars/GAIR-NLP/auto-j.svg?style=social&label=Star) ¬∑ ([gair-nlp.github](https://gair-nlp.github.io/auto-j/))
- **Can LLMs Follow Simple Rules?**, `arXiv, 2311.04235`, [arxiv](http://arxiv.org/abs/2311.04235v1), [pdf](http://arxiv.org/pdf/2311.04235v1.pdf), cication: [**-1**](None)

	 *Norman Mu, Sarah Chen, Zifan Wang, Sizhe Chen, David Karamardian, Lulwa Aljeraisy, Dan Hendrycks, David Wagner*
- **Don't Make Your LLM an Evaluation Benchmark Cheater**, `arXiv, 2311.01964`, [arxiv](http://arxiv.org/abs/2311.01964v1), [pdf](http://arxiv.org/pdf/2311.01964v1.pdf), cication: [**-1**](None)

	 *Kun Zhou, Yutao Zhu, Zhipeng Chen, Wentong Chen, Wayne Xin Zhao, Xu Chen, Yankai Lin, Ji-Rong Wen, Jiawei Han*
- **PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task
  Completion**, `arXiv, 2311.01767`, [arxiv](http://arxiv.org/abs/2311.01767v2), [pdf](http://arxiv.org/pdf/2311.01767v2.pdf), cication: [**-1**](None)

	 *Yiduo Guo, Zekai Zhang, Yaobo Liang, Dongyan Zhao, Nan Duan*
- **GPT-Fathom: Benchmarking Large Language Models to Decipher the
  Evolutionary Path towards GPT-4 and Beyond**, `arXiv, 2309.16583`, [arxiv](http://arxiv.org/abs/2309.16583v3), [pdf](http://arxiv.org/pdf/2309.16583v3.pdf), cication: [**1**](https://scholar.google.com/scholar?cites=11825396027393223998&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Shen Zheng, Yuyu Zhang, Yijie Zhu, Chenguang Xi, Pengyang Gao, Xun Zhou, Kevin Chen-Chuan Chang* ¬∑ ([gpt-fathom](https://github.com/gpt-fathom/gpt-fathom) - gpt-fathom) ![Star](https://img.shields.io/github/stars/gpt-fathom/gpt-fathom.svg?style=social&label=Star) ¬∑ [[qbitai](https://www.qbitai.com/2023/11/95409.html)]
- **Leveraging Word Guessing Games to Assess the Intelligence of Large
  Language Models**, `arXiv, 2310.20499`, [arxiv](http://arxiv.org/abs/2310.20499v2), [pdf](http://arxiv.org/pdf/2310.20499v2.pdf), cication: [**-1**](None)

	 *Tian Liang, Zhiwei He, Jen-tse Huang, Wenxuan Wang, Wenxiang Jiao, Rui Wang, Yujiu Yang, Zhaopeng Tu, Shuming Shi, Xing Wang*
- **Does GPT-4 Pass the Turing Test?**, `arXiv, 2310.20216`, [arxiv](http://arxiv.org/abs/2310.20216v1), [pdf](http://arxiv.org/pdf/2310.20216v1.pdf), cication: [**-1**](None)

	 *Cameron Jones, Benjamin Bergen*
- **ALCUNA: Large Language Models Meet New Knowledge**, `arXiv, 2310.14820`, [arxiv](http://arxiv.org/abs/2310.14820v1), [pdf](http://arxiv.org/pdf/2310.14820v1.pdf), cication: [**-1**](None)

	 *Xunjian Yin, Baizhou Huang, Xiaojun Wan*
- **A Framework for Automated Measurement of Responsible AI Harms in
  Generative AI Applications**, `arXiv, 2310.17750`, [arxiv](http://arxiv.org/abs/2310.17750v1), [pdf](http://arxiv.org/pdf/2310.17750v1.pdf), cication: [**-1**](None)

	 *Ahmed Magooda, Alec Helyar, Kyle Jackson, David Sullivan, Chad Atalla, Emily Sheng, Dan Vann, Richard Edgar, Hamid Palangi, Roman Lutz*
- **Exploring OCR Capabilities of GPT-4V(ision) : A Quantitative and
  In-depth Evaluation**, `arXiv, 2310.16809`, [arxiv](http://arxiv.org/abs/2310.16809v2), [pdf](http://arxiv.org/pdf/2310.16809v2.pdf), cication: [**2**](https://scholar.google.com/scholar?cites=7703323566044468754&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Yongxin Shi, Dezhi Peng, Wenhui Liao, Zening Lin, Xinhong Chen, Chongyu Liu, Yuyi Zhang, Lianwen Jin* ¬∑ ([gpt-4v_ocr](https://github.com/scut-dlvclab/gpt-4v_ocr) - scut-dlvclab) ![Star](https://img.shields.io/github/stars/scut-dlvclab/gpt-4v_ocr.svg?style=social&label=Star)
- **JudgeLM: Fine-tuned Large Language Models are Scalable Judges**, `arXiv, 2310.17631`, [arxiv](http://arxiv.org/abs/2310.17631v1), [pdf](http://arxiv.org/pdf/2310.17631v1.pdf), cication: [**-1**](None)

	 *Lianghui Zhu, Xinggang Wang, Xinlong Wang*
- **An Early Evaluation of GPT-4V(ision)**, `arXiv, 2310.16534`, [arxiv](http://arxiv.org/abs/2310.16534v1), [pdf](http://arxiv.org/pdf/2310.16534v1.pdf), cication: [**1**](https://scholar.google.com/scholar?cites=11688624601117600790&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Yang Wu, Shilong Wang, Hao Yang, Tian Zheng, Hongbo Zhang, Yanyan Zhao, Bing Qin*
- **Generative Judge for Evaluating Alignment**, `arXiv, 2310.05470`, [arxiv](http://arxiv.org/abs/2310.05470v1), [pdf](http://arxiv.org/pdf/2310.05470v1.pdf), cication: [**-1**](None)

	 *Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, Hai Zhao, Pengfei Liu*
- **The Foundation Model Transparency Index**, `arXiv, 2310.12941`, [arxiv](http://arxiv.org/abs/2310.12941v1), [pdf](http://arxiv.org/pdf/2310.12941v1.pdf), cication: [**3**](https://scholar.google.com/scholar?cites=6897485371215453301&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Rishi Bommasani, Kevin Klyman, Shayne Longpre, Sayash Kapoor, Nestor Maslej, Betty Xiong, Daniel Zhang, Percy Liang* ¬∑ [[qbitai](https://www.qbitai.com/2023/10/91045.html)] ¬∑ [[crfm.stanford](https://crfm.stanford.edu/fmti/fmti.pdf)]
- **ReForm-Eval: Evaluating Large Vision Language Models via Unified
  Re-Formulation of Task-Oriented Benchmarks**, `arXiv, 2310.02569`, [arxiv](http://arxiv.org/abs/2310.02569v2), [pdf](http://arxiv.org/pdf/2310.02569v2.pdf), cication: [**-1**](None)

	 *Zejun Li, Ye Wang, Mengfei Du, Qingwen Liu, Binhao Wu, Jiwen Zhang, Chengxing Zhou, Zhihao Fan, Jie Fu, Jingjing Chen* ¬∑ [[jiqizhixin](https://www.jiqizhixin.com/articles/2023-10-20)]
- **CLEVA: Chinese Language Models EVAluation Platform**, `arXiv, 2308.04813`, [arxiv](http://arxiv.org/abs/2308.04813v2), [pdf](http://arxiv.org/pdf/2308.04813v2.pdf), cication: [**-1**](None)

	 *Yanyang Li, Jianqiao Zhao, Duo Zheng, Zi-Yuan Hu, Zhi Chen, Xiaohui Su, Yongfeng Huang, Shijia Huang, Dahua Lin, Michael R. Lyu* ¬∑ [[qbitai](https://www.qbitai.com/2023/10/90385.html)]
- **Prometheus: Inducing Fine-grained Evaluation Capability in Language
  Models**, `arXiv, 2310.08491`, [arxiv](http://arxiv.org/abs/2310.08491v1), [pdf](http://arxiv.org/pdf/2310.08491v1.pdf), cication: [**-1**](None)

	 *Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne*
- **A Closer Look into Automatic Evaluation Using Large Language Models**, `arXiv, 2310.05657`, [arxiv](http://arxiv.org/abs/2310.05657v1), [pdf](http://arxiv.org/pdf/2310.05657v1.pdf), cication: [**-1**](None)

	 *Cheng-Han Chiang, Hung-yi Lee* ¬∑ ([mp.weixin.qq](https://mp.weixin.qq.com/s?__biz=MzIyMzk1MDE3Nw==&mid=2247613756&idx=5&sn=e3241595841293d9e0b73c6b2e70a21d)) ¬∑ ([A-Closer-Look-To-LLM-Evaluation](https://github.com/d223302/A-Closer-Look-To-LLM-Evaluation/) - d223302) ![Star](https://img.shields.io/github/stars/d223302/A-Closer-Look-To-LLM-Evaluation.svg?style=social&label=Star)
- **Probing the Moral Development of Large Language Models through Defining
  Issues Test**, `arXiv, 2309.13356`, [arxiv](http://arxiv.org/abs/2309.13356v2), [pdf](http://arxiv.org/pdf/2309.13356v2.pdf), cication: [**-1**](None)

	 *Kumar Tanmay, Aditi Khandelwal, Utkarsh Agarwal, Monojit Choudhury* ¬∑ [[mp.weixin.qq](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652388927&idx=4&sn=5c9897283252d8f266144fbcaa9bc43e)]
- **GPT-Fathom: Benchmarking Large Language Models to Decipher the
  Evolutionary Path towards GPT-4 and Beyond**, `arXiv, 2309.16583`, [arxiv](http://arxiv.org/abs/2309.16583v3), [pdf](http://arxiv.org/pdf/2309.16583v3.pdf), cication: [**1**](https://scholar.google.com/scholar?cites=11825396027393223998&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Shen Zheng, Yuyu Zhang, Yijie Zhu, Chenguang Xi, Pengyang Gao, Xun Zhou, Kevin Chen-Chuan Chang*
- **Calibrating LLM-Based Evaluator**, `arXiv, 2309.13308`, [arxiv](http://arxiv.org/abs/2309.13308v1), [pdf](http://arxiv.org/pdf/2309.13308v1.pdf), cication: [**-1**](None)

	 *Yuxuan Liu, Tianchi Yang, Shaohan Huang, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, Qi Zhang*
- **Probing the Moral Development of Large Language Models through Defining
  Issues Test**, `arXiv, 2309.13356`, [arxiv](http://arxiv.org/abs/2309.13356v2), [pdf](http://arxiv.org/pdf/2309.13356v2.pdf), cication: [**-1**](None)

	 *Kumar Tanmay, Aditi Khandelwal, Utkarsh Agarwal, Monojit Choudhury*
- **Struc-Bench: Are Large Language Models Really Good at Generating Complex
  Structured Data?**, `arXiv, 2309.08963`, [arxiv](http://arxiv.org/abs/2309.08963v2), [pdf](http://arxiv.org/pdf/2309.08963v2.pdf), cication: [**1**](https://scholar.google.com/scholar?cites=2082058242118583458&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Xiangru Tang, Yiming Zong, Jason Phang, Yilun Zhao, Wangchunshu Zhou, Arman Cohan, Mark Gerstein*
- **Investigating Answerability of LLMs for Long-Form Question Answering**, `arXiv, 2309.08210`, [arxiv](http://arxiv.org/abs/2309.08210v1), [pdf](http://arxiv.org/pdf/2309.08210v1.pdf), cication: [**-1**](None)

	 *Meghana Moorthy Bhat, Rui Meng, Ye Liu, Yingbo Zhou, Semih Yavuz*
- **Are Large Language Model-based Evaluators the Solution to Scaling Up
  Multilingual Evaluation?**, `arXiv, 2309.07462`, [arxiv](http://arxiv.org/abs/2309.07462v1), [pdf](http://arxiv.org/pdf/2309.07462v1.pdf), cication: [**4**](https://scholar.google.com/scholar?cites=3253818796886735588&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Rishav Hada, Varun Gumma, Adrian de Wynter, Harshita Diddee, Mohamed Ahmed, Monojit Choudhury, Kalika Bali, Sunayana Sitaram*
- **Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs**, `arXiv, 2308.13387`, [arxiv](http://arxiv.org/abs/2308.13387v2), [pdf](http://arxiv.org/pdf/2308.13387v2.pdf), cication: [**3**](https://scholar.google.com/scholar?cites=2648067072454919746&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Yuxia Wang, Haonan Li, Xudong Han, Preslav Nakov, Timothy Baldwin* ¬∑ [[jiqizhixin](https://www.jiqizhixin.com/articles/2023-09-05-7)]
- **The ConceptARC Benchmark: Evaluating Understanding and Generalization in
  the ARC Domain**, `arXiv, 2305.07141`, [arxiv](http://arxiv.org/abs/2305.07141v1), [pdf](http://arxiv.org/pdf/2305.07141v1.pdf), cication: [**10**](https://scholar.google.com/scholar?cites=8946879368609581934&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Arseny Moskvichev, Victor Vikram Odouard, Melanie Mitchell* ¬∑ [[mp.weixin.qq](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652371669&idx=4&sn=cb7350a12a2ff8a4c238d7220b2273a5)]
- **MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities**, `arXiv, 2308.02490`, [arxiv](http://arxiv.org/abs/2308.02490v3), [pdf](http://arxiv.org/pdf/2308.02490v3.pdf), cication: [**10**](https://scholar.google.com/scholar?cites=5160742062303244685&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, Lijuan Wang*
- **ARB: Advanced Reasoning Benchmark for Large Language Models**, `arXiv, 2307.13692`, [arxiv](http://arxiv.org/abs/2307.13692v2), [pdf](http://arxiv.org/pdf/2307.13692v2.pdf), cication: [**6**](https://scholar.google.com/scholar?cites=5512964966028642645&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Tomohiro Sawada, Daniel Paleka, Alexander Havrilla, Pranav Tadepalli, Paula Vidas, Alexander Kranias, John J. Nay, Kshitij Gupta, Aran Komatsuzaki*
- **L-Eval: Instituting Standardized Evaluation for Long Context Language
  Models**, `arXiv, 2307.11088`, [arxiv](http://arxiv.org/abs/2307.11088v3), [pdf](http://arxiv.org/pdf/2307.11088v3.pdf), cication: [**-1**](None)

	 *Chenxin An, Shansan Gong, Ming Zhong, Xingjian Zhao, Mukai Li, Jun Zhang, Lingpeng Kong, Xipeng Qiu* ¬∑ ([leval](https://github.com/openlmlab/leval) - openlmlab) ![Star](https://img.shields.io/github/stars/openlmlab/leval.svg?style=social&label=Star)
- **Instruction-following Evaluation through Verbalizer Manipulation**, `arXiv, 2307.10558`, [arxiv](http://arxiv.org/abs/2307.10558v1), [pdf](http://arxiv.org/pdf/2307.10558v1.pdf), cication: [**4**](https://scholar.google.com/scholar?cites=3294772868387818937&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Shiyang Li, Jun Yan, Hai Wang, Zheng Tang, Xiang Ren, Vijay Srinivasan, Hongxia Jin*
- **FLASK: Fine-grained Language Model Evaluation based on Alignment Skill
  Sets**, `arXiv, 2307.10928`, [arxiv](http://arxiv.org/abs/2307.10928v2), [pdf](http://arxiv.org/pdf/2307.10928v2.pdf), cication: [**5**](https://scholar.google.com/scholar?cites=11317511132993952746&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Seonghyeon Ye, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, Seungone Kim, Yongrae Jo, James Thorne, Juho Kim, Minjoon Seo*
- **How is ChatGPT's behavior changing over time?**, `arXiv, 2307.09009`, [arxiv](http://arxiv.org/abs/2307.09009v3), [pdf](http://arxiv.org/pdf/2307.09009v3.pdf), cication: [**64**](https://scholar.google.com/scholar?cites=7228649203975536747&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Lingjiao Chen, Matei Zaharia, James Zou*
- **Generating Benchmarks for Factuality Evaluation of Language Models**, `arXiv, 2307.06908`, [arxiv](http://arxiv.org/abs/2307.06908v1), [pdf](http://arxiv.org/pdf/2307.06908v1.pdf), cication: [**6**](https://scholar.google.com/scholar?cites=5608570652501844927&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Dor Muhlgay, Ori Ram, Inbal Magar, Yoav Levine, Nir Ratner, Yonatan Belinkov, Omri Abend, Kevin Leyton-Brown, Amnon Shashua, Yoav Shoham*

	 ¬∑ ([factor](https://github.com/AI21Labs/factor) - AI21Labs) ![Star](https://img.shields.io/github/stars/AI21Labs/factor.svg?style=social&label=Star)

- **PromptBench: Towards Evaluating the Robustness of Large Language Models
  on Adversarial Prompts**, `arXiv, 2306.04528`, [arxiv](http://arxiv.org/abs/2306.04528v4), [pdf](http://arxiv.org/pdf/2306.04528v4.pdf), cication: [**32**](https://scholar.google.com/scholar?cites=6727691362756502405&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong Wang, Linyi Yang, Wei Ye, Yue Zhang, Neil Zhenqiang Gong* ¬∑ ([promptbench](https://github.com/microsoft/promptbench) - microsoft) ![Star](https://img.shields.io/github/stars/microsoft/promptbench.svg?style=social&label=Star)
- **Empowering Cross-lingual Behavioral Testing of NLP Models with
  Typological Features**, `arXiv, 2307.05454`, [arxiv](http://arxiv.org/abs/2307.05454v1), [pdf](http://arxiv.org/pdf/2307.05454v1.pdf), cication: [**-1**](None)

	 *Ester Hlavnova, Sebastian Ruder*
- **GLUE-X: Evaluating Natural Language Understanding Models from an
  Out-of-distribution Generalization Perspective**, `arXiv, 2211.08073`, [arxiv](http://arxiv.org/abs/2211.08073v4), [pdf](http://arxiv.org/pdf/2211.08073v4.pdf), cication: [**21**](https://scholar.google.com/scholar?cites=2869414843777058922&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Linyi Yang, Shuibai Zhang, Libo Qin, Yafu Li, Yidong Wang, Hanmeng Liu, Jindong Wang, Xing Xie, Yue Zhang*
- **M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining
  Large Language Models**, `arXiv, 2306.05179`, [arxiv](http://arxiv.org/abs/2306.05179v1), [pdf](http://arxiv.org/pdf/2306.05179v1.pdf), cication: [**7**](https://scholar.google.com/scholar?cites=5326118821139338567&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Wenxuan Zhang, Sharifah Mahani Aljunied, Chang Gao, Yew Ken Chia, Lidong Bing* ¬∑ [[jiqizhixin](https://www.jiqizhixin.com/articles/2023-07-03-7)] ¬∑ ([M3Exam](https://github.com/DAMO-NLP-SG/M3Exam) - DAMO-NLP-SG) ![Star](https://img.shields.io/github/stars/DAMO-NLP-SG/M3Exam.svg?style=social&label=Star)
- **MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language
  Models**, `arXiv, 2306.13394`, [arxiv](http://arxiv.org/abs/2306.13394v2), [pdf](http://arxiv.org/pdf/2306.13394v2.pdf), cication: [**32**](https://scholar.google.com/scholar?cites=8526876186588328715&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng* ¬∑ ([Awesome-Multimodal-Large-Language-Models](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models) - BradyFU) ![Star](https://img.shields.io/github/stars/BradyFU/Awesome-Multimodal-Large-Language-Models.svg?style=social&label=Star) ¬∑ [[mp.weixin.qq](https://mp.weixin.qq.com/s?__biz=MzAxMTk4NDkwNw==&mid=2247494157&idx=2&sn=350b94abcb3d1b2956ef015c51496b6f)]
- **Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena**, `arXiv, 2306.05685`, [arxiv](http://arxiv.org/abs/2306.05685v3), [pdf](http://arxiv.org/pdf/2306.05685v3.pdf), cication: [**136**](https://scholar.google.com/scholar?cites=2115026370478138399&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing* ¬∑ [[twitter](https://twitter.com/lmsysorg/status/1675612625273761793)] ¬∑ [[lmsys](https://lmsys.org/blog/2023-06-22-leaderboard/)]
- **Generative AI for Programming Education: Benchmarking ChatGPT, GPT-4,
  and Human Tutors**, `international journal of management, 2023`, [arxiv](http://arxiv.org/abs/2306.17156v3), [pdf](http://arxiv.org/pdf/2306.17156v3.pdf), cication: [**8**](https://scholar.google.com/scholar?cites=3129038780473031168&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Tung Phung, Victor-Alexandru PƒÉdurean, Jos√© Cambronero, Sumit Gulwani, Tobias Kohn, Rupak Majumdar, Adish Singla, Gustavo Soares*
- **Benchmarking Large Language Model Capabilities for Conditional
  Generation**, `arXiv, 2306.16793`, [arxiv](http://arxiv.org/abs/2306.16793v1), [pdf](http://arxiv.org/pdf/2306.16793v1.pdf), cication: [**2**](https://scholar.google.com/scholar?cites=10717420948164970757&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Joshua Maynez, Priyanka Agrawal, Sebastian Gehrmann*
- **CMMLU: Measuring massive multitask language understanding in Chinese**, `arXiv, 2306.09212`, [arxiv](http://arxiv.org/abs/2306.09212v1), [pdf](http://arxiv.org/pdf/2306.09212v1.pdf), cication: [**14**](https://scholar.google.com/scholar?cites=7424799427308772526&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, Timothy Baldwin* ¬∑ [[jiqizhixin](https://www.jiqizhixin.com/articles/2023-06-29-3)] ¬∑ ([CMMLU](https://github.com/haonan-li/CMMLU) - haonan-li) ![Star](https://img.shields.io/github/stars/haonan-li/CMMLU.svg?style=social&label=Star)

- **Bring Your Own Data! Self-Supervised Evaluation for Large Language
  Models**, `arXiv, 2306.13651`, [arxiv](http://arxiv.org/abs/2306.13651v2), [pdf](http://arxiv.org/pdf/2306.13651v2.pdf), cication: [**7**](https://scholar.google.com/scholar?cites=1188786261128674806&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Neel Jain, Khalid Saifullah, Yuxin Wen, John Kirchenbauer, Manli Shu, Aniruddha Saha, Micah Goldblum, Jonas Geiping, Tom Goldstein* ¬∑ ([byod](https://github.com/neelsjain/byod) - neelsjain) ![Star](https://img.shields.io/github/stars/neelsjain/byod.svg?style=social&label=Star)
- **INSTRUCTEVAL: Towards Holistic Evaluation of Instruction-Tuned Large
  Language Models**, `arXiv, 2306.04757`, [arxiv](http://arxiv.org/abs/2306.04757v3), [pdf](http://arxiv.org/pdf/2306.04757v3.pdf), cication: [**19**](https://scholar.google.com/scholar?cites=3329388628137464648&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Yew Ken Chia, Pengfei Hong, Lidong Bing, Soujanya Poria* ¬∑ [[jiqizhixin](https://www.jiqizhixin.com/articles/2023-06-13-3)]
- **Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena**, `arXiv, 2306.05685`, [arxiv](http://arxiv.org/abs/2306.05685v3), [pdf](http://arxiv.org/pdf/2306.05685v3.pdf), cication: [**136**](https://scholar.google.com/scholar?cites=2115026370478138399&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing*

## Projects
- [LLM Leaderboard best models ‚ù§Ô∏è‚Äçüî• - a open-llm-leaderboard Collection](https://huggingface.co/collections/open-llm-leaderboard/llm-leaderboard-best-models-652d6c7965a4619fb5c27a03)
- [**opencompass**](https://github.com/InternLM/opencompass) - InternLM ![Star](https://img.shields.io/github/stars/InternLM/opencompass.svg?style=social&label=Star)

	 *OpenCompass is an LLM evaluation platform, supporting a wide range of models (LLaMA, ChatGLM2, ChatGPT, Claude, etc) over 50+ datasets.* ¬∑ [[opencompass.org](https://opencompass.org.cn/MMBench)]
- [**FlagAI**](https://github.com/FlagAI-Open/FlagAI/tree/master/examples/Aquila) - FlagAI-Open ![Star](https://img.shields.io/github/stars/FlagAI-Open/FlagAI.svg?style=social&label=Star)

	 ¬∑ [[qbitai](https://www.qbitai.com/2023/06/64469.html)]
- [**toolqa**](https://github.com/night-chen/toolqa) - night-chen ![Star](https://img.shields.io/github/stars/night-chen/toolqa.svg?style=social&label=Star)

	 *ToolQA, a new dataset to evaluate the capabilities of LLMs in answering challenging questions with external tools. It offers two levels (easy/hard) across eight real-life scenarios.*
- [**glue-x**](https://github.com/yanglinyi/glue-x) - yanglinyi ![Star](https://img.shields.io/github/stars/yanglinyi/glue-x.svg?style=social&label=Star)

	 *We leverage 14 datasets as OOD test data and conduct evaluations on 8 NLU tasks over 21 popularly used models.*
- [**alpaca_farm**](https://github.com/tatsu-lab/alpaca_farm) - tatsu-lab ![Star](https://img.shields.io/github/stars/tatsu-lab/alpaca_farm.svg?style=social&label=Star)

	 *A Simulation Framework for RLHF and alternatives.* ¬∑ [[jiqizhixin](https://www.jiqizhixin.com/articles/2023-05-25-3)] ¬∑ [[mp.weixin.qq](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652341456&idx=4&sn=6be28e23b9e7435b76fe73920203b44d)]
-  [Open LLM Leaderboard - a Hugging Face Space by HuggingFaceH4](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)

	 ¬∑ [[huggingface](https://huggingface.co/blog/evaluating-mmlu-leaderboard)]

## Other
- [Comparing the Performance of LLMs: A Deep Dive into Roberta, Llama 2, and Mistral for Disaster Tweets Analysis with Lora](https://huggingface.co/blog/Lora-for-sequence-classification-with-Roberta-Llama-Mistral)
- [Anthropic \\ Challenges in evaluating AI systems](https://www.anthropic.com/index/evaluating-ai-systems)

	 ¬∑ [[jiqizhixin](https://www.jiqizhixin.com/articles/2023-11-06-6)]
- [ËøôÈáåÁöÑÊµãËØÑÔºå‰∏ªÊâì‰∏Ä‰∏™Âø´Áã†ÂáÜ](https://mp.weixin.qq.com/s?__biz=MzAxMTk4NDkwNw==&mid=2247494977&idx=1&sn=e2f84e5dea26f3c9065b5b5298d39fca)
- [Â§ßÊ®°ÂûãÁü•ËØÜ&Êé®ÁêÜËØÑ‰º∞Âü∫ÂáÜ](https://mp.weixin.qq.com/s?__biz=MzAxMTk4NDkwNw==&mid=2247493959&idx=1&sn=6df3d2398a59d1bb92ddde1ed7a62040)
- [SuperCLUE‰∏≠ÊñáÂ§ßÊ®°ÂûãËØÑÊµãÔºöÂïÜÊ±§ÂïÜÈáèÊèΩÊÄªÊ¶ú„ÄÅAIÊô∫ËÉΩ‰Ωì‰∏§È°πÁ¨¨‰∏Ä | ÈáèÂ≠ê‰Ωç](https://www.qbitai.com/2023/10/89814.html)
- [How Long Can Open-Source LLMs Truly Promise on Context Length? | LMSYS Org](https://lmsys.org/blog/2023-06-29-longchat/)

	 ¬∑ [[mp.weixin.qq](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652347271&idx=3&sn=48fe098e6789f50e25b26cd757898935)]
- [Chatbot Arena Leaderboard Week 8: Introducing MT-Bench and Vicuna-33B | LMSYS Org](https://lmsys.org/blog/2023-06-22-leaderboard/)

	 ¬∑ [[mp.weixin.qq](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652343523&idx=1&sn=c87f0d7de8ffedeb2f04c7df67b5245b)]

## Extra reference
- [**awesome-llms-evaluation-papers**](https://github.com/tjunlp-lab/awesome-llms-evaluation-papers) - tjunlp-lab ![Star](https://img.shields.io/github/stars/tjunlp-lab/awesome-llms-evaluation-papers.svg?style=social&label=Star)

	 *The papers are organized according to our survey: Evaluating Large Language Models: A Comprehensive Survey.* ¬∑ [[jiqizhixin](https://www.jiqizhixin.com/articles/2023-11-06)]