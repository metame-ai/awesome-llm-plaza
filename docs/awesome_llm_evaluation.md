# Awesome llm evaluation

- [Awesome llm evaluation](#awesome-llm-evaluation)
	- [Survey](#survey)
	- [Papers](#papers)
	- [Projects](#projects)
	- [Other](#other)
	- [Extra reference](#extra-reference)


## Survey
- **Leveraging Large Language Models for NLG Evaluation: A Survey**, `arXiv, 2401.07103`, [arxiv](http://arxiv.org/abs/2401.07103v1), [pdf](http://arxiv.org/pdf/2401.07103v1.pdf), cication: [**-1**](None)

	 *Zhen Li, Xiaohan Xu, Tao Shen, Can Xu, Jia-Chen Gu, Chongyang Tao*
- **ChatGPT's One-year Anniversary: Are Open-Source Large Language Models
  Catching up?**, `arXiv, 2311.16989`, [arxiv](http://arxiv.org/abs/2311.16989v2), [pdf](http://arxiv.org/pdf/2311.16989v2.pdf), cication: [**-1**](None)

	 *Hailin Chen, Fangkai Jiao, Xingxuan Li, Chengwei Qin, Mathieu Ravaut, Ruochen Zhao, Caiming Xiong, Shafiq Joty*
- **Evaluating Large Language Models: A Comprehensive Survey**, `arXiv, 2310.19736`, [arxiv](http://arxiv.org/abs/2310.19736v2), [pdf](http://arxiv.org/pdf/2310.19736v2.pdf), cication: [**-1**](None)

	 *Zishan Guo, Renren Jin, Chuang Liu, Yufei Huang, Dan Shi, Supryadi, Linhao Yu, Yan Liu, Jiaxuan Li, Bojian Xiong* · ([Awesome-LLMs-Evaluation-Papers](https://github.com/tjunlp-lab/Awesome-LLMs-Evaluation-Papers) - tjunlp-lab) ![Star](https://img.shields.io/github/stars/tjunlp-lab/Awesome-LLMs-Evaluation-Papers.svg?style=social&label=Star)
- **Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language
  Models' Alignment**, `arXiv, 2308.05374`, [arxiv](http://arxiv.org/abs/2308.05374v1), [pdf](http://arxiv.org/pdf/2308.05374v1.pdf), cication: [**12**](https://scholar.google.com/scholar?cites=9999906366581522663&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Yang Liu, Yuanshun Yao, Jean-Francois Ton, Xiaoying Zhang, Ruocheng Guo, Hao Cheng, Yegor Klochkov, Muhammad Faaiz Taufiq, Hang Li* · [jiqizhixin](https://www.jiqizhixin.com/articles/2023-10-03-7)]
- **A Survey on Evaluation of Large Language Models**, `arXiv, 2307.03109`, [arxiv](http://arxiv.org/abs/2307.03109v8), [pdf](http://arxiv.org/pdf/2307.03109v8.pdf), cication: [**-1**](None)

	 *Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang*

## Papers
- **Replacing Judges with Juries: Evaluating LLM Generations with a Panel of
  Diverse Models**, `arXiv, 2404.18796`, [arxiv](http://arxiv.org/abs/2404.18796v2), [pdf](http://arxiv.org/pdf/2404.18796v2.pdf), cication: [**-1**](None)

	 *Pat Verga, Sebastian Hofstatter, Sophia Althammer, Yixuan Su, Aleksandra Piktus, Arkady Arkhangorodsky, Minjie Xu, Naomi White, Patrick Lewis*
- **A Careful Examination of Large Language Model Performance on Grade
  School Arithmetic**, `arXiv, 2405.00332`, [arxiv](http://arxiv.org/abs/2405.00332v3), [pdf](http://arxiv.org/pdf/2405.00332v3.pdf), cication: [**-1**](None)

	 *Hugh Zhang, Jeff Da, Dean Lee, Vaughn Robinson, Catherine Wu, Will Song, Tiffany Zhao, Pranav Raja, Dylan Slack, Qin Lyu*
- **How Good Are Low-bit Quantized LLaMA3 Models? An Empirical Study**, `arXiv, 2404.14047`, [arxiv](http://arxiv.org/abs/2404.14047v1), [pdf](http://arxiv.org/pdf/2404.14047v1.pdf), cication: [**-1**](None)

	 *Wei Huang, Xudong Ma, Haotong Qin, Xingyu Zheng, Chengtao Lv, Hong Chen, Jie Luo, Xiaojuan Qi, Xianglong Liu, Michele Magno* · ([LLaMA3-Quantization](https://github.com/Macaronlin/LLaMA3-Quantization) - Macaronlin) ![Star](https://img.shields.io/github/stars/Macaronlin/LLaMA3-Quantization.svg?style=social&label=Star)
- **Elephants Never Forget: Memorization and Learning of Tabular Data in
  Large Language Models**, `arXiv, 2404.06209`, [arxiv](http://arxiv.org/abs/2404.06209v1), [pdf](http://arxiv.org/pdf/2404.06209v1.pdf), cication: [**-1**](None)

	 *Sebastian Bordt, Harsha Nori, Vanessa Rodrigues, Besmira Nushi, Rich Caruana* · ([LLM-Tabular-Memorization-Checker](https://github.com/interpretml/LLM-Tabular-Memorization-Checker) - interpretml) ![Star](https://img.shields.io/github/stars/interpretml/LLM-Tabular-Memorization-Checker.svg?style=social&label=Star)
- **Autonomous Evaluation and Refinement of Digital Agents**, `arXiv, 2404.06474`, [arxiv](http://arxiv.org/abs/2404.06474v2), [pdf](http://arxiv.org/pdf/2404.06474v2.pdf), cication: [**-1**](None)

	 *Jiayi Pan, Yichi Zhang, Nicholas Tomlin, Yifei Zhou, Sergey Levine, Alane Suhr* · ([Agent-Eval-Refine](https://github.com/Berkeley-NLP/Agent-Eval-Refine) - Berkeley-NLP) ![Star](https://img.shields.io/github/stars/Berkeley-NLP/Agent-Eval-Refine.svg?style=social&label=Star)
- **Length-Controlled AlpacaEval: A Simple Way to Debias Automatic
  Evaluators**, `arXiv, 2404.04475`, [arxiv](http://arxiv.org/abs/2404.04475v1), [pdf](http://arxiv.org/pdf/2404.04475v1.pdf), cication: [**-1**](None)

	 *Yann Dubois, Balázs Galambosi, Percy Liang, Tatsunori B. Hashimoto*
- **FABLES: Evaluating faithfulness and content selection in book-length
  summarization**, `arXiv, 2404.01261`, [arxiv](http://arxiv.org/abs/2404.01261v1), [pdf](http://arxiv.org/pdf/2404.01261v1.pdf), cication: [**-1**](None)

	 *Yekyung Kim, Yapei Chang, Marzena Karpinska, Aparna Garimella, Varun Manjunatha, Kyle Lo, Tanya Goyal, Mohit Iyyer* · ([FABLES](https://github.com/mungg/FABLES) - mungg) ![Star](https://img.shields.io/github/stars/mungg/FABLES.svg?style=social&label=Star)
- **EQ-Bench: An Emotional Intelligence Benchmark for Large Language Models**, `arXiv, 2312.06281`, [arxiv](http://arxiv.org/abs/2312.06281v2), [pdf](http://arxiv.org/pdf/2312.06281v2.pdf), cication: [**-1**](None)

	 *Samuel J. Paech*

	 · ([EQ-Bench](https://github.com/EQ-bench/EQ-Bench) - EQ-bench) ![Star](https://img.shields.io/github/stars/EQ-bench/EQ-Bench.svg?style=social&label=Star) · ([eqbench](https://eqbench.com/creative_writing.html?utm_source=ainews&utm_medium=email&utm_campaign=ainews-cohere-command-r-anthropic-claude-tool-use))
- **The RealHumanEval: Evaluating Large Language Models' Abilities to
  Support Programmers**, `arXiv, 2404.02806`, [arxiv](http://arxiv.org/abs/2404.02806v1), [pdf](http://arxiv.org/pdf/2404.02806v1.pdf), cication: [**-1**](None)

	 *Hussein Mozannar, Valerie Chen, Mohammed Alsobay, Subhro Das, Sebastian Zhao, Dennis Wei, Manish Nagireddy, Prasanna Sattigeri, Ameet Talwalkar, David Sontag* · ([realhumaneval](https://github.com/clinicalml/realhumaneval) - clinicalml) ![Star](https://img.shields.io/github/stars/clinicalml/realhumaneval.svg?style=social&label=Star)
- **Evalverse: Unified and Accessible Library for Large Language Model
  Evaluation**, `arXiv, 2404.00943`, [arxiv](http://arxiv.org/abs/2404.00943v1), [pdf](http://arxiv.org/pdf/2404.00943v1.pdf), cication: [**-1**](None)

	 *Jihoo Kim, Wonho Song, Dahyun Kim, Yunsu Kim, Yungi Kim, Chanjun Park* · ([evalverse](https://github.com/upstageai/evalverse) - upstageai) ![Star](https://img.shields.io/github/stars/upstageai/evalverse.svg?style=social&label=Star)
- **Long-form factuality in large language models**, `arXiv, 2403.18802`, [arxiv](http://arxiv.org/abs/2403.18802v1), [pdf](http://arxiv.org/pdf/2403.18802v1.pdf), cication: [**-1**](None)

	 *Jerry Wei, Chengrun Yang, Xinying Song, Yifeng Lu, Nathan Hu, Dustin Tran, Daiyi Peng, Ruibo Liu, Da Huang, Cosmo Du* · ([long-form-factuality](https://github.com/google-deepmind/long-form-factuality) - google-deepmind) ![Star](https://img.shields.io/github/stars/google-deepmind/long-form-factuality.svg?style=social&label=Star)
	- `LongFact is a comprehensive prompt set for benchmarking the long-form factuality of LLMs across 38 topics.`
- **A comparison of Human, GPT-3.5, and GPT-4 Performance in a
  University-Level Coding Course**, `arXiv, 2403.16977`, [arxiv](http://arxiv.org/abs/2403.16977v1), [pdf](http://arxiv.org/pdf/2403.16977v1.pdf), cication: [**-1**](None)

	 *Will Yeadon, Alex Peach, Craig P. Testrow*
- **A Toolbox for Surfacing Health Equity Harms and Biases in Large Language
  Models**, `arXiv, 2403.12025`, [arxiv](http://arxiv.org/abs/2403.12025v1), [pdf](http://arxiv.org/pdf/2403.12025v1.pdf), cication: [**-1**](None)

	 *Stephen R. Pfohl, Heather Cole-Lewis, Rory Sayres, Darlene Neal, Mercy Asiedu, Awa Dieng, Nenad Tomasev, Qazi Mamunur Rashid, Shekoofeh Azizi, Negar Rostamzadeh*
- **Large language models surpass human experts in predicting neuroscience
  results**, `arXiv, 2403.03230`, [arxiv](http://arxiv.org/abs/2403.03230v2), [pdf](http://arxiv.org/pdf/2403.03230v2.pdf), cication: [**-1**](None)

	 *Xiaoliang Luo, Akilles Rechardt, Guangzhi Sun, Kevin K. Nejad, Felipe Yáñez, Bati Yilmaz, Kangjoo Lee, Alexandra O. Cohen, Valentina Borghesani, Anton Pashkov*
- **LiveCodeBench: Holistic and Contamination Free Evaluation of Large
  Language Models for Code**, `arXiv, 2403.07974`, [arxiv](http://arxiv.org/abs/2403.07974v1), [pdf](http://arxiv.org/pdf/2403.07974v1.pdf), cication: [**-1**](None)

	 *Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, Ion Stoica*

	 · ([livecodebench.github](https://livecodebench.github.io/))
- **AutoEval Done Right: Using Synthetic Data for Model Evaluation**, `arXiv, 2403.07008`, [arxiv](http://arxiv.org/abs/2403.07008v1), [pdf](http://arxiv.org/pdf/2403.07008v1.pdf), cication: [**-1**](None)

	 *Pierre Boyeau, Anastasios N. Angelopoulos, Nir Yosef, Jitendra Malik, Michael I. Jordan*
- **AC-EVAL: Evaluating Ancient Chinese Language Understanding in Large
  Language Models**, `arXiv, 2403.06574`, [arxiv](http://arxiv.org/abs/2403.06574v1), [pdf](http://arxiv.org/pdf/2403.06574v1.pdf), cication: [**-1**](None)

	 *Yuting Wei, Yuanxing Xu, Xinru Wei, Simin Yang, Yangfu Zhu, Yuqing Li, Di Liu, Bin Wu* · ([AC-EVAL](https://github.com/yuting-wei/AC-EVAL?tab=readme-ov-file) - yuting-wei) ![Star](https://img.shields.io/github/stars/yuting-wei/AC-EVAL.svg?style=social&label=Star)
- **WorkArena: How Capable Are Web Agents at Solving Common Knowledge Work
  Tasks?**, `arXiv, 2403.07718`, [arxiv](http://arxiv.org/abs/2403.07718v1), [pdf](http://arxiv.org/pdf/2403.07718v1.pdf), cication: [**-1**](None)

	 *Alexandre Drouin, Maxime Gasse, Massimo Caccia, Issam H. Laradji, Manuel Del Verme, Tom Marty, Léo Boisvert, Megh Thakkar, Quentin Cappart, David Vazquez*
- **CLongEval: A Chinese Benchmark for Evaluating Long-Context Large
  Language Models**, `arXiv, 2403.03514`, [arxiv](http://arxiv.org/abs/2403.03514v1), [pdf](http://arxiv.org/pdf/2403.03514v1.pdf), cication: [**-1**](None)

	 *Zexuan Qiu, Jingjing Li, Shijue Huang, Wanjun Zhong, Irwin King*
- **Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference**, `arXiv, 2403.04132`, [arxiv](http://arxiv.org/abs/2403.04132v1), [pdf](http://arxiv.org/pdf/2403.04132v1.pdf), cication: [**-1**](None)

	 *Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E. Gonzalez*
- **Functional Benchmarks for Robust Evaluation of Reasoning Performance,
  and the Reasoning Gap**, `arXiv, 2402.19450`, [arxiv](http://arxiv.org/abs/2402.19450v1), [pdf](http://arxiv.org/pdf/2402.19450v1.pdf), cication: [**-1**](None)

	 *Saurabh Srivastava, Annarose M B, Anto P V, Shashank Menon, Ajay Sukumar, Adwaith Samod T, Alan Philipose, Stevin Prince, Sooraj Thomas* · ([fneval](https://github.com/consequentai/fneval/) - consequentai) ![Star](https://img.shields.io/github/stars/consequentai/fneval.svg?style=social&label=Star)
- **Evaluating Quantized Large Language Models**, `arXiv, 2402.18158`, [arxiv](http://arxiv.org/abs/2402.18158v1), [pdf](http://arxiv.org/pdf/2402.18158v1.pdf), cication: [**-1**](None)

	 *Shiyao Li, Xuefei Ning, Luning Wang, Tengxuan Liu, Xiangsheng Shi, Shengen Yan, Guohao Dai, Huazhong Yang, Yu Wang*
- **Evaluating Very Long-Term Conversational Memory of LLM Agents**, `arXiv, 2402.17753`, [arxiv](http://arxiv.org/abs/2402.17753v1), [pdf](http://arxiv.org/pdf/2402.17753v1.pdf), cication: [**-1**](None)

	 *Adyasha Maharana, Dong-Ho Lee, Sergey Tulyakov, Mohit Bansal, Francesco Barbieri, Yuwei Fang*
- **CIF-Bench: A Chinese Instruction-Following Benchmark for Evaluating the
  Generalizability of Large Language Models**, `arXiv, 2402.13109`, [arxiv](http://arxiv.org/abs/2402.13109v1), [pdf](http://arxiv.org/pdf/2402.13109v1.pdf), cication: [**-1**](None)

	 *Yizhi LI, Ge Zhang, Xingwei Qu, Jiali Li, Zhaoqun Li, Zekun Wang, Hao Li, Ruibin Yuan, Yinghao Ma, Kai Zhang* · ([CIF-Bench](https://github.com/yizhilll/CIF-Bench) - yizhilll) ![Star](https://img.shields.io/github/stars/yizhilll/CIF-Bench.svg?style=social&label=Star) · ([yizhilll.github](https://yizhilll.github.io/CIF-Bench/))
- **EmoBench: Evaluating the Emotional Intelligence of Large Language Models**, `arXiv, 2402.12071`, [arxiv](http://arxiv.org/abs/2402.12071v1), [pdf](http://arxiv.org/pdf/2402.12071v1.pdf), cication: [**-1**](None)

	 *Sahand Sabour, Siyang Liu, Zheyuan Zhang, June M. Liu, Jinfeng Zhou, Alvionna S. Sunaryo, Juanzi Li, Tatia M. C. Lee, Rada Mihalcea, Minlie Huang* · ([EmoBench](https://github.com/Sahandfer/EmoBench?tab=readme-ov-file) - Sahandfer) ![Star](https://img.shields.io/github/stars/Sahandfer/EmoBench.svg?style=social&label=Star)
- **Copilot Evaluation Harness: Evaluating LLM-Guided Software Programming**, `arXiv, 2402.14261`, [arxiv](http://arxiv.org/abs/2402.14261v1), [pdf](http://arxiv.org/pdf/2402.14261v1.pdf), cication: [**-1**](None)

	 *Anisha Agarwal, Aaron Chan, Shubham Chandel, Jinu Jang, Shaun Miller, Roshanak Zilouchian Moghaddam, Yevhen Mohylevskyy, Neel Sundaresan, Michele Tufano*
- **OlympiadBench: A Challenging Benchmark for Promoting AGI with
  Olympiad-Level Bilingual Multimodal Scientific Problems**, `arXiv, 2402.14008`, [arxiv](http://arxiv.org/abs/2402.14008v1), [pdf](http://arxiv.org/pdf/2402.14008v1.pdf), cication: [**-1**](None)

	 *Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang*
- **$\infty$Bench: Extending Long Context Evaluation Beyond 100K Tokens**, `arXiv, 2402.13718`, [arxiv](http://arxiv.org/abs/2402.13718v1), [pdf](http://arxiv.org/pdf/2402.13718v1.pdf), cication: [**-1**](None)

	 *Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao Chen, Moo Khai Hao, Xu Han, Zhen Leng Thai, Shuo Wang, Zhiyuan Liu*
- **The FinBen: An Holistic Financial Benchmark for Large Language Models**, `arXiv, 2402.12659`, [arxiv](http://arxiv.org/abs/2402.12659v1), [pdf](http://arxiv.org/pdf/2402.12659v1.pdf), cication: [**-1**](None)

	 *Qianqian Xie, Weiguang Han, Zhengyu Chen, Ruoyu Xiang, Xiao Zhang, Yueru He, Mengxi Xiao, Dong Li, Yongfu Dai, Duanyu Feng*
- **TofuEval: Evaluating Hallucinations of LLMs on Topic-Focused Dialogue
  Summarization**, `arXiv, 2402.13249`, [arxiv](http://arxiv.org/abs/2402.13249v1), [pdf](http://arxiv.org/pdf/2402.13249v1.pdf), cication: [**-1**](None)

	 *Liyan Tang, Igor Shalyminov, Amy Wing-mei Wong, Jon Burnsky, Jake W. Vincent, Yu'an Yang, Siffi Singh, Song Feng, Hwanjun Song, Hang Su*
- **LLM Comparator: Visual Analytics for Side-by-Side Evaluation of Large
  Language Models**, `arXiv, 2402.10524`, [arxiv](http://arxiv.org/abs/2402.10524v1), [pdf](http://arxiv.org/pdf/2402.10524v1.pdf), cication: [**-1**](None)

	 *Minsuk Kahng, Ian Tenney, Mahima Pushkarna, Michael Xieyang Liu, James Wexler, Emily Reif, Krystal Kallarackal, Minsuk Chang, Michael Terry, Lucas Dixon*
- **A Chain-of-Thought Is as Strong as Its Weakest Link: A Benchmark for
  Verifiers of Reasoning Chains**, `arXiv, 2402.00559`, [arxiv](http://arxiv.org/abs/2402.00559v1), [pdf](http://arxiv.org/pdf/2402.00559v1.pdf), cication: [**-1**](None)

	 *Alon Jacovi, Yonatan Bitton, Bernd Bohnet, Jonathan Herzig, Or Honovich, Michael Tseng, Michael Collins, Roee Aharoni, Mor Geva* · ([huggingface](https://huggingface.co/datasets/google/reveal))
- **E-EVAL: A Comprehensive Chinese K-12 Education Evaluation Benchmark for
  Large Language Models**, `arXiv, 2401.15927`, [arxiv](http://arxiv.org/abs/2401.15927v1), [pdf](http://arxiv.org/pdf/2401.15927v1.pdf), cication: [**-1**](None)

	 *Jinchang Hou, Chang Ao, Haihong Wu, Xiangtao Kong, Zhigang Zheng, Daijia Tang, Chengming Li, Xiping Hu, Ruifeng Xu, Shiwen Ni*
- **From GPT-4 to Gemini and Beyond: Assessing the Landscape of MLLMs on
  Generalizability, Trustworthiness and Causality through Four Modalities**, `arXiv, 2401.15071`, [arxiv](http://arxiv.org/abs/2401.15071v2), [pdf](http://arxiv.org/pdf/2401.15071v2.pdf), cication: [**-1**](None)

	 *Chaochao Lu, Chen Qian, Guodong Zheng, Hongxing Fan, Hongzhi Gao, Jie Zhang, Jing Shao, Jingyi Deng, Jinlan Fu, Kexin Huang*
- **Benchmarking LLMs via Uncertainty Quantification**, `arXiv, 2401.12794`, [arxiv](http://arxiv.org/abs/2401.12794v1), [pdf](http://arxiv.org/pdf/2401.12794v1.pdf), cication: [**-1**](None)

	 *Fanghua Ye, Mingming Yang, Jianhui Pang, Longyue Wang, Derek F. Wong, Emine Yilmaz, Shuming Shi, Zhaopeng Tu* · ([llm-uncertainty-bench](https://github.com/smartyfh/llm-uncertainty-bench?tab=readme-ov-file) - smartyfh) ![Star](https://img.shields.io/github/stars/smartyfh/llm-uncertainty-bench.svg?style=social&label=Star)
- **CMMMU: A Chinese Massive Multi-discipline Multimodal Understanding
  Benchmark**, `arXiv, 2401.11944`, [arxiv](http://arxiv.org/abs/2401.11944v1), [pdf](http://arxiv.org/pdf/2401.11944v1.pdf), cication: [**-1**](None)

	 *Ge Zhang, Xinrun Du, Bei Chen, Yiming Liang, Tongxu Luo, Tianyu Zheng, Kang Zhu, Yuyang Cheng, Chunpu Xu, Shuyue Guo*
- **AttackEval: How to Evaluate the Effectiveness of Jailbreak Attacking on
  Large Language Models**, `arXiv, 2401.09002`, [arxiv](http://arxiv.org/abs/2401.09002v1), [pdf](http://arxiv.org/pdf/2401.09002v1.pdf), cication: [**-1**](None)

	 *Dong shu, Mingyu Jin, Suiyuan Zhu, Beichen Wang, Zihao Zhou, Chong Zhang, Yongfeng Zhang*

- **Narrowing the Knowledge Evaluation Gap: Open-Domain Question Answering
  with Multi-Granularity Answers**, `arXiv, 2401.04695`, [arxiv](http://arxiv.org/abs/2401.04695v1), [pdf](http://arxiv.org/pdf/2401.04695v1.pdf), cication: [**-1**](None)

	 *Gal Yona, Roee Aharoni, Mor Geva*
- **CRUXEval: A Benchmark for Code Reasoning, Understanding and Execution**, `arXiv, 2401.03065`, [arxiv](http://arxiv.org/abs/2401.03065v1), [pdf](http://arxiv.org/pdf/2401.03065v1.pdf), cication: [**-1**](None)

	 *Alex Gu, Baptiste Rozière, Hugh Leather, Armando Solar-Lezama, Gabriel Synnaeve, Sida I. Wang*
- **Has Your Pretrained Model Improved? A Multi-head Posterior Based
  Approach**, `arXiv, 2401.02987`, [arxiv](http://arxiv.org/abs/2401.02987v1), [pdf](http://arxiv.org/pdf/2401.02987v1.pdf), cication: [**-1**](None)

	 *Prince Aboagye, Yan Zheng, Junpeng Wang, Uday Singh Saini, Xin Dai, Michael Yeh, Yujie Fan, Zhongfang Zhuang, Shubham Jain, Liang Wang*
- **Can AI Be as Creative as Humans?**, `arXiv, 2401.01623`, [arxiv](http://arxiv.org/abs/2401.01623v2), [pdf](http://arxiv.org/pdf/2401.01623v2.pdf), cication: [**-1**](None)

	 *Haonan Wang, James Zou, Michael Mozer, Anirudh Goyal, Alex Lamb, Linjun Zhang, Weijie J Su, Zhun Deng, Michael Qizhe Xie, Hannah Brown*
- **Task Contamination: Language Models May Not Be Few-Shot Anymore**, `arXiv, 2312.16337`, [arxiv](http://arxiv.org/abs/2312.16337v1), [pdf](http://arxiv.org/pdf/2312.16337v1.pdf), cication: [**-1**](None)

	 *Changmao Li, Jeffrey Flanigan* · ([mp.weixin.qq](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652426123&idx=3&sn=4715e6475a7b48c52240b79e03171210))
- **Gemini in Reasoning: Unveiling Commonsense in Multimodal Large Language
  Models**, `arXiv, 2312.17661`, [arxiv](http://arxiv.org/abs/2312.17661v1), [pdf](http://arxiv.org/pdf/2312.17661v1.pdf), cication: [**-1**](None)

	 *Yuqing Wang, Yun Zhao*
- **Comparing Humans, GPT-4, and GPT-4V On Abstraction and Reasoning Tasks**, `arXiv, 2311.09247`, [arxiv](http://arxiv.org/abs/2311.09247v3), [pdf](http://arxiv.org/pdf/2311.09247v3.pdf), cication: [**-1**](None)

	 *Melanie Mitchell, Alessandro B. Palmarini, Arseny Moskvichev* · ([mp.weixin.qq](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652425976&idx=3&sn=bd62c6ce6e66fe581c724935eca5716d&poc_token=HOGvkmWjhD-kCvFC9CTEEaU8I5ShCtege3HP4PyI))
- **Gemini vs GPT-4V: A Preliminary Comparison and Combination of
  Vision-Language Models Through Qualitative Cases**, `arXiv, 2312.15011`, [arxiv](http://arxiv.org/abs/2312.15011v1), [pdf](http://arxiv.org/pdf/2312.15011v1.pdf), cication: [**-1**](None)

	 *Zhangyang Qi, Ye Fang, Mengchen Zhang, Zeyi Sun, Tong Wu, Ziwei Liu, Dahua Lin, Jiaqi Wang, Hengshuang Zhao*

	 · ([gemini-vs-gpt4v](https://github.com/qi-zhangyang/gemini-vs-gpt4v) - qi-zhangyang) ![Star](https://img.shields.io/github/stars/qi-zhangyang/gemini-vs-gpt4v.svg?style=social&label=Star)
- **Principled Instructions Are All You Need for Questioning LLaMA-1/2,
  GPT-3.5/4**, `arXiv, 2312.16171`, [arxiv](http://arxiv.org/abs/2312.16171v1), [pdf](http://arxiv.org/pdf/2312.16171v1.pdf), cication: [**-1**](None)

	 *Sondos Mahmoud Bsharat, Aidar Myrzakhan, Zhiqiang Shen*

	 · ([ATLAS](https://github.com/VILA-Lab/ATLAS) - VILA-Lab) ![Star](https://img.shields.io/github/stars/VILA-Lab/ATLAS.svg?style=social&label=Star)
- **LLM4VG: Large Language Models Evaluation for Video Grounding**, `arXiv, 2312.14206`, [arxiv](http://arxiv.org/abs/2312.14206v1), [pdf](http://arxiv.org/pdf/2312.14206v1.pdf), cication: [**-1**](None)

	 *Wei Feng, Xin Wang, Hong Chen, Zeyang Zhang, Zihan Song, Yuwei Zhou, Wenwu Zhu*
- **Jack of All Tasks, Master of Many: Designing General-purpose
  Coarse-to-Fine Vision-Language Model**, `arXiv, 2312.12423`, [arxiv](http://arxiv.org/abs/2312.12423v1), [pdf](http://arxiv.org/pdf/2312.12423v1.pdf), cication: [**-1**](None)

	 *Shraman Pramanick, Guangxing Han, Rui Hou, Sayan Nag, Ser-Nam Lim, Nicolas Ballas, Qifan Wang, Rama Chellappa, Amjad Almahairi* · ([shramanpramanick.github](https://shramanpramanick.github.io/VistaLLM/))
- **A Challenger to GPT-4V? Early Explorations of Gemini in Visual Expertise**, `arXiv, 2312.12436`, [arxiv](http://arxiv.org/abs/2312.12436v2), [pdf](http://arxiv.org/pdf/2312.12436v2.pdf), cication: [**-1**](None)

	 *Chaoyou Fu, Renrui Zhang, Zihan Wang, Yubo Huang, Zhengye Zhang, Longtian Qiu, Gaoxiang Ye, Yunhang Shen, Mengdan Zhang, Peixian Chen* · ([Awesome-Multimodal-Large-Language-Models](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models) - BradyFU) ![Star](https://img.shields.io/github/stars/BradyFU/Awesome-Multimodal-Large-Language-Models.svg?style=social&label=Star)
	 · ([mp.weixin.qq](https://mp.weixin.qq.com/s?__biz=MzAxMTk4NDkwNw==&mid=2247495335&idx=1&sn=bd1f60cc04239669203c944824acf171))
	 · ([mp.weixin.qq](https://mp.weixin.qq.com/s?__biz=MzAxMTk4NDkwNw==&mid=2247495335&idx=1&sn=bd1f60cc04239669203c944824acf171&poc_token=HF7ojmWjBpIfrreUuRxQCloV4ajsKbiPcePT5WvD))
- **The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context
  Learning**, `arXiv, 2312.01552`, [arxiv](http://arxiv.org/abs/2312.01552v1), [pdf](http://arxiv.org/pdf/2312.01552v1.pdf), cication: [**-1**](None)

	 *Bill Yuchen Lin, Abhilasha Ravichander, Ximing Lu, Nouha Dziri, Melanie Sclar, Khyathi Chandu, Chandra Bhagavatula, Yejin Choi* · ([huggingface](https://huggingface.co/spaces/allenai/URIAL-Bench)) · ([URIAL](https://github.com/Re-Align/URIAL) - Re-Align) ![Star](https://img.shields.io/github/stars/Re-Align/URIAL.svg?style=social&label=Star)
- **An In-depth Look at Gemini's Language Abilities**, `arXiv, 2312.11444`, [arxiv](http://arxiv.org/abs/2312.11444v1), [pdf](http://arxiv.org/pdf/2312.11444v1.pdf), cication: [**-1**](None)

	 *Syeda Nahida Akter, Zichun Yu, Aashiq Muhamed, Tianyue Ou, Alex Bäuerle, Ángel Alexander Cabrera, Krish Dholakia, Chenyan Xiong, Graham Neubig* · ([gemini-benchmark](https://github.com/neulab/gemini-benchmark) - neulab) ![Star](https://img.shields.io/github/stars/neulab/gemini-benchmark.svg?style=social&label=Star)
- **Catwalk: A Unified Language Model Evaluation Framework for Many Datasets**, `arXiv, 2312.10253`, [arxiv](http://arxiv.org/abs/2312.10253v1), [pdf](http://arxiv.org/pdf/2312.10253v1.pdf), cication: [**-1**](None)

	 *Dirk Groeneveld, Anas Awadalla, Iz Beltagy, Akshita Bhagia, Ian Magnusson, Hao Peng, Oyvind Tafjord, Pete Walsh, Kyle Richardson, Jesse Dodge* · ([catwalk](https://github.com/allenai/catwalk) - allenai) ![Star](https://img.shields.io/github/stars/allenai/catwalk.svg?style=social&label=Star)
- **PromptBench: A Unified Library for Evaluation of Large Language Models**, `arXiv, 2312.07910`, [arxiv](http://arxiv.org/abs/2312.07910v1), [pdf](http://arxiv.org/pdf/2312.07910v1.pdf), cication: [**-1**](None)

	 *Kaijie Zhu, Qinlin Zhao, Hao Chen, Jindong Wang, Xing Xie*

	 · ([promptbench](https://github.com/microsoft/promptbench) - microsoft) ![Star](https://img.shields.io/github/stars/microsoft/promptbench.svg?style=social&label=Star)
- **CritiqueLLM: Scaling LLM-as-Critic for Effective and Explainable
  Evaluation of Large Language Model Generation**, `arXiv, 2311.18702`, [arxiv](http://arxiv.org/abs/2311.18702v1), [pdf](http://arxiv.org/pdf/2311.18702v1.pdf), cication: [**-1**](None)

	 *Pei Ke, Bosi Wen, Zhuoer Feng, Xiao Liu, Xuanyu Lei, Jiale Cheng, Shengyuan Wang, Aohan Zeng, Yuxiao Dong, Hongning Wang*

	 · ([critiquellm](https://github.com/thu-coai/critiquellm) - thu-coai) ![Star](https://img.shields.io/github/stars/thu-coai/critiquellm.svg?style=social&label=Star)
- **How Well Does GPT-4V(ision) Adapt to Distribution Shifts? A Preliminary
  Investigation**, `arXiv, 2312.07424`, [arxiv](http://arxiv.org/abs/2312.07424v2), [pdf](http://arxiv.org/pdf/2312.07424v2.pdf), cication: [**-1**](None)

	 *Zhongyi Han, Guanglin Zhou, Rundong He, Jindong Wang, Tailin Wu, Yilong Yin, Salman Khan, Lina Yao, Tongliang Liu, Kun Zhang*

	 · ([gpt-4v-distribution-shift](https://github.com/jameszhou-gl/gpt-4v-distribution-shift) - jameszhou-gl) ![Star](https://img.shields.io/github/stars/jameszhou-gl/gpt-4v-distribution-shift.svg?style=social&label=Star)
- [Catch me if you can! How to beat GPT-4 with a 13B model | LMSYS Org](https://lmsys.org/blog/2023-11-14-llm-decontaminator/)

	 · ([youtube](https://www.youtube.com/watch?v=dxH1GFCfdF0&t=2s&ab_channel=MatthewBerman))
- **Evaluating and Mitigating Discrimination in Language Model Decisions**, `arXiv, 2312.03689`, [arxiv](http://arxiv.org/abs/2312.03689v1), [pdf](http://arxiv.org/pdf/2312.03689v1.pdf), cication: [**-1**](None)

	 *Alex Tamkin, Amanda Askell, Liane Lovitt, Esin Durmus, Nicholas Joseph, Shauna Kravec, Karina Nguyen, Jared Kaplan, Deep Ganguli* · ([huggingface](https://huggingface.co/datasets/Anthropic/discrim-eval))
- **Instruction-Following Evaluation for Large Language Models**, `arXiv, 2311.07911`, [arxiv](http://arxiv.org/abs/2311.07911v1), [pdf](http://arxiv.org/pdf/2311.07911v1.pdf), cication: [**-1**](None)

	 *Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, Le Hou* · ([google-research](https://github.com/google-research/google-research/tree/master/instruction_following_eval) - google-research) ![Star](https://img.shields.io/github/stars/google-research/google-research.svg?style=social&label=Star)
- **Unnatural Error Correction: GPT-4 Can Almost Perfectly Handle Unnatural
  Scrambled Text**, `arXiv, 2311.18805`, [arxiv](http://arxiv.org/abs/2311.18805v1), [pdf](http://arxiv.org/pdf/2311.18805v1.pdf), cication: [**-1**](None)

	 *Qi Cao, Takeshi Kojima, Yutaka Matsuo, Yusuke Iwasawa* · ([qbitai](https://www.qbitai.com/2023/12/103102.html))
- **GPQA: A Graduate-Level Google-Proof Q&A Benchmark**, `arXiv, 2311.12022`, [arxiv](http://arxiv.org/abs/2311.12022v1), [pdf](http://arxiv.org/pdf/2311.12022v1.pdf), cication: [**-1**](None)

	 *David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, Samuel R. Bowman*
- **SelfEval: Leveraging the discriminative nature of generative models for
  evaluation**, `arXiv, 2311.10708`, [arxiv](http://arxiv.org/abs/2311.10708v1), [pdf](http://arxiv.org/pdf/2311.10708v1.pdf), cication: [**-1**](None)

	 *Sai Saketh Rambhatla, Ishan Misra*
- **Rethinking Benchmark and Contamination for Language Models with
  Rephrased Samples**, `arXiv, 2311.04850`, [arxiv](http://arxiv.org/abs/2311.04850v2), [pdf](http://arxiv.org/pdf/2311.04850v2.pdf), cication: [**-1**](None)

	 *Shuo Yang, Wei-Lin Chiang, Lianmin Zheng, Joseph E. Gonzalez, Ion Stoica* · ([llm-decontaminator](https://github.com/lm-sys/llm-decontaminator) - lm-sys) ![Star](https://img.shields.io/github/stars/lm-sys/llm-decontaminator.svg?style=social&label=Star) · ([jiqizhixin](https://www.jiqizhixin.com/articles/2023-11-18-7))
- **Fusion-Eval: Integrating Evaluators with LLMs**, `arXiv, 2311.09204`, [arxiv](http://arxiv.org/abs/2311.09204v1), [pdf](http://arxiv.org/pdf/2311.09204v1.pdf), cication: [**-1**](None)

	 *Lei Shu, Nevan Wichers, Liangchen Luo, Yun Zhu, Yinxiao Liu, Jindong Chen, Lei Meng*
- **Llamas Know What GPTs Don't Show: Surrogate Models for Confidence
  Estimation**, `arXiv, 2311.08877`, [arxiv](http://arxiv.org/abs/2311.08877v1), [pdf](http://arxiv.org/pdf/2311.08877v1.pdf), cication: [**-1**](None)

	 *Vaishnavi Shrivastava, Percy Liang, Ananya Kumar*
- **Instruction-Following Evaluation for Large Language Models**, `arXiv, 2311.07911`, [arxiv](http://arxiv.org/abs/2311.07911v1), [pdf](http://arxiv.org/pdf/2311.07911v1.pdf), cication: [**-1**](None)

	 *Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, Le Hou* · ([google-research](https://github.com/google-research/google-research/tree/master/instruction_following_eval) - google-research) ![Star](https://img.shields.io/github/stars/google-research/google-research.svg?style=social&label=Star)
- **MEGAVERSE: Benchmarking Large Language Models Across Languages,
  Modalities, Models and Tasks**, `arXiv, 2311.07463`, [arxiv](http://arxiv.org/abs/2311.07463v1), [pdf](http://arxiv.org/pdf/2311.07463v1.pdf), cication: [**-1**](None)

	 *Sanchit Ahuja, Divyanshu Aggarwal, Varun Gumma, Ishaan Watts, Ashutosh Sathe, Millicent Ochieng, Rishav Hada, Prachi Jain, Maxamed Axmed, Kalika Bali*
- **TIGERScore: Towards Building Explainable Metric for All Text Generation
  Tasks**, `arXiv, 2310.00752`, [arxiv](http://arxiv.org/abs/2310.00752v3), [pdf](http://arxiv.org/pdf/2310.00752v3.pdf), cication: [**11**](https://scholar.google.com/scholar?cites=2304567616397319447&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Dongfu Jiang, Yishan Li, Ge Zhang, Wenhao Huang, Bill Yuchen Lin, Wenhu Chen* · ([huggingface](https://huggingface.co/spaces/TIGER-Lab/TIGERScore))
- **Generative Judge for Evaluating Alignment**, `arXiv, 2310.05470`, [arxiv](http://arxiv.org/abs/2310.05470v1), [pdf](http://arxiv.org/pdf/2310.05470v1.pdf), cication: [**-1**](None)

	 *Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, Hai Zhao, Pengfei Liu* · ([auto-j](https://github.com/GAIR-NLP/auto-j) - GAIR-NLP) ![Star](https://img.shields.io/github/stars/GAIR-NLP/auto-j.svg?style=social&label=Star) · ([gair-nlp.github](https://gair-nlp.github.io/auto-j/))
- **Can LLMs Follow Simple Rules?**, `arXiv, 2311.04235`, [arxiv](http://arxiv.org/abs/2311.04235v1), [pdf](http://arxiv.org/pdf/2311.04235v1.pdf), cication: [**-1**](None)

	 *Norman Mu, Sarah Chen, Zifan Wang, Sizhe Chen, David Karamardian, Lulwa Aljeraisy, Dan Hendrycks, David Wagner*

	 · ([llm_rules](https://github.com/normster/llm_rules) - normster) ![Star](https://img.shields.io/github/stars/normster/llm_rules.svg?style=social&label=Star)
- **Don't Make Your LLM an Evaluation Benchmark Cheater**, `arXiv, 2311.01964`, [arxiv](http://arxiv.org/abs/2311.01964v1), [pdf](http://arxiv.org/pdf/2311.01964v1.pdf), cication: [**-1**](None)

	 *Kun Zhou, Yutao Zhu, Zhipeng Chen, Wentong Chen, Wayne Xin Zhao, Xu Chen, Yankai Lin, Ji-Rong Wen, Jiawei Han*
- **PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task
  Completion**, `arXiv, 2311.01767`, [arxiv](http://arxiv.org/abs/2311.01767v2), [pdf](http://arxiv.org/pdf/2311.01767v2.pdf), cication: [**-1**](None)

	 *Yiduo Guo, Zekai Zhang, Yaobo Liang, Dongyan Zhao, Nan Duan*
- **GPT-Fathom: Benchmarking Large Language Models to Decipher the
  Evolutionary Path towards GPT-4 and Beyond**, `arXiv, 2309.16583`, [arxiv](http://arxiv.org/abs/2309.16583v3), [pdf](http://arxiv.org/pdf/2309.16583v3.pdf), cication: [**1**](https://scholar.google.com/scholar?cites=11825396027393223998&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Shen Zheng, Yuyu Zhang, Yijie Zhu, Chenguang Xi, Pengyang Gao, Xun Zhou, Kevin Chen-Chuan Chang* · ([gpt-fathom](https://github.com/gpt-fathom/gpt-fathom) - gpt-fathom) ![Star](https://img.shields.io/github/stars/gpt-fathom/gpt-fathom.svg?style=social&label=Star) · [qbitai](https://www.qbitai.com/2023/11/95409.html)]
- **Leveraging Word Guessing Games to Assess the Intelligence of Large
  Language Models**, `arXiv, 2310.20499`, [arxiv](http://arxiv.org/abs/2310.20499v2), [pdf](http://arxiv.org/pdf/2310.20499v2.pdf), cication: [**-1**](None)

	 *Tian Liang, Zhiwei He, Jen-tse Huang, Wenxuan Wang, Wenxiang Jiao, Rui Wang, Yujiu Yang, Zhaopeng Tu, Shuming Shi, Xing Wang*
- **Does GPT-4 Pass the Turing Test?**, `arXiv, 2310.20216`, [arxiv](http://arxiv.org/abs/2310.20216v1), [pdf](http://arxiv.org/pdf/2310.20216v1.pdf), cication: [**-1**](None)

	 *Cameron Jones, Benjamin Bergen*
- **ALCUNA: Large Language Models Meet New Knowledge**, `arXiv, 2310.14820`, [arxiv](http://arxiv.org/abs/2310.14820v1), [pdf](http://arxiv.org/pdf/2310.14820v1.pdf), cication: [**-1**](None)

	 *Xunjian Yin, Baizhou Huang, Xiaojun Wan*
- **Evaluation Metrics in the Era of GPT-4: Reliably Evaluating Large
  Language Models on Sequence to Sequence Tasks**, `arXiv, 2310.13800`, [arxiv](http://arxiv.org/abs/2310.13800v1), [pdf](http://arxiv.org/pdf/2310.13800v1.pdf), cication: [**2**](https://scholar.google.com/scholar?cites=9747894331578744789&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Andrea Sottana, Bin Liang, Kai Zou, Zheng Yuan*
- **A Framework for Automated Measurement of Responsible AI Harms in
  Generative AI Applications**, `arXiv, 2310.17750`, [arxiv](http://arxiv.org/abs/2310.17750v1), [pdf](http://arxiv.org/pdf/2310.17750v1.pdf), cication: [**-1**](None)

	 *Ahmed Magooda, Alec Helyar, Kyle Jackson, David Sullivan, Chad Atalla, Emily Sheng, Dan Vann, Richard Edgar, Hamid Palangi, Roman Lutz*
- **Exploring OCR Capabilities of GPT-4V(ision) : A Quantitative and
  In-depth Evaluation**, `arXiv, 2310.16809`, [arxiv](http://arxiv.org/abs/2310.16809v2), [pdf](http://arxiv.org/pdf/2310.16809v2.pdf), cication: [**2**](https://scholar.google.com/scholar?cites=7703323566044468754&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Yongxin Shi, Dezhi Peng, Wenhui Liao, Zening Lin, Xinhong Chen, Chongyu Liu, Yuyi Zhang, Lianwen Jin* · ([gpt-4v_ocr](https://github.com/scut-dlvclab/gpt-4v_ocr) - scut-dlvclab) ![Star](https://img.shields.io/github/stars/scut-dlvclab/gpt-4v_ocr.svg?style=social&label=Star)
- **JudgeLM: Fine-tuned Large Language Models are Scalable Judges**, `arXiv, 2310.17631`, [arxiv](http://arxiv.org/abs/2310.17631v1), [pdf](http://arxiv.org/pdf/2310.17631v1.pdf), cication: [**-1**](None)

	 *Lianghui Zhu, Xinggang Wang, Xinlong Wang*
- **An Early Evaluation of GPT-4V(ision)**, `arXiv, 2310.16534`, [arxiv](http://arxiv.org/abs/2310.16534v1), [pdf](http://arxiv.org/pdf/2310.16534v1.pdf), cication: [**1**](https://scholar.google.com/scholar?cites=11688624601117600790&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Yang Wu, Shilong Wang, Hao Yang, Tian Zheng, Hongbo Zhang, Yanyan Zhao, Bing Qin*
- **Generative Judge for Evaluating Alignment**, `arXiv, 2310.05470`, [arxiv](http://arxiv.org/abs/2310.05470v1), [pdf](http://arxiv.org/pdf/2310.05470v1.pdf), cication: [**-1**](None)

	 *Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, Hai Zhao, Pengfei Liu*
- **The Foundation Model Transparency Index**, `arXiv, 2310.12941`, [arxiv](http://arxiv.org/abs/2310.12941v1), [pdf](http://arxiv.org/pdf/2310.12941v1.pdf), cication: [**3**](https://scholar.google.com/scholar?cites=6897485371215453301&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Rishi Bommasani, Kevin Klyman, Shayne Longpre, Sayash Kapoor, Nestor Maslej, Betty Xiong, Daniel Zhang, Percy Liang* · [qbitai](https://www.qbitai.com/2023/10/91045.html)] · [crfm.stanford](https://crfm.stanford.edu/fmti/fmti.pdf)]
- **ReForm-Eval: Evaluating Large Vision Language Models via Unified
  Re-Formulation of Task-Oriented Benchmarks**, `arXiv, 2310.02569`, [arxiv](http://arxiv.org/abs/2310.02569v2), [pdf](http://arxiv.org/pdf/2310.02569v2.pdf), cication: [**-1**](None)

	 *Zejun Li, Ye Wang, Mengfei Du, Qingwen Liu, Binhao Wu, Jiwen Zhang, Chengxing Zhou, Zhihao Fan, Jie Fu, Jingjing Chen* · [jiqizhixin](https://www.jiqizhixin.com/articles/2023-10-20)]
- **CLEVA: Chinese Language Models EVAluation Platform**, `arXiv, 2308.04813`, [arxiv](http://arxiv.org/abs/2308.04813v2), [pdf](http://arxiv.org/pdf/2308.04813v2.pdf), cication: [**-1**](None)

	 *Yanyang Li, Jianqiao Zhao, Duo Zheng, Zi-Yuan Hu, Zhi Chen, Xiaohui Su, Yongfeng Huang, Shijia Huang, Dahua Lin, Michael R. Lyu* · [qbitai](https://www.qbitai.com/2023/10/90385.html)]
- **Prometheus: Inducing Fine-grained Evaluation Capability in Language
  Models**, `arXiv, 2310.08491`, [arxiv](http://arxiv.org/abs/2310.08491v1), [pdf](http://arxiv.org/pdf/2310.08491v1.pdf), cication: [**-1**](None)

	 *Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne*
- **A Closer Look into Automatic Evaluation Using Large Language Models**, `arXiv, 2310.05657`, [arxiv](http://arxiv.org/abs/2310.05657v1), [pdf](http://arxiv.org/pdf/2310.05657v1.pdf), cication: [**-1**](None)

	 *Cheng-Han Chiang, Hung-yi Lee* · ([mp.weixin.qq](https://mp.weixin.qq.com/s?__biz=MzIyMzk1MDE3Nw==&mid=2247613756&idx=5&sn=e3241595841293d9e0b73c6b2e70a21d)) · ([A-Closer-Look-To-LLM-Evaluation](https://github.com/d223302/A-Closer-Look-To-LLM-Evaluation/) - d223302) ![Star](https://img.shields.io/github/stars/d223302/A-Closer-Look-To-LLM-Evaluation.svg?style=social&label=Star)
- **Probing the Moral Development of Large Language Models through Defining
  Issues Test**, `arXiv, 2309.13356`, [arxiv](http://arxiv.org/abs/2309.13356v2), [pdf](http://arxiv.org/pdf/2309.13356v2.pdf), cication: [**-1**](None)

	 *Kumar Tanmay, Aditi Khandelwal, Utkarsh Agarwal, Monojit Choudhury* · [mp.weixin.qq](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652388927&idx=4&sn=5c9897283252d8f266144fbcaa9bc43e)]
- **GPT-Fathom: Benchmarking Large Language Models to Decipher the
  Evolutionary Path towards GPT-4 and Beyond**, `arXiv, 2309.16583`, [arxiv](http://arxiv.org/abs/2309.16583v3), [pdf](http://arxiv.org/pdf/2309.16583v3.pdf), cication: [**1**](https://scholar.google.com/scholar?cites=11825396027393223998&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Shen Zheng, Yuyu Zhang, Yijie Zhu, Chenguang Xi, Pengyang Gao, Xun Zhou, Kevin Chen-Chuan Chang*
- **Calibrating LLM-Based Evaluator**, `arXiv, 2309.13308`, [arxiv](http://arxiv.org/abs/2309.13308v1), [pdf](http://arxiv.org/pdf/2309.13308v1.pdf), cication: [**-1**](None)

	 *Yuxuan Liu, Tianchi Yang, Shaohan Huang, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, Qi Zhang*
- **Probing the Moral Development of Large Language Models through Defining
  Issues Test**, `arXiv, 2309.13356`, [arxiv](http://arxiv.org/abs/2309.13356v2), [pdf](http://arxiv.org/pdf/2309.13356v2.pdf), cication: [**-1**](None)

	 *Kumar Tanmay, Aditi Khandelwal, Utkarsh Agarwal, Monojit Choudhury*
- **Struc-Bench: Are Large Language Models Really Good at Generating Complex
  Structured Data?**, `arXiv, 2309.08963`, [arxiv](http://arxiv.org/abs/2309.08963v2), [pdf](http://arxiv.org/pdf/2309.08963v2.pdf), cication: [**1**](https://scholar.google.com/scholar?cites=2082058242118583458&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Xiangru Tang, Yiming Zong, Jason Phang, Yilun Zhao, Wangchunshu Zhou, Arman Cohan, Mark Gerstein*
- **Investigating Answerability of LLMs for Long-Form Question Answering**, `arXiv, 2309.08210`, [arxiv](http://arxiv.org/abs/2309.08210v1), [pdf](http://arxiv.org/pdf/2309.08210v1.pdf), cication: [**-1**](None)

	 *Meghana Moorthy Bhat, Rui Meng, Ye Liu, Yingbo Zhou, Semih Yavuz*
- **Are Large Language Model-based Evaluators the Solution to Scaling Up
  Multilingual Evaluation?**, `arXiv, 2309.07462`, [arxiv](http://arxiv.org/abs/2309.07462v1), [pdf](http://arxiv.org/pdf/2309.07462v1.pdf), cication: [**4**](https://scholar.google.com/scholar?cites=3253818796886735588&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Rishav Hada, Varun Gumma, Adrian de Wynter, Harshita Diddee, Mohamed Ahmed, Monojit Choudhury, Kalika Bali, Sunayana Sitaram*
- **Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs**, `arXiv, 2308.13387`, [arxiv](http://arxiv.org/abs/2308.13387v2), [pdf](http://arxiv.org/pdf/2308.13387v2.pdf), cication: [**3**](https://scholar.google.com/scholar?cites=2648067072454919746&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Yuxia Wang, Haonan Li, Xudong Han, Preslav Nakov, Timothy Baldwin* · [jiqizhixin](https://www.jiqizhixin.com/articles/2023-09-05-7)]
	 
- **DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT
  Models**, `arXiv, 2306.11698`, [arxiv](http://arxiv.org/abs/2306.11698v2), [pdf](http://arxiv.org/pdf/2306.11698v2.pdf), cication: [**-1**](None)

	 *Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui Zhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer*

	 · ([decodingtrust.github](https://decodingtrust.github.io/))
- **Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena**, `arXiv, 2306.05685`, [arxiv](http://arxiv.org/abs/2306.05685v3), [pdf](http://arxiv.org/pdf/2306.05685v3.pdf), cication: [**-1**](None)

	 *Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing*
- **The ConceptARC Benchmark: Evaluating Understanding and Generalization in
  the ARC Domain**, `arXiv, 2305.07141`, [arxiv](http://arxiv.org/abs/2305.07141v1), [pdf](http://arxiv.org/pdf/2305.07141v1.pdf), cication: [**10**](https://scholar.google.com/scholar?cites=8946879368609581934&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Arseny Moskvichev, Victor Vikram Odouard, Melanie Mitchell* · [mp.weixin.qq](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652371669&idx=4&sn=cb7350a12a2ff8a4c238d7220b2273a5)]
- **MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities**, `arXiv, 2308.02490`, [arxiv](http://arxiv.org/abs/2308.02490v3), [pdf](http://arxiv.org/pdf/2308.02490v3.pdf), cication: [**10**](https://scholar.google.com/scholar?cites=5160742062303244685&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, Lijuan Wang*
- **ARB: Advanced Reasoning Benchmark for Large Language Models**, `arXiv, 2307.13692`, [arxiv](http://arxiv.org/abs/2307.13692v2), [pdf](http://arxiv.org/pdf/2307.13692v2.pdf), cication: [**6**](https://scholar.google.com/scholar?cites=5512964966028642645&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Tomohiro Sawada, Daniel Paleka, Alexander Havrilla, Pranav Tadepalli, Paula Vidas, Alexander Kranias, John J. Nay, Kshitij Gupta, Aran Komatsuzaki*
- **L-Eval: Instituting Standardized Evaluation for Long Context Language
  Models**, `arXiv, 2307.11088`, [arxiv](http://arxiv.org/abs/2307.11088v3), [pdf](http://arxiv.org/pdf/2307.11088v3.pdf), cication: [**-1**](None)

	 *Chenxin An, Shansan Gong, Ming Zhong, Xingjian Zhao, Mukai Li, Jun Zhang, Lingpeng Kong, Xipeng Qiu* · ([leval](https://github.com/openlmlab/leval) - openlmlab) ![Star](https://img.shields.io/github/stars/openlmlab/leval.svg?style=social&label=Star)
- **Instruction-following Evaluation through Verbalizer Manipulation**, `arXiv, 2307.10558`, [arxiv](http://arxiv.org/abs/2307.10558v1), [pdf](http://arxiv.org/pdf/2307.10558v1.pdf), cication: [**4**](https://scholar.google.com/scholar?cites=3294772868387818937&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Shiyang Li, Jun Yan, Hai Wang, Zheng Tang, Xiang Ren, Vijay Srinivasan, Hongxia Jin*
- **FLASK: Fine-grained Language Model Evaluation based on Alignment Skill
  Sets**, `arXiv, 2307.10928`, [arxiv](http://arxiv.org/abs/2307.10928v2), [pdf](http://arxiv.org/pdf/2307.10928v2.pdf), cication: [**5**](https://scholar.google.com/scholar?cites=11317511132993952746&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Seonghyeon Ye, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, Seungone Kim, Yongrae Jo, James Thorne, Juho Kim, Minjoon Seo*
- **How is ChatGPT's behavior changing over time?**, `arXiv, 2307.09009`, [arxiv](http://arxiv.org/abs/2307.09009v3), [pdf](http://arxiv.org/pdf/2307.09009v3.pdf), cication: [**64**](https://scholar.google.com/scholar?cites=7228649203975536747&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Lingjiao Chen, Matei Zaharia, James Zou*
- **Generating Benchmarks for Factuality Evaluation of Language Models**, `arXiv, 2307.06908`, [arxiv](http://arxiv.org/abs/2307.06908v1), [pdf](http://arxiv.org/pdf/2307.06908v1.pdf), cication: [**6**](https://scholar.google.com/scholar?cites=5608570652501844927&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Dor Muhlgay, Ori Ram, Inbal Magar, Yoav Levine, Nir Ratner, Yonatan Belinkov, Omri Abend, Kevin Leyton-Brown, Amnon Shashua, Yoav Shoham*

	 · ([factor](https://github.com/AI21Labs/factor) - AI21Labs) ![Star](https://img.shields.io/github/stars/AI21Labs/factor.svg?style=social&label=Star)

- **PromptBench: Towards Evaluating the Robustness of Large Language Models
  on Adversarial Prompts**, `arXiv, 2306.04528`, [arxiv](http://arxiv.org/abs/2306.04528v4), [pdf](http://arxiv.org/pdf/2306.04528v4.pdf), cication: [**32**](https://scholar.google.com/scholar?cites=6727691362756502405&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong Wang, Linyi Yang, Wei Ye, Yue Zhang, Neil Zhenqiang Gong* · ([promptbench](https://github.com/microsoft/promptbench) - microsoft) ![Star](https://img.shields.io/github/stars/microsoft/promptbench.svg?style=social&label=Star)
- **Empowering Cross-lingual Behavioral Testing of NLP Models with
  Typological Features**, `arXiv, 2307.05454`, [arxiv](http://arxiv.org/abs/2307.05454v1), [pdf](http://arxiv.org/pdf/2307.05454v1.pdf), cication: [**-1**](None)

	 *Ester Hlavnova, Sebastian Ruder*
- **GLUE-X: Evaluating Natural Language Understanding Models from an
  Out-of-distribution Generalization Perspective**, `arXiv, 2211.08073`, [arxiv](http://arxiv.org/abs/2211.08073v4), [pdf](http://arxiv.org/pdf/2211.08073v4.pdf), cication: [**21**](https://scholar.google.com/scholar?cites=2869414843777058922&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Linyi Yang, Shuibai Zhang, Libo Qin, Yafu Li, Yidong Wang, Hanmeng Liu, Jindong Wang, Xing Xie, Yue Zhang*
- **M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining
  Large Language Models**, `arXiv, 2306.05179`, [arxiv](http://arxiv.org/abs/2306.05179v1), [pdf](http://arxiv.org/pdf/2306.05179v1.pdf), cication: [**7**](https://scholar.google.com/scholar?cites=5326118821139338567&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Wenxuan Zhang, Sharifah Mahani Aljunied, Chang Gao, Yew Ken Chia, Lidong Bing* · [jiqizhixin](https://www.jiqizhixin.com/articles/2023-07-03-7)] · ([M3Exam](https://github.com/DAMO-NLP-SG/M3Exam) - DAMO-NLP-SG) ![Star](https://img.shields.io/github/stars/DAMO-NLP-SG/M3Exam.svg?style=social&label=Star)
- **MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language
  Models**, `arXiv, 2306.13394`, [arxiv](http://arxiv.org/abs/2306.13394v2), [pdf](http://arxiv.org/pdf/2306.13394v2.pdf), cication: [**32**](https://scholar.google.com/scholar?cites=8526876186588328715&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng* · ([Awesome-Multimodal-Large-Language-Models](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models) - BradyFU) ![Star](https://img.shields.io/github/stars/BradyFU/Awesome-Multimodal-Large-Language-Models.svg?style=social&label=Star) · [mp.weixin.qq](https://mp.weixin.qq.com/s?__biz=MzAxMTk4NDkwNw==&mid=2247494157&idx=2&sn=350b94abcb3d1b2956ef015c51496b6f)]
- **Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena**, `arXiv, 2306.05685`, [arxiv](http://arxiv.org/abs/2306.05685v3), [pdf](http://arxiv.org/pdf/2306.05685v3.pdf), cication: [**136**](https://scholar.google.com/scholar?cites=2115026370478138399&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing* · [twitter](https://twitter.com/lmsysorg/status/1675612625273761793)] · [lmsys](https://lmsys.org/blog/2023-06-22-leaderboard/)]
- **Generative AI for Programming Education: Benchmarking ChatGPT, GPT-4,
  and Human Tutors**, `international journal of management, 2023`, [arxiv](http://arxiv.org/abs/2306.17156v3), [pdf](http://arxiv.org/pdf/2306.17156v3.pdf), cication: [**8**](https://scholar.google.com/scholar?cites=3129038780473031168&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Tung Phung, Victor-Alexandru Pădurean, José Cambronero, Sumit Gulwani, Tobias Kohn, Rupak Majumdar, Adish Singla, Gustavo Soares*
- **Benchmarking Large Language Model Capabilities for Conditional
  Generation**, `arXiv, 2306.16793`, [arxiv](http://arxiv.org/abs/2306.16793v1), [pdf](http://arxiv.org/pdf/2306.16793v1.pdf), cication: [**2**](https://scholar.google.com/scholar?cites=10717420948164970757&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Joshua Maynez, Priyanka Agrawal, Sebastian Gehrmann*
- **CMMLU: Measuring massive multitask language understanding in Chinese**, `arXiv, 2306.09212`, [arxiv](http://arxiv.org/abs/2306.09212v1), [pdf](http://arxiv.org/pdf/2306.09212v1.pdf), cication: [**14**](https://scholar.google.com/scholar?cites=7424799427308772526&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, Timothy Baldwin* · [jiqizhixin](https://www.jiqizhixin.com/articles/2023-06-29-3)] · ([CMMLU](https://github.com/haonan-li/CMMLU) - haonan-li) ![Star](https://img.shields.io/github/stars/haonan-li/CMMLU.svg?style=social&label=Star)

- **Bring Your Own Data! Self-Supervised Evaluation for Large Language
  Models**, `arXiv, 2306.13651`, [arxiv](http://arxiv.org/abs/2306.13651v2), [pdf](http://arxiv.org/pdf/2306.13651v2.pdf), cication: [**7**](https://scholar.google.com/scholar?cites=1188786261128674806&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Neel Jain, Khalid Saifullah, Yuxin Wen, John Kirchenbauer, Manli Shu, Aniruddha Saha, Micah Goldblum, Jonas Geiping, Tom Goldstein* · ([byod](https://github.com/neelsjain/byod) - neelsjain) ![Star](https://img.shields.io/github/stars/neelsjain/byod.svg?style=social&label=Star)
- **INSTRUCTEVAL: Towards Holistic Evaluation of Instruction-Tuned Large
  Language Models**, `arXiv, 2306.04757`, [arxiv](http://arxiv.org/abs/2306.04757v3), [pdf](http://arxiv.org/pdf/2306.04757v3.pdf), cication: [**19**](https://scholar.google.com/scholar?cites=3329388628137464648&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Yew Ken Chia, Pengfei Hong, Lidong Bing, Soujanya Poria* · [jiqizhixin](https://www.jiqizhixin.com/articles/2023-06-13-3)]
- **Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena**, `arXiv, 2306.05685`, [arxiv](http://arxiv.org/abs/2306.05685v3), [pdf](http://arxiv.org/pdf/2306.05685v3.pdf), cication: [**136**](https://scholar.google.com/scholar?cites=2115026370478138399&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing*

## Projects
- [**prometheus-eval**](https://github.com/prometheus-eval/prometheus-eval) - prometheus-eval ![Star](https://img.shields.io/github/stars/prometheus-eval/prometheus-eval.svg?style=social&label=Star)

	 *Evaluate your LLM's response with Prometheus 💯*
- [**LLM-Performance-Leaderboard**](https://huggingface.co/spaces/ArtificialAnalysis/LLM-Performance-Leaderboard) - ArtificialAnalysis 🤗

	 · ([huggingface](https://huggingface.co/blog/leaderboard-artificial-analysis))
- [Introducing the Open Chain of Thought Leaderboard](https://huggingface.co/blog/leaderboard-cot)
- [**arena-hard**](https://github.com/lm-sys/arena-hard) - lm-sys ![Star](https://img.shields.io/github/stars/lm-sys/arena-hard.svg?style=social&label=Star)

	 · ([lmsys](https://lmsys.org/blog/2024-04-19-arena-hard/)) · ([huggingface](https://huggingface.co/spaces/lmsys/arena-hard-browser))

	 · ([qbitai](https://www.qbitai.com/2024/04/136831.html))
- [**simple-evals**](https://github.com/openai/simple-evals) - openai ![Star](https://img.shields.io/github/stars/openai/simple-evals.svg?style=social&label=Star)
- [**LeaderboardFinder**](https://huggingface.co/spaces/leaderboards/LeaderboardFinder) - leaderboards 🤗
- [Introducing the Chatbot Guardrails Arena](https://huggingface.co/blog/arena-lighthouz)

	 · ([huggingface](https://huggingface.co/spaces/lighthouzai/guardrails-arena))
- [LiveCodeBench Leaderboard](https://livecodebench.github.io/leaderboard.html)
- [**WildBench**](https://huggingface.co/spaces/allenai/WildBench) - allenai 🤗
- [**gorilla**](https://github.com/ShishirPatil/gorilla) - ShishirPatil ![Star](https://img.shields.io/github/stars/ShishirPatil/gorilla.svg?style=social&label=Star)

	 *Gorilla: An API store for LLMs* · ([gorilla.cs.berkeley](https://gorilla.cs.berkeley.edu/leaderboard.html))
- [**URIAL-Bench**](https://huggingface.co/spaces/re-align/URIAL-Bench) - re-align 🤗
- [**NPHardEval-leaderboard**](https://huggingface.co/spaces/NPHardEval/NPHardEval-leaderboard) - NPHardEval 🤗

	 · ([huggingface](https://huggingface.co/blog/leaderboards-on-the-hub-nphardeval))
- [Introducing the Enterprise Scenarios Leaderboard: a Leaderboard for Real World Use Cases](https://huggingface.co/blog/leaderboards-on-the-hub-patronus)
- [**finetuning-subnet**](https://github.com/NousResearch/finetuning-subnet/tree/master) - NousResearch ![Star](https://img.shields.io/github/stars/NousResearch/finetuning-subnet.svg?style=social&label=Star)

	 · ([huggingface](https://huggingface.co/spaces/NousResearch/finetuning_subnet_leaderboard))
- [**llm_contamination_detector**](https://huggingface.co/spaces/Yeyito/llm_contamination_detector) - Yeyito 🤗
- [**detect-pretrain-code-contamination**](https://github.com/swj0419/detect-pretrain-code-contamination) - swj0419 ![Star](https://img.shields.io/github/stars/swj0419/detect-pretrain-code-contamination.svg?style=social&label=Star)
- [Holistic Evaluation of Language Models (HELM)](https://crfm.stanford.edu/helm/lite/v1.0.0/#/leaderboard)
- [**deepeval**](https://github.com/confident-ai/deepeval?utm_source=ai-drop.beehiiv.com&utm_medium=referral&utm_campaign=google-unveils-medlm-ai-models-for-health-care) - confident-ai ![Star](https://img.shields.io/github/stars/confident-ai/deepeval?utm_source=ai-drop.beehiiv.com&utm_medium=referral&utm_campaign=google-unveils-medlm-ai-models-for-health-care.svg?style=social&label=Star)

	 *The Evaluation Framework for LLMs*
- [**Nexus_Function_Calling_Leaderboard**](https://huggingface.co/spaces/Nexusflow/Nexus_Function_Calling_Leaderboard) - Nexusflow 🤗
- [Open LLM Leaderboard: DROP deep dive](https://huggingface.co/blog/leaderboard-drop-dive)
- [LLM Leaderboard best models ❤️‍🔥 - a open-llm-leaderboard Collection](https://huggingface.co/collections/open-llm-leaderboard/llm-leaderboard-best-models-652d6c7965a4619fb5c27a03)
- [**opencompass**](https://github.com/InternLM/opencompass) - InternLM ![Star](https://img.shields.io/github/stars/InternLM/opencompass.svg?style=social&label=Star)

	 *OpenCompass is an LLM evaluation platform, supporting a wide range of models (LLaMA, ChatGLM2, ChatGPT, Claude, etc) over 50+ datasets.* · [opencompass.org](https://opencompass.org.cn/MMBench)]
- [**FlagAI**](https://github.com/FlagAI-Open/FlagAI/tree/master/examples/Aquila) - FlagAI-Open ![Star](https://img.shields.io/github/stars/FlagAI-Open/FlagAI.svg?style=social&label=Star)

	 · [qbitai](https://www.qbitai.com/2023/06/64469.html)]
- [**toolqa**](https://github.com/night-chen/toolqa) - night-chen ![Star](https://img.shields.io/github/stars/night-chen/toolqa.svg?style=social&label=Star)

	 *ToolQA, a new dataset to evaluate the capabilities of LLMs in answering challenging questions with external tools. It offers two levels (easy/hard) across eight real-life scenarios.*
- [**glue-x**](https://github.com/yanglinyi/glue-x) - yanglinyi ![Star](https://img.shields.io/github/stars/yanglinyi/glue-x.svg?style=social&label=Star)

	 *We leverage 14 datasets as OOD test data and conduct evaluations on 8 NLU tasks over 21 popularly used models.*
- [**alpaca_farm**](https://github.com/tatsu-lab/alpaca_farm) - tatsu-lab ![Star](https://img.shields.io/github/stars/tatsu-lab/alpaca_farm.svg?style=social&label=Star)

	 *A Simulation Framework for RLHF and alternatives.* · [jiqizhixin](https://www.jiqizhixin.com/articles/2023-05-25-3)] · [mp.weixin.qq](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652341456&idx=4&sn=6be28e23b9e7435b76fe73920203b44d)]
-  [Open LLM Leaderboard - a Hugging Face Space by HuggingFaceH4](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)

	 · [huggingface](https://huggingface.co/blog/evaluating-mmlu-leaderboard)]

## Other
- [One of the most popular evaluation strategies is LLM-as-a-judge, which uses GPT-4 to evaluate model quality](https://twitter.com/cwolferesearch/status/1782453549223321660)
- [Evaluation & Hallucination Detection for Abstractive Summaries](https://eugeneyan.com/writing/abstractive/)
- [Measuring the Persuasiveness of Language Models](https://www.anthropic.com/news/measuring-model-persuasiveness)
- [Introducing the Red-Teaming Resistance Leaderboard](https://huggingface.co/blog/leaderboards-on-the-hub-haizelab)
- [Stanford CS25: V3 I Recipe for Training Helpful Chatbots - YouTube](https://youtu.be/mcep6W8oB1I?t=2069)
- [**yet-another-applied-llm-benchmark**](https://github.com/carlini/yet-another-applied-llm-benchmark/tree/main) - carlini ![Star](https://img.shields.io/github/stars/carlini/yet-another-applied-llm-benchmark.svg?style=social&label=Star)

	 · ([nicholas.carlini](https://nicholas.carlini.com/writing/2024/my-benchmark-for-large-language-models.html))
- [Hugging Face’s Philipp Schmid on Rethinking AI Evaluation](https://analyticsindiamag.com/hugging-faces-philipp-schmid-on-rethinking-ai-evaluation/)
- [Comparing the Performance of LLMs: A Deep Dive into Roberta, Llama 2, and Mistral for Disaster Tweets Analysis with Lora](https://huggingface.co/blog/Lora-for-sequence-classification-with-Roberta-Llama-Mistral)
- [Anthropic \\ Challenges in evaluating AI systems](https://www.anthropic.com/index/evaluating-ai-systems)

	 · [jiqizhixin](https://www.jiqizhixin.com/articles/2023-11-06-6)]
- [How Long Can Open-Source LLMs Truly Promise on Context Length? | LMSYS Org](https://lmsys.org/blog/2023-06-29-longchat/)

	 · [mp.weixin.qq](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652347271&idx=3&sn=48fe098e6789f50e25b26cd757898935)]
- [Chatbot Arena Leaderboard Week 8: Introducing MT-Bench and Vicuna-33B | LMSYS Org](https://lmsys.org/blog/2023-06-22-leaderboard/)

	 · [mp.weixin.qq](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652343523&idx=1&sn=c87f0d7de8ffedeb2f04c7df67b5245b)]

- [国内百模谁第一？清华14大LLM最新评测报告出炉，GLM-4、文心4.0站在第一梯队](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652468382&idx=2&sn=a377bc22b35f401e398dedb693f40a5c)
- [这里的测评，主打一个快狠准](https://mp.weixin.qq.com/s?__biz=MzAxMTk4NDkwNw==&mid=2247494977&idx=1&sn=e2f84e5dea26f3c9065b5b5298d39fca)
- [大模型知识&推理评估基准](https://mp.weixin.qq.com/s?__biz=MzAxMTk4NDkwNw==&mid=2247493959&idx=1&sn=6df3d2398a59d1bb92ddde1ed7a62040)
- [SuperCLUE中文大模型评测：商汤商量揽总榜、AI智能体两项第一 | 量子位](https://www.qbitai.com/2023/10/89814.html)

## Extra reference
- [**awesome-llms-evaluation-papers**](https://github.com/tjunlp-lab/awesome-llms-evaluation-papers) - tjunlp-lab ![Star](https://img.shields.io/github/stars/tjunlp-lab/awesome-llms-evaluation-papers.svg?style=social&label=Star)

	 *The papers are organized according to our survey: Evaluating Large Language Models: A Comprehensive Survey.* · [jiqizhixin](https://www.jiqizhixin.com/articles/2023-11-06)]
	