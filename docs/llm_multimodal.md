# LLM Multimodal

- [LLM Multimodal](#llm-multimodal) 
  - [Survey](#survey)
  - [Omni](#omni)
  - [Multimodal](#multimodal)
  - [Misc](#misc)


## Survey

- **Next Token Prediction Towards Multimodal Intelligence: A Comprehensive 
  Survey**, `arXiv, 2412.18619`, [arxiv](http://arxiv.org/abs/2412.18619v2), [pdf](http://arxiv.org/pdf/2412.18619v2.pdf), cication: [**-1**](None) 

	 *Liang Chen, Zekun Wang, Shuhuai Ren, ..., Tianyu Liu, Baobao Chang* · ([Awesome-Multimodal-Next-Token-Prediction](https://github.com/LMM101/Awesome-Multimodal-Next-Token-Prediction) - LMM101) ![Star](https://img.shields.io/github/stars/LMM101/Awesome-Multimodal-Next-Token-Prediction.svg?style=social&label=Star)

## Omni

- **From Specific-MLLM to Omni-MLLM: A Survey about the MLLMs alligned with 
  Multi-Modality**, `arXiv, 2412.11694`, [arxiv](http://arxiv.org/abs/2412.11694v1), [pdf](http://arxiv.org/pdf/2412.11694v1.pdf), cication: [**-1**](None) 

	 *Shixin Jiang, Jiafeng Liang, Ming Liu, ..., Bing Qin*
- [**Infini-Megrez**](https://github.com/infinigence/Infini-Megrez) - infinigence ![Star](https://img.shields.io/github/stars/infinigence/Infini-Megrez.svg?style=social&label=Star) 
- 🌟 **InternLM-XComposer2.5-OmniLive: A Comprehensive Multimodal System for 
  Long-term Streaming Video and Audio Interactions**, `arXiv, 2412.09596`, [arxiv](http://arxiv.org/abs/2412.09596v1), [pdf](http://arxiv.org/pdf/2412.09596v1.pdf), cication: [**-1**](None) 

	 *Pan Zhang, Xiaoyi Dong, Yuhang Cao, ..., Dahua Lin, Jiaqi Wang* · ([InternLM-XComposer](https://github.com/InternLM/InternLM-XComposer/tree/main/InternLM-XComposer-2.5-OmniLive) - InternLM) ![Star](https://img.shields.io/github/stars/InternLM/InternLM-XComposer.svg?style=social&label=Star) · ([huggingface](https://huggingface.co/internlm/internlm-xcomposer2d5-ol-7b))
- **Lyra: An Efficient and Speech-Centric Framework for Omni-Cognition**, `arXiv, 2412.09501`, [arxiv](http://arxiv.org/abs/2412.09501v1), [pdf](http://arxiv.org/pdf/2412.09501v1.pdf), cication: [**-1**](None) 

	 *Zhisheng Zhong, Chengyao Wang, Yuqi Liu, ..., Shu Liu, Jiaya Jia* · ([lyra-omni.github](https://lyra-omni.github.io/)) · ([103.170.5](https://103.170.5.190:17860/)) · ([Lyra](https://github.com/dvlab-research/Lyra) - dvlab-research) ![Star](https://img.shields.io/github/stars/dvlab-research/Lyra.svg?style=social&label=Star) · ([huggingface](https://huggingface.co/collections/zszhong/lyra-model-674ea5bb3b39ff8f15de75fc))
- **OmniFlow: Any-to-Any Generation with Multi-Modal Rectified Flows**, `arXiv, 2412.01169`, [arxiv](http://arxiv.org/abs/2412.01169v1), [pdf](http://arxiv.org/pdf/2412.01169v1.pdf), cication: [**-1**](None) 

	 *Shufan Li, Konstantinos Kallidromitis, Akash Gokul, ..., Kazuki Kozuka, Aditya Grover* · ([OmniFlows](https://github.com/jacklishufan/OmniFlows) - jacklishufan) ![Star](https://img.shields.io/github/stars/jacklishufan/OmniFlows.svg?style=social&label=Star)
- [**AnyModal**](https://github.com/ritabratamaiti/AnyModal) - ritabratamaiti ![Star](https://img.shields.io/github/stars/ritabratamaiti/AnyModal.svg?style=social&label=Star) 

	 *A Flexible Multimodal Language Model Framework*
- **Everything is a Video: Unifying Modalities through Next-Frame Prediction**, `arXiv, 2411.10503`, [arxiv](http://arxiv.org/abs/2411.10503v1), [pdf](http://arxiv.org/pdf/2411.10503v1.pdf), cication: [**-1**](None) 

	 *G. Thomas Hudson, Dean Slack, Thomas Winterbottom, ..., Junjie Shentu, Noura Al Moubayed*
- **WavChat: A Survey of Spoken Dialogue Models**, `arXiv, 2411.13577`, [arxiv](http://arxiv.org/abs/2411.13577v1), [pdf](http://arxiv.org/pdf/2411.13577v1.pdf), cication: [**-1**](None) 

	 *Shengpeng Ji, Yifu Chen, Minghui Fang, ..., Jin Xu, Zhou Zhao*
- **GPT-4o System Card**, `arXiv, 2410.21276`, [arxiv](http://arxiv.org/abs/2410.21276v1), [pdf](http://arxiv.org/pdf/2410.21276v1.pdf), cication: [**-1**](None) 

	 *OpenAI, :, Aaron Hurst, ..., Yunxing Dai, Yury Malkov*
- **Ichigo: Mixed-Modal Early-Fusion Realtime Voice Assistant**, `arXiv, 2410.15316`, [arxiv](http://arxiv.org/abs/2410.15316v1), [pdf](http://arxiv.org/pdf/2410.15316v1.pdf), cication: [**-1**](None) 

	 *Alan Dao, Dinh Bach Vu, Huy Hoang Ha* · ([ichigo.homebrew](https://ichigo.homebrew.ltd/)) · ([homebrew](https://homebrew.ltd/)) · ([ichigo](https://github.com/homebrewltd/ichigo) - homebrewltd) ![Star](https://img.shields.io/github/stars/homebrewltd/ichigo.svg?style=social&label=Star)
- **Mini-Omni2: Towards Open-source GPT-4o with Vision, Speech and Duplex 
  Capabilities**, `arXiv, 2410.11190`, [arxiv](http://arxiv.org/abs/2410.11190v2), [pdf](http://arxiv.org/pdf/2410.11190v2.pdf), cication: [**1**](https://scholar.google.com/scholar?cites=14534896134025731094&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Zhifei Xie, Changqiao Wu* · ([mini-omni2](https://github.com/gpt-omni/mini-omni2) - gpt-omni) ![Star](https://img.shields.io/github/stars/gpt-omni/mini-omni2.svg?style=social&label=Star)
- **MixEval-X: Any-to-Any Evaluations from Real-World Data Mixtures**, `arXiv, 2410.13754`, [arxiv](http://arxiv.org/abs/2410.13754v2), [pdf](http://arxiv.org/pdf/2410.13754v2.pdf), cication: [**-1**](None) 

	 *Jinjie Ni, Yifan Song, Deepanway Ghosal, ..., Yang You, Michael Shieh*
- **OMCAT: Omni Context Aware Transformer**, `arXiv, 2410.12109`, [arxiv](http://arxiv.org/abs/2410.12109v1), [pdf](http://arxiv.org/pdf/2410.12109v1.pdf), cication: [**-1**](None) 

	 *Arushi Goel, Karan Sapra, Matthieu Le, ..., Andrew Tao, Bryan Catanzaro* · ([om-cat.github](https://om-cat.github.io/))

## Multimodal

- **OpenOmni: Large Language Models Pivot Zero-shot Omnimodal Alignment 
  across Language with Real-time Self-Aware Emotional Speech Synthesis**, `arXiv, 2501.04561`, [arxiv](http://arxiv.org/abs/2501.04561v2), [pdf](http://arxiv.org/pdf/2501.04561v2.pdf), cication: [**-1**](None) 

	 *Run Luo, Ting-En Lin, Haonan Zhang, ..., Hamid Alinejad-Rokny, Fei Huang*
- **VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction**, `arXiv, 2501.01957`, [arxiv](http://arxiv.org/abs/2501.01957v1), [pdf](http://arxiv.org/pdf/2501.01957v1.pdf), cication: [**-1**](None) 

	 *Chaoyou Fu, Haojia Lin, Xiong Wang, ..., Caifeng Shan, Ran He* · ([VITA](https://github.com/VITA-MLLM/VITA) - VITA-MLLM) ![Star](https://img.shields.io/github/stars/VITA-MLLM/VITA.svg?style=social&label=Star)
- **3DGraphLLM: Combining Semantic Graphs and Large Language Models for 3D 
  Scene Understanding**, `arXiv, 2412.18450`, [arxiv](http://arxiv.org/abs/2412.18450v1), [pdf](http://arxiv.org/pdf/2412.18450v1.pdf), cication: [**-1**](None) 

	 *Tatiana Zemskova, Dmitry Yudin* · ([3DGraphLLM](https://github.com/CognitiveAISystems/3DGraphLLM) - CognitiveAISystems) ![Star](https://img.shields.io/github/stars/CognitiveAISystems/3DGraphLLM.svg?style=social&label=Star)
- 🌟 **Mixture-of-Transformers: A Sparse and Scalable Architecture for 
  Multi-Modal Foundation Models**, `arXiv, 2411.04996`, [arxiv](http://arxiv.org/abs/2411.04996v1), [pdf](http://arxiv.org/pdf/2411.04996v1.pdf), cication: [**-1**](None) 

	 *Weixin Liang, Lili Yu, Liang Luo, ..., Luke Zettlemoyer, Xi Victoria Lin*
- **DPLM-2: A Multimodal Diffusion Protein Language Model**, `arXiv, 2410.13782`, [arxiv](http://arxiv.org/abs/2410.13782v1), [pdf](http://arxiv.org/pdf/2410.13782v1.pdf), cication: [**-1**](None) 

	 *Xinyou Wang, Zaixiang Zheng, Fei Ye, ..., Shujian Huang, Quanquan Gu* · ([bytedance.github](https://bytedance.github.io/dplm/dplm-2)) · ([arxiv](https://arxiv.org/abs/2410.13782))

## Misc