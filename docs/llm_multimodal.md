# LLM Multimodal

- [LLM Multimodal](#llm-multimodal) 
  - [Survey](#survey)
  - [Omni](#omni)
  - [Multimodal](#multimodal)
  - [Misc](#misc)


## Survey

- **Next Token Prediction Towards Multimodal Intelligence: A Comprehensive 
  Survey**, `arXiv, 2412.18619`, [arxiv](http://arxiv.org/abs/2412.18619v2), [pdf](http://arxiv.org/pdf/2412.18619v2.pdf), cication: [**-1**](None) 

	 *Liang Chen, Zekun Wang, Shuhuai Ren, ..., Tianyu Liu, Baobao Chang* 路 ([Awesome-Multimodal-Next-Token-Prediction](https://github.com/LMM101/Awesome-Multimodal-Next-Token-Prediction) - LMM101) ![Star](https://img.shields.io/github/stars/LMM101/Awesome-Multimodal-Next-Token-Prediction.svg?style=social&label=Star)

## Omni

- **From Specific-MLLM to Omni-MLLM: A Survey about the MLLMs alligned with 
  Multi-Modality**, `arXiv, 2412.11694`, [arxiv](http://arxiv.org/abs/2412.11694v1), [pdf](http://arxiv.org/pdf/2412.11694v1.pdf), cication: [**-1**](None) 

	 *Shixin Jiang, Jiafeng Liang, Ming Liu, ..., Bing Qin*
- [**Infini-Megrez**](https://github.com/infinigence/Infini-Megrez) - infinigence ![Star](https://img.shields.io/github/stars/infinigence/Infini-Megrez.svg?style=social&label=Star) 
-  **InternLM-XComposer2.5-OmniLive: A Comprehensive Multimodal System for 
  Long-term Streaming Video and Audio Interactions**, `arXiv, 2412.09596`, [arxiv](http://arxiv.org/abs/2412.09596v1), [pdf](http://arxiv.org/pdf/2412.09596v1.pdf), cication: [**-1**](None) 

	 *Pan Zhang, Xiaoyi Dong, Yuhang Cao, ..., Dahua Lin, Jiaqi Wang* 路 ([InternLM-XComposer](https://github.com/InternLM/InternLM-XComposer/tree/main/InternLM-XComposer-2.5-OmniLive) - InternLM) ![Star](https://img.shields.io/github/stars/InternLM/InternLM-XComposer.svg?style=social&label=Star) 路 ([huggingface](https://huggingface.co/internlm/internlm-xcomposer2d5-ol-7b))
- **Lyra: An Efficient and Speech-Centric Framework for Omni-Cognition**, `arXiv, 2412.09501`, [arxiv](http://arxiv.org/abs/2412.09501v1), [pdf](http://arxiv.org/pdf/2412.09501v1.pdf), cication: [**-1**](None) 

	 *Zhisheng Zhong, Chengyao Wang, Yuqi Liu, ..., Shu Liu, Jiaya Jia* 路 ([lyra-omni.github](https://lyra-omni.github.io/)) 路 ([103.170.5](https://103.170.5.190:17860/)) 路 ([Lyra](https://github.com/dvlab-research/Lyra) - dvlab-research) ![Star](https://img.shields.io/github/stars/dvlab-research/Lyra.svg?style=social&label=Star) 路 ([huggingface](https://huggingface.co/collections/zszhong/lyra-model-674ea5bb3b39ff8f15de75fc))
- **OmniFlow: Any-to-Any Generation with Multi-Modal Rectified Flows**, `arXiv, 2412.01169`, [arxiv](http://arxiv.org/abs/2412.01169v1), [pdf](http://arxiv.org/pdf/2412.01169v1.pdf), cication: [**-1**](None) 

	 *Shufan Li, Konstantinos Kallidromitis, Akash Gokul, ..., Kazuki Kozuka, Aditya Grover* 路 ([OmniFlows](https://github.com/jacklishufan/OmniFlows) - jacklishufan) ![Star](https://img.shields.io/github/stars/jacklishufan/OmniFlows.svg?style=social&label=Star)
- [**AnyModal**](https://github.com/ritabratamaiti/AnyModal) - ritabratamaiti ![Star](https://img.shields.io/github/stars/ritabratamaiti/AnyModal.svg?style=social&label=Star) 

	 *A Flexible Multimodal Language Model Framework*
- **Everything is a Video: Unifying Modalities through Next-Frame Prediction**, `arXiv, 2411.10503`, [arxiv](http://arxiv.org/abs/2411.10503v1), [pdf](http://arxiv.org/pdf/2411.10503v1.pdf), cication: [**-1**](None) 

	 *G. Thomas Hudson, Dean Slack, Thomas Winterbottom, ..., Junjie Shentu, Noura Al Moubayed*
- **WavChat: A Survey of Spoken Dialogue Models**, `arXiv, 2411.13577`, [arxiv](http://arxiv.org/abs/2411.13577v1), [pdf](http://arxiv.org/pdf/2411.13577v1.pdf), cication: [**-1**](None) 

	 *Shengpeng Ji, Yifu Chen, Minghui Fang, ..., Jin Xu, Zhou Zhao*
- **GPT-4o System Card**, `arXiv, 2410.21276`, [arxiv](http://arxiv.org/abs/2410.21276v1), [pdf](http://arxiv.org/pdf/2410.21276v1.pdf), cication: [**-1**](None) 

	 *OpenAI, :, Aaron Hurst, ..., Yunxing Dai, Yury Malkov*
- **Ichigo: Mixed-Modal Early-Fusion Realtime Voice Assistant**, `arXiv, 2410.15316`, [arxiv](http://arxiv.org/abs/2410.15316v1), [pdf](http://arxiv.org/pdf/2410.15316v1.pdf), cication: [**-1**](None) 

	 *Alan Dao, Dinh Bach Vu, Huy Hoang Ha* 路 ([ichigo.homebrew](https://ichigo.homebrew.ltd/)) 路 ([homebrew](https://homebrew.ltd/)) 路 ([ichigo](https://github.com/homebrewltd/ichigo) - homebrewltd) ![Star](https://img.shields.io/github/stars/homebrewltd/ichigo.svg?style=social&label=Star)
- **Mini-Omni2: Towards Open-source GPT-4o with Vision, Speech and Duplex 
  Capabilities**, `arXiv, 2410.11190`, [arxiv](http://arxiv.org/abs/2410.11190v2), [pdf](http://arxiv.org/pdf/2410.11190v2.pdf), cication: [**1**](https://scholar.google.com/scholar?cites=14534896134025731094&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Zhifei Xie, Changqiao Wu* 路 ([mini-omni2](https://github.com/gpt-omni/mini-omni2) - gpt-omni) ![Star](https://img.shields.io/github/stars/gpt-omni/mini-omni2.svg?style=social&label=Star)
- **MixEval-X: Any-to-Any Evaluations from Real-World Data Mixtures**, `arXiv, 2410.13754`, [arxiv](http://arxiv.org/abs/2410.13754v2), [pdf](http://arxiv.org/pdf/2410.13754v2.pdf), cication: [**-1**](None) 

	 *Jinjie Ni, Yifan Song, Deepanway Ghosal, ..., Yang You, Michael Shieh*
- **OMCAT: Omni Context Aware Transformer**, `arXiv, 2410.12109`, [arxiv](http://arxiv.org/abs/2410.12109v1), [pdf](http://arxiv.org/pdf/2410.12109v1.pdf), cication: [**-1**](None) 

	 *Arushi Goel, Karan Sapra, Matthieu Le, ..., Andrew Tao, Bryan Catanzaro* 路 ([om-cat.github](https://om-cat.github.io/))

## Multimodal

- **Kimi-Audio Technical Report**, `arXiv, 2504.18425`, [arxiv](http://arxiv.org/abs/2504.18425v1), [pdf](http://arxiv.org/pdf/2504.18425v1.pdf), cication: [**-1**](None) 

	 *KimiTeam, Ding Ding, Zeqian Ju, ..., Yutong Zhang, Zaida Zhou* 路 ([Kimi-Audio.](https://github.com/MoonshotAI/Kimi-Audio.) - MoonshotAI) ![Star](https://img.shields.io/github/stars/MoonshotAI/Kimi-Audio..svg?style=social&label=Star)
- **Have we unified image generation and understanding yet? An empirical 
  study of GPT-4o's image generation ability**, `arXiv, 2504.08003`, [arxiv](http://arxiv.org/abs/2504.08003v1), [pdf](http://arxiv.org/pdf/2504.08003v1.pdf), cication: [**-1**](None) 

	 *Ning Li, Jingran Zhang, Justin Cui*
-  **Qwen2.5-Omni Technical Report**, `arXiv, 2503.20215`, [arxiv](http://arxiv.org/abs/2503.20215v1), [pdf](http://arxiv.org/pdf/2503.20215v1.pdf), cication: [**-1**](None) 

	 *Jin Xu, Zhifang Guo, Jinzheng He, ..., Yunfei Chu, Junyang Lin* 路 ([chat.qwenlm](https://chat.qwenlm.ai)) 路 ([qwenlm.github](https://qwenlm.github.io/blog/qwen2.5-omni/)) 路 ([Qwen2.5-Omni](https://github.com/QwenLM/Qwen2.5-Omni) - QwenLM) ![Star](https://img.shields.io/github/stars/QwenLM/Qwen2.5-Omni.svg?style=social&label=Star) 路 ([huggingface](https://huggingface.co/Qwen/Qwen2.5-Omni-7B)) 路 ([huggingface](https://huggingface.co/spaces/Qwen/Qwen2.5-Omni-7B-Demo)) 路 ([modelscope](https://modelscope.cn/models/Qwen/Qwen2.5-Omni-7B)) 路 ([help.aliyun](https://help.aliyun.com/zh/model-studio/user-guide/qwen-omni)) 路 ([modelscope](https://modelscope.cn/studios/Qwen/Qwen2.5-Omni-Demo))
- **Vision-Speech Models: Teaching Speech Models to Converse about Images**, `arXiv, 2503.15633`, [arxiv](http://arxiv.org/abs/2503.15633v1), [pdf](http://arxiv.org/pdf/2503.15633v1.pdf), cication: [**-1**](None) 

	 *Am茅lie Royer, Moritz B枚hle, Gabriel de Marmiesse, ..., Alexandre D茅fossez, Patrick P茅rez* 路 ([kyutai](https://kyutai.org/moshivis)) 路 ([moshivis](https://github.com/kyutai-labs/moshivis) - kyutai-labs) ![Star](https://img.shields.io/github/stars/kyutai-labs/moshivis.svg?style=social&label=Star)
- [**SpatialLM**](https://github.com/manycore-research/SpatialLM) - manycore-research ![Star](https://img.shields.io/github/stars/manycore-research/SpatialLM.svg?style=social&label=Star) 
-  [Phi-4-multimodal-instruct is a lightweight open multimodal foundationmodel that leverages the language, vision, and speech research](https://huggingface.co/microsoft/Phi-4-multimodal-instruct)   

	 路 ([models](https://github.com/marketplace/models/azureml/Phi-4-multimodal-instruct/playground) - marketplace) ![Star](https://img.shields.io/github/stars/marketplace/models.svg?style=social&label=Star)
- **video-SALMONN-o1: Reasoning-enhanced Audio-visual Large Language Model**, `arXiv, 2502.11775`, [arxiv](http://arxiv.org/abs/2502.11775v1), [pdf](http://arxiv.org/pdf/2502.11775v1.pdf), cication: [**-1**](None) 

	 *Guangzhi Sun, Yudong Yang, Jimin Zhuang, ..., Zejun MA, Chao Zhang* 路 ([video-SALMONN-o1](https://github.com/BriansIDP/video-SALMONN-o1) - BriansIDP) ![Star](https://img.shields.io/github/stars/BriansIDP/video-SALMONN-o1.svg?style=social&label=Star)
- **Exploring the Potential of Encoder-free Architectures in 3D LMMs**, `arXiv, 2502.09620`, [arxiv](http://arxiv.org/abs/2502.09620v1), [pdf](http://arxiv.org/pdf/2502.09620v1.pdf), cication: [**-1**](None) 

	 *Yiwen Tang, Zoey Guo, Zhuhao Wang, ..., Xuelong Li, Bin Zhao* 路 ([ENEL](https://github.com/Ivan-Tang-3D/ENEL) - Ivan-Tang-3D) ![Star](https://img.shields.io/github/stars/Ivan-Tang-3D/ENEL.svg?style=social&label=Star)
- **Baichuan-Omni-1.5 Technical Report**, `arXiv, 2501.15368`, [arxiv](http://arxiv.org/abs/2501.15368v1), [pdf](http://arxiv.org/pdf/2501.15368v1.pdf), cication: [**-1**](None) 

	 *Yadong Li, Jun Liu, Tao Zhang, ..., Zenan Zhou, Weipeng Chen* 路 ([Baichuan-Omni-1.5](https://github.com/baichuan-inc/Baichuan-Omni-1.5) - baichuan-inc) ![Star](https://img.shields.io/github/stars/baichuan-inc/Baichuan-Omni-1.5.svg?style=social&label=Star)
- **Ola: Pushing the Frontiers of Omni-Modal Language Model with Progressive 
  Modality Alignment**, `arXiv, 2502.04328`, [arxiv](http://arxiv.org/abs/2502.04328v2), [pdf](http://arxiv.org/pdf/2502.04328v2.pdf), cication: [**-1**](None) 

	 *Zuyan Liu, Yuhao Dong, Jiahui Wang, ..., Jiwen Lu, Yongming Rao* 路 ([ola-omni.github](https://ola-omni.github.io/)) 路 ([Ola](https://github.com/Ola-Omni/Ola) - Ola-Omni) ![Star](https://img.shields.io/github/stars/Ola-Omni/Ola.svg?style=social&label=Star) 路 ([huggingface](https://huggingface.co/THUdyh/Ola-7b))
-  **MinMo: A Multimodal Large Language Model for Seamless Voice Interaction**, `arXiv, 2501.06282`, [arxiv](http://arxiv.org/abs/2501.06282v1), [pdf](http://arxiv.org/pdf/2501.06282v1.pdf), cication: [**-1**](None) 

	 *Qian Chen, Yafeng Chen, Yanni Chen, ..., Chong Zhang, Jinren Zhou* 路 ([funaudiollm.github](https://funaudiollm.github.io/minmo/))
-  [**MiniCPM-o**](https://github.com/OpenBMB/MiniCPM-o) - OpenBMB ![Star](https://img.shields.io/github/stars/OpenBMB/MiniCPM-o.svg?style=social&label=Star) 
- [A GPT-4o Level MLLM for Vision, Speech and Multimodal Live Streaming on Your Phone](https://huggingface.co/openbmb/MiniCPM-o-2_6)   
- **OpenOmni: Large Language Models Pivot Zero-shot Omnimodal Alignment 
  across Language with Real-time Self-Aware Emotional Speech Synthesis**, `arXiv, 2501.04561`, [arxiv](http://arxiv.org/abs/2501.04561v2), [pdf](http://arxiv.org/pdf/2501.04561v2.pdf), cication: [**-1**](None) 

	 *Run Luo, Ting-En Lin, Haonan Zhang, ..., Hamid Alinejad-Rokny, Fei Huang*
- **VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction**, `arXiv, 2501.01957`, [arxiv](http://arxiv.org/abs/2501.01957v1), [pdf](http://arxiv.org/pdf/2501.01957v1.pdf), cication: [**-1**](None) 

	 *Chaoyou Fu, Haojia Lin, Xiong Wang, ..., Caifeng Shan, Ran He* 路 ([VITA](https://github.com/VITA-MLLM/VITA) - VITA-MLLM) ![Star](https://img.shields.io/github/stars/VITA-MLLM/VITA.svg?style=social&label=Star)
- **3DGraphLLM: Combining Semantic Graphs and Large Language Models for 3D 
  Scene Understanding**, `arXiv, 2412.18450`, [arxiv](http://arxiv.org/abs/2412.18450v1), [pdf](http://arxiv.org/pdf/2412.18450v1.pdf), cication: [**-1**](None) 

	 *Tatiana Zemskova, Dmitry Yudin* 路 ([3DGraphLLM](https://github.com/CognitiveAISystems/3DGraphLLM) - CognitiveAISystems) ![Star](https://img.shields.io/github/stars/CognitiveAISystems/3DGraphLLM.svg?style=social&label=Star)
-  **Mixture-of-Transformers: A Sparse and Scalable Architecture for 
  Multi-Modal Foundation Models**, `arXiv, 2411.04996`, [arxiv](http://arxiv.org/abs/2411.04996v1), [pdf](http://arxiv.org/pdf/2411.04996v1.pdf), cication: [**-1**](None) 

	 *Weixin Liang, Lili Yu, Liang Luo, ..., Luke Zettlemoyer, Xi Victoria Lin*
- **DPLM-2: A Multimodal Diffusion Protein Language Model**, `arXiv, 2410.13782`, [arxiv](http://arxiv.org/abs/2410.13782v1), [pdf](http://arxiv.org/pdf/2410.13782v1.pdf), cication: [**-1**](None) 

	 *Xinyou Wang, Zaixiang Zheng, Fei Ye, ..., Shujian Huang, Quanquan Gu* 路 ([bytedance.github](https://bytedance.github.io/dplm/dplm-2)) 路 ([arxiv](https://arxiv.org/abs/2410.13782))

## Misc