# LLM Multimodal

- [LLM Multimodal](#llm-multimodal)
  - [Survey](#survey)
  - [Omni](#omni)
  - [Multimodal](#multimodal)
  - [Misc](#misc)


## Survey


## Omni

- [Paper page - Ichigo: Mixed-Modal Early-Fusion Realtime Voice Assistant](https://huggingface.co/papers/2410.15316)
   - [ichigo.homebrew.ltd](https://ichigo.homebrew.ltd/)
   - [homebrew.ltd](https://homebrew.ltd/)
   - [github.com](https://github.com/homebrewltd/ichigo)
- [Paper page - Mini-Omni2: Towards Open-source GPT-4o with Vision, Speech and Duplex  Capabilities](https://huggingface.co/papers/2410.11190)
   - [github.com](https://github.com/gpt-omni/mini-omni2)
- [Paper page - MixEval-X: Any-to-Any Evaluations from Real-World Data Mixtures](https://huggingface.co/papers/2410.13754)
- [Paper page - OMCAT: Omni Context Aware Transformer](https://huggingface.co/papers/2410.12109)
   - [om-cat.github.io](https://om-cat.github.io/)

## Multimodal

- [Paper page - DPLM-2: A Multimodal Diffusion Protein Language Model](https://huggingface.co/papers/2410.13782)
   - [bytedance.github.io](https://bytedance.github.io/dplm/dplm-2)
   - [arxiv.org](https://arxiv.org/abs/2410.13782)

## Misc


