# LLM Security

- [LLM Security](#llm-security) 
  - [Survey](#survey)
  - [LLM Security](#llm-security)
  - [Red Team](#red-team)
  - [Projects](#projects)
  - [Misc](#misc)


## Survey


## LLM Security

- [How to Backdoor Large Language Models](https://blog.sshh.io/p/how-to-backdoor-large-language-models) 
- **Early External Safety Testing of OpenAI's o3-mini: Insights from the
  Pre-Deployment Evaluation**, `arXiv, 2501.17749`, [arxiv](http://arxiv.org/abs/2501.17749v1), [pdf](http://arxiv.org/pdf/2501.17749v1.pdf), cication: [**-1**](None) 

	 *Aitor Arrieta, Miriam Ugarte, Pablo Valle, ..., Jos茅 Antonio Parejo, Sergio Segura*
-  **GuardReasoner: Towards Reasoning-based LLM Safeguards**, `arXiv, 2501.18492`, [arxiv](http://arxiv.org/abs/2501.18492v1), [pdf](http://arxiv.org/pdf/2501.18492v1.pdf), cication: [**-1**](None) 

	 *Yue Liu, Hongcheng Gao, Shengfang Zhai, ..., Jiaheng Zhang, Bryan Hooi* 路 ([GuardReasoner](https://github.com/yueliu1999/GuardReasoner/) - yueliu1999) ![Star](https://img.shields.io/github/stars/yueliu1999/GuardReasoner.svg?style=social&label=Star)
- **o3-mini vs DeepSeek-R1: Which One is Safer?**, `arXiv, 2501.18438`, [arxiv](http://arxiv.org/abs/2501.18438v2), [pdf](http://arxiv.org/pdf/2501.18438v2.pdf), cication: [**-1**](None) 

	 *Aitor Arrieta, Miriam Ugarte, Pablo Valle, ..., Jos茅 Antonio Parejo, Sergio Segura*
- **SafeRAG: Benchmarking Security in Retrieval-Augmented Generation of
  Large Language Model**, `arXiv, 2501.18636`, [arxiv](http://arxiv.org/abs/2501.18636v1), [pdf](http://arxiv.org/pdf/2501.18636v1.pdf), cication: [**-1**](None) 

	 *Xun Liang, Simin Niu, Zhiyu Li, ..., Mengwei Wang, Jiawei Yang* 路 ([SafeRAG](https://github.com/IAAR-Shanghai/SafeRAG) - IAAR-Shanghai) ![Star](https://img.shields.io/github/stars/IAAR-Shanghai/SafeRAG.svg?style=social&label=Star)
- [J2: Jailbreaking to Jailbreak](https://scale.com/research/j2) 

	 路 ([](https://x.com/_zifan_wang/status/1889353245761638512))
- [first-ever International AI Safety Report](https://x.com/Yoshua_Bengio/status/1884593469265502482)   

	 路 ([assets.publishing.service.gov](https://assets.publishing.service.gov.uk/media/679a0c48a77d250007d313ee/International_AI_Safety_Report_2025_accessible_f.pdf))
- [Constitutional Classifiers: Defending against universal jailbreaks](https://www.anthropic.com/research/constitutional-classifiers) 

	 路 ([](https://x.com/AnthropicAI/status/1886452489681023333))
- [Updating the Frontier Safety Framework](https://deepmind.google/discover/blog/updating-the-frontier-safety-framework/) 
- **Constitutional Classifiers: Defending against Universal Jailbreaks 
  across Thousands of Hours of Red Teaming**, `arXiv, 2501.18837`, [arxiv](http://arxiv.org/abs/2501.18837v1), [pdf](http://arxiv.org/pdf/2501.18837v1.pdf), cication: [**-1**](None) 

	 *Mrinank Sharma, Meg Tong, Jesse Mu, ..., Jared Kaplan, Ethan Perez*
- [Trading Inference-Time Compute for Adversarial Robustness](https://openai.com/index/trading-inference-time-compute-for-adversarial-robustness/) 

	 路 ([cdn.openai](https://cdn.openai.com/papers/trading-inference-time-compute-for-adversarial-robustness-20250121_1.pdf))
- **Lessons From Red Teaming 100 Generative AI Products**, `arXiv, 2501.07238`, [arxiv](http://arxiv.org/abs/2501.07238v1), [pdf](http://arxiv.org/pdf/2501.07238v1.pdf), cication: [**-1**](None) 

	 *Blake Bullwinkel, Amanda Minnich, Shiven Chawla, ..., Chang Kawaguchi, Mark Russinovich*
- **Auto-RT: Automatic Jailbreak Strategy Exploration for Red-Teaming Large 
  Language Models**, `arXiv, 2501.01830`, [arxiv](http://arxiv.org/abs/2501.01830v1), [pdf](http://arxiv.org/pdf/2501.01830v1.pdf), cication: [**-1**](None) 

	 *Yanjiang Liu, Shuhen Zhou, Yaojie Lu, ..., Xianpei Han, Le Sun*
- **Safeguard Fine-Tuned LLMs Through Pre- and Post-Tuning Model Merging**, `arXiv, 2412.19512`, [arxiv](http://arxiv.org/abs/2412.19512v1), [pdf](http://arxiv.org/pdf/2412.19512v1.pdf), cication: [**-1**](None) 

	 *Hua Farn, Hsuan Su, Shachi H Kumar, ..., Shang-Tse Chen, Hung-yi Lee*
- [Alignment faking in large language models](https://www.anthropic.com/research/alignment-faking) 

	 路 ([assets.anthropic](https://assets.anthropic.com/m/983c85a201a962f/original/Alignment-Faking-in-Large-Language-Models-full-paper.pdf))
- [Deliberative alignment: reasoning enables safer language models](https://openai.com/index/deliberative-alignment/) 
- [Best-of-N Jailbreaking](https://jplhughes.github.io/bon-jailbreaking/) 

	 路 ([](https://x.com/AnthropicAI/status/1867608917595107443))
- **Granite Guardian**, `arXiv, 2412.07724`, [arxiv](http://arxiv.org/abs/2412.07724v1), [pdf](http://arxiv.org/pdf/2412.07724v1.pdf), cication: [**-1**](None) 

	 *Inkit Padhi, Manish Nagireddy, Giandomenico Cornacchia, ..., Kush R. Varshney, Prasanna Sattigeri* 路 ([granite-guardian](https://github.com/ibm-granite/granite-guardian) - ibm-granite) ![Star](https://img.shields.io/github/stars/ibm-granite/granite-guardian.svg?style=social&label=Star)
- **Llama Guard 3 Vision: Safeguarding Human-AI Image Understanding 
  Conversations**, `arXiv, 2411.10414`, [arxiv](http://arxiv.org/abs/2411.10414v1), [pdf](http://arxiv.org/pdf/2411.10414v1.pdf), cication: [**-1**](None) 

	 *Jianfeng Chi, Ujjwal Karn, Hongyuan Zhan, ..., Kartikeya Upasani, Mahesh Pasupuleti* 路 ([llama](https://www.llama.com/trust-and-safety/)) 路 ([llama-recipes](https://github.com/meta-llama/llama-recipes/tree/main/recipes/responsible_ai/llama_guard) - meta-llama) ![Star](https://img.shields.io/github/stars/meta-llama/llama-recipes.svg?style=social&label=Star)
- **Building Trust: Foundations of Security, Safety and Transparency in AI**, `arXiv, 2411.12275`, [arxiv](http://arxiv.org/abs/2411.12275v1), [pdf](http://arxiv.org/pdf/2411.12275v1.pdf), cication: [**-1**](None) 

	 *Huzaifa Sidhpurwala, Garth Mollett, Emily Fox, ..., Mark Bestavros, Huamin Chen*
- **Rapid Response: Mitigating LLM Jailbreaks with a Few Examples**, `arXiv, 2411.07494`, [arxiv](http://arxiv.org/abs/2411.07494v1), [pdf](http://arxiv.org/pdf/2411.07494v1.pdf), cication: [**-1**](None) 

	 *Alwin Peng, Julian Michael, Henry Sleight, ..., Ethan Perez, Mrinank Sharma* 路 ([](https://x.com/AnthropicAI/status/1856752093945540673)) 路 ([rapidresponsebench](https://github.com/rapidresponsebench/rapidresponsebench) - rapidresponsebench) ![Star](https://img.shields.io/github/stars/rapidresponsebench/rapidresponsebench.svg?style=social&label=Star)
- **AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents**, `arXiv, 2410.09024`, [arxiv](http://arxiv.org/abs/2410.09024v2), [pdf](http://arxiv.org/pdf/2410.09024v2.pdf), cication: [**2**](https://scholar.google.com/scholar?cites=2401564792328774425&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII) 

	 *Maksym Andriushchenko, Alexandra Souly, Mateusz Dziemian, ..., Yarin Gal, Xander Davies*

## Red Team

- [Advancing red teaming with people and AI](https://openai.com/index/advancing-red-teaming-with-people-and-ai/) 

## Projects

- [**Awesome-LLM-Safety-Papers**](https://github.com/tjunlp-lab/Awesome-LLM-Safety-Papers) - tjunlp-lab ![Star](https://img.shields.io/github/stars/tjunlp-lab/Awesome-LLM-Safety-Papers.svg?style=social&label=Star) 
- [**PromptJailbreakManual**](https://github.com/Acmesec/PromptJailbreakManual) - Acmesec ![Star](https://img.shields.io/github/stars/Acmesec/PromptJailbreakManual.svg?style=social&label=Star) 
- [**CS-Eval**](https://github.com/CS-EVAL/CS-Eval) - CS-EVAL ![Star](https://img.shields.io/github/stars/CS-EVAL/CS-Eval.svg?style=social&label=Star) 
- [**garak**](https://github.com/NVIDIA/garak) - NVIDIA ![Star](https://img.shields.io/github/stars/NVIDIA/garak.svg?style=social&label=Star) 
- [**arch**](https://github.com/katanemo/arch) - katanemo ![Star](https://img.shields.io/github/stars/katanemo/arch.svg?style=social&label=Star) 

	 路 ([x](https://x.com/salman_paracha/status/1848374304196719047))

## Misc

- [The Rise of AI Hackbots | Joseph Thacker | TEDxUKY](https://www.youtube.com/watch?v=Y_x6KXV1y_0)  :clapper: 
- [DeepSeek Fails Researchers' Safety Tests](https://www.pcmag.com/news/deepseek-fails-every-safety-test-thrown-at-it-by-researchers) 