# Awesome llm misc

- [Awesome llm misc](#awesome-llm-misc)
	- [Survey](#survey)
		- [Blogs](#blogs)
	- [Toolkits](#toolkits)
	- [Unlearning](#unlearning)
	- [Personality](#personality)
	- [World Model](#world-model)
	- [Red teaming (safety)](#red-teaming-safety)
	- [Forecasting](#forecasting)
	- [Chat arena](#chat-arena)
	- [State Space Model](#state-space-model)
	- [New model](#new-model)
	- [LLM detection](#llm-detection)
	- [Interpretability](#interpretability)
	- [Generaliazation](#generaliazation)
	- [LLM editting](#llm-editting)
	- [AGI insights](#agi-insights)
	- [Callibration](#callibration)
	- [Books](#books)
	- [Privacy](#privacy)
	- [Misc](#misc)
	- [Impacts](#impacts)
	- [Course \& Tutorial](#course--tutorial)
		- [CUDA](#cuda)
	- [Extra reference](#extra-reference)

## Survey
- **Controllable Text Generation for Large Language Models: A Survey**, `arXiv, 2408.12599`, [arxiv](http://arxiv.org/abs/2408.12599v1), [pdf](http://arxiv.org/pdf/2408.12599v1.pdf), cication: [**1**](https://scholar.google.com/scholar?cites=16876198695229003709&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Xun Liang, Hanyu Wang, Yezhaohui Wang, Shichao Song, Jiawei Yang, Simin Niu, Jie Hu, Dan Liu, Shunyu Yao, Feiyu Xiong* · ([CTGSurvey](https://github.com/IAAR-Shanghai/CTGSurvey) - IAAR-Shanghai) ![Star](https://img.shields.io/github/stars/IAAR-Shanghai/CTGSurvey.svg?style=social&label=Star)
- **Language Modeling on Tabular Data: A Survey of Foundations, Techniques
  and Evolution**, `arXiv, 2408.10548`, [arxiv](http://arxiv.org/abs/2408.10548v1), [pdf](http://arxiv.org/pdf/2408.10548v1.pdf), cication: [**-1**](None)

	 *Yucheng Ruan, Xiang Lan, Jingying Ma, Yizhi Dong, Kai He, Mengling Feng* · ([Language-Modeling-on-Tabular-Data-Survey.git](https://github.com/lanxiang1017/Language-Modeling-on-Tabular-Data-Survey.git) - lanxiang1017) ![Star](https://img.shields.io/github/stars/lanxiang1017/Language-Modeling-on-Tabular-Data-Survey.git.svg?style=social&label=Star)
- **A Survey of Mamba**, `arXiv, 2408.01129`, [arxiv](http://arxiv.org/abs/2408.01129v1), [pdf](http://arxiv.org/pdf/2408.01129v1.pdf), cication: [**-1**](None)

	 *Haohao Qu, Liangbo Ning, Rui An, Wenqi Fan, Tyler Derr, Xin Xu, Qing Li*
- [**agi-survey**](https://github.com/ulab-uiuc/agi-survey) - ulab-uiuc ![Star](https://img.shields.io/github/stars/ulab-uiuc/agi-survey.svg?style=social&label=Star)
- **A Survey on Self-Evolution of Large Language Models**, `arXiv, 2404.14387`, [arxiv](http://arxiv.org/abs/2404.14387v1), [pdf](http://arxiv.org/pdf/2404.14387v1.pdf), cication: [**-1**](None)

	 *Zhengwei Tao, Ting-En Lin, Xiancai Chen, Hangyu Li, Yuchuan Wu, Yongbin Li, Zhi Jin, Fei Huang, Dacheng Tao, Jingren Zhou*
- **State Space Model for New-Generation Network Alternative to
  Transformers: A Survey**, `arXiv, 2404.09516`, [arxiv](http://arxiv.org/abs/2404.09516v1), [pdf](http://arxiv.org/pdf/2404.09516v1.pdf), cication: [**-1**](None)

	 *Xiao Wang, Shiao Wang, Yuhe Ding, Yuehang Li, Wentao Wu, Yao Rong, Weizhe Kong, Ju Huang, Shihao Li, Haoxiang Yang*
- **Multilingual Large Language Model: A Survey of Resources, Taxonomy and
  Frontiers**, `arXiv, 2404.04925`, [arxiv](http://arxiv.org/abs/2404.04925v1), [pdf](http://arxiv.org/pdf/2404.04925v1.pdf), cication: [**-1**](None)

	 *Libo Qin, Qiguang Chen, Yuhang Zhou, Zhi Chen, Yinghui Li, Lizi Liao, Min Li, Wanxiang Che, Philip S. Yu*
- **A Survey on Multilingual Large Language Models: Corpora, Alignment, and
  Bias**, `arXiv, 2404.00929`, [arxiv](http://arxiv.org/abs/2404.00929v1), [pdf](http://arxiv.org/pdf/2404.00929v1.pdf), cication: [**-1**](None)

	 *Yuemei Xu, Ling Hu, Jiayi Zhao, Zihan Qiu, Yuqi Ye, Hanwen Gu*
- **ChatGPT Alternative Solutions: Large Language Models Survey**, `arXiv, 2403.14469`, [arxiv](http://arxiv.org/abs/2403.14469v1), [pdf](http://arxiv.org/pdf/2403.14469v1.pdf), cication: [**-1**](None)

	 *Hanieh Alipour, Nick Pendar, Kohinoor Roy*
- **Large Language Models and Causal Inference in Collaboration: A
  Comprehensive Survey**, `arXiv, 2403.09606`, [arxiv](http://arxiv.org/abs/2403.09606v1), [pdf](http://arxiv.org/pdf/2403.09606v1.pdf), cication: [**-1**](None)

	 *Xiaoyu Liu, Paiheng Xu, Junda Wu, Jiaxin Yuan, Yifan Yang, Yuhang Zhou, Fuxiao Liu, Tianrui Guan, Haoliang Wang, Tong Yu*
- **Knowledge Conflicts for LLMs: A Survey**, `arXiv, 2403.08319`, [arxiv](http://arxiv.org/abs/2403.08319v1), [pdf](http://arxiv.org/pdf/2403.08319v1.pdf), cication: [**-1**](None)

	 *Rongwu Xu, Zehan Qi, Cunxiang Wang, Hongru Wang, Yue Zhang, Wei Xu*
- **Large Language Models on Tabular Data -- A Survey**, `arXiv, 2402.17944`, [arxiv](http://arxiv.org/abs/2402.17944v1), [pdf](http://arxiv.org/pdf/2402.17944v1.pdf), cication: [**-1**](None)

	 *Xi Fang, Weijie Xu, Fiona Anting Tan, Jiani Zhang, Ziqing Hu, Yanjun Qi, Scott Nickleach, Diego Socolinsky, Srinivasan Sengamedu, Christos Faloutsos*
- **Large Language Models and Games: A Survey and Roadmap**, `arXiv, 2402.18659`, [arxiv](http://arxiv.org/abs/2402.18659v1), [pdf](http://arxiv.org/pdf/2402.18659v1.pdf), cication: [**-1**](None)

	 *Roberto Gallotta, Graham Todd, Marvin Zammit, Sam Earle, Antonios Liapis, Julian Togelius, Georgios N. Yannakakis*
- **A Survey on Recent Advances in LLM-Based Multi-turn Dialogue Systems**, `arXiv, 2402.18013`, [arxiv](http://arxiv.org/abs/2402.18013v1), [pdf](http://arxiv.org/pdf/2402.18013v1.pdf), cication: [**-1**](None)

	 *Zihao Yi, Jiarui Ouyang, Yuwen Liu, Tianhao Liao, Zhe Xu, Ying Shen*
- **A Survey of Large Language Models in Cybersecurity**, `arXiv, 2402.16968`, [arxiv](http://arxiv.org/abs/2402.16968v1), [pdf](http://arxiv.org/pdf/2402.16968v1.pdf), cication: [**-1**](None)

	 *Gabriel de Jesus Coelho da Silva, Carlos Becker Westphall*
- **Large Language Models for Data Annotation: A Survey**, `arXiv, 2402.13446`, [arxiv](http://arxiv.org/abs/2402.13446v1), [pdf](http://arxiv.org/pdf/2402.13446v1.pdf), cication: [**-1**](None)

	 *Zhen Tan, Alimohammad Beigi, Song Wang, Ruocheng Guo, Amrita Bhattacharjee, Bohan Jiang, Mansooreh Karami, Jundong Li, Lu Cheng, Huan Liu*
- **Large Language Models: A Survey**, `arXiv, 2402.06196`, [arxiv](http://arxiv.org/abs/2402.06196v1), [pdf](http://arxiv.org/pdf/2402.06196v1.pdf), cication: [**-1**](None)

	 *Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu, Richard Socher, Xavier Amatriain, Jianfeng Gao*
- **Continual Learning for Large Language Models: A Survey**, `arXiv, 2402.01364`, [arxiv](http://arxiv.org/abs/2402.01364v1), [pdf](http://arxiv.org/pdf/2402.01364v1.pdf), cication: [**-1**](None)

	 *Tongtong Wu, Linhao Luo, Yuan-Fang Li, Shirui Pan, Thuy-Trang Vu, Gholamreza Haffari*
- **From Google Gemini to OpenAI Q* (Q-Star): A Survey of Reshaping the
  Generative Artificial Intelligence (AI) Research Landscape**, `arXiv, 2312.10868`, [arxiv](http://arxiv.org/abs/2312.10868v1), [pdf](http://arxiv.org/pdf/2312.10868v1.pdf), cication: [**-1**](None)

	 *Timothy R. McIntosh, Teo Susnjak, Tong Liu, Paul Watters, Malka N. Halgamuge*
- **A Survey of Large Language Models Attribution**, `arXiv, 2311.03731`, [arxiv](http://arxiv.org/abs/2311.03731v1), [pdf](http://arxiv.org/pdf/2311.03731v1.pdf), cication: [**-1**](None)

	 *Dongfang Li, Zetian Sun, Xinshuo Hu, Zhenyu Liu, Ziyang Chen, Baotian Hu, Aiguo Wu, Min Zhang* · ([awesome-llm-attributions](https://github.com/HITsz-TMG/awesome-llm-attributions) - HITsz-TMG) ![Star](https://img.shields.io/github/stars/HITsz-TMG/awesome-llm-attributions.svg?style=social&label=Star)
- **On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large
  Language Models**, `arXiv, 2307.09793`, [arxiv](http://arxiv.org/abs/2307.09793v1), [pdf](http://arxiv.org/pdf/2307.09793v1.pdf), cication: [**1**](https://scholar.google.com/scholar?cites=10725945032792348366&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Sarah Gao, Andrew Kean Gao* · ([constellation.sites.stanford](https://constellation.sites.stanford.edu/))
- **A Survey of Large Language Models**, `arXiv, 2303.18223`, [arxiv](http://arxiv.org/abs/2303.18223v12), [pdf](http://arxiv.org/pdf/2303.18223v12.pdf), cication: [**285**](https://scholar.google.com/scholar?cites=4202230929734215725&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong* · ([LLMSurvey](https://github.com/RUCAIBox/LLMSurvey) - RUCAIBox) ![Star](https://img.shields.io/github/stars/RUCAIBox/LLMSurvey.svg?style=social&label=Star)

### Blogs
- [2023, year of open LLMs](https://huggingface.co/blog/2023-in-llms)
- [Research Papers in November 2023](https://magazine.sebastianraschka.com/p/research-papers-in-november-2023)
- [AI and Open Source in 2023 - by Sebastian Raschka, PhD](https://magazine.sebastianraschka.com/p/ai-and-open-source-in-2023)
- [The History of Open-Source LLMs: Imitation and Alignment (Part Three)](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-imitation)
- [Research Papers (October 2023) - by Sebastian Raschka, PhD](https://magazine.sebastianraschka.com/p/research-papers-october-2023)
- [A Survey of Techniques for Maximizing LLM Performance](https://www.youtube.com/watch?v=ahnGLM-RC1Y&ab_channel=OpenAI)
- [Transformer Taxonomy (the last lit review) | kipply's blog](https://kipp.ly/transformer-taxonomy/?continueFlag=a897a8d0eb16dcae5398f1b58cc5e06f)

	 · ([jiqizhixin](https://www.jiqizhixin.com/articles/2023-08-14-9))
- [Catching up on the weird world of LLMs](https://simonwillison.net/2023/Aug/3/weird-world-of-llms/)

## Toolkits
- [**chat-langchain**](https://github.com/langchain-ai/chat-langchain) - langchain-ai ![Star](https://img.shields.io/github/stars/langchain-ai/chat-langchain.svg?style=social&label=Star)
- [**ChatGPT4**](https://huggingface.co/spaces/yuntian-deng/ChatGPT4) - yuntian-deng 🤗
- [**amazing-openai-api**](https://github.com/soulteary/amazing-openai-api) - soulteary ![Star](https://img.shields.io/github/stars/soulteary/amazing-openai-api.svg?style=social&label=Star)

	 *Convert different model APIs into the OpenAI API format out of the box.*
- [**jan**](https://github.com/janhq/jan) - janhq ![Star](https://img.shields.io/github/stars/janhq/jan.svg?style=social&label=Star)

	 *Jan is an open source alternative to ChatGPT that runs 100% offline on your computer*
- [**GPT_API_free**](https://github.com/chatanywhere/GPT_API_free) - chatanywhere ![Star](https://img.shields.io/github/stars/chatanywhere/GPT_API_free.svg?style=social&label=Star)

	 *Free ChatGPT API Key，免费ChatGPT API，支持GPT4 API（免费），ChatGPT国内可用免费转发API，直连无需代理。可以搭配ChatBox等软件/插件使用，极大降低接口使用成本。国内即可无限制畅快聊天。*
- [**BricksLLM**](https://github.com/bricks-cloud/BricksLLM) - bricks-cloud ![Star](https://img.shields.io/github/stars/bricks-cloud/BricksLLM.svg?style=social&label=Star)

	 *Simplifying LLM ops in production*
- [**skypilot**](https://github.com/skypilot-org/skypilot) - skypilot-org ![Star](https://img.shields.io/github/stars/skypilot-org/skypilot.svg?style=social&label=Star)

	 *SkyPilot: Run LLMs, AI, and Batch jobs on any cloud. Get maximum savings, highest GPU availability, and managed execution—all with a simple interface.*

	 · ([blog.skypilot](https://blog.skypilot.co/introducing-sky-serve/))
- [**vllm**](https://github.com/vllm-project/vllm) - vllm-project ![Star](https://img.shields.io/github/stars/vllm-project/vllm.svg?style=social&label=Star)

	 *A high-throughput and memory-efficient inference and serving engine for LLMs*
- [**langflow**](https://github.com/logspace-ai/langflow) - logspace-ai ![Star](https://img.shields.io/github/stars/logspace-ai/langflow.svg?style=social&label=Star)

	 *⛓️ LangFlow is a UI for LangChain, designed with react-flow to provide an effortless way to experiment and prototype flows.*
- [**torchscale**](https://github.com/microsoft/torchscale) - microsoft ![Star](https://img.shields.io/github/stars/microsoft/torchscale.svg?style=social&label=Star)

	 *Foundation Architecture for (M)LLMs*
- [**LLM-As-Chatbot**](https://github.com/deep-diver/LLM-As-Chatbot) - deep-diver ![Star](https://img.shields.io/github/stars/deep-diver/LLM-As-Chatbot.svg?style=social&label=Star)

	 *LLM as a Chatbot Service*
- [**Llama-2-Open-Source-LLM-CPU-Inference**](https://github.com/kennethleungty/Llama-2-Open-Source-LLM-CPU-Inference) - kennethleungty ![Star](https://img.shields.io/github/stars/kennethleungty/Llama-2-Open-Source-LLM-CPU-Inference.svg?style=social&label=Star)

	 *Running Llama 2 and other Open-Source LLMs on CPU Inference Locally for Document Q&A*
- [**ollama**](https://github.com/jmorganca/ollama) - jmorganca ![Star](https://img.shields.io/github/stars/jmorganca/ollama.svg?style=social&label=Star)

	 *Get up and running with large language models locally*
- [**OpenLLM**](https://github.com/bentoml/OpenLLM) - bentoml ![Star](https://img.shields.io/github/stars/bentoml/OpenLLM.svg?style=social&label=Star)

	 *An open platform for operating large language models (LLMs) in production. Fine-tune, serve, deploy, and monitor any LLMs with ease.*
- [**litellm**](https://github.com/BerriAI/litellm) - BerriAI ![Star](https://img.shields.io/github/stars/BerriAI/litellm.svg?style=social&label=Star)

	 *Call all LLM APIs using the OpenAI format. Use Bedrock, Azure, OpenAI, Cohere, Anthropic, Ollama, Sagemaker, HuggingFace, Replicate (100+ LLMs)*
- [**ollama**](https://github.com/jmorganca/ollama) - jmorganca ![Star](https://img.shields.io/github/stars/jmorganca/ollama.svg?style=social&label=Star)

	 *Get up and running with Llama 2 and other large language models locally*
- [**gpu_poor**](https://github.com/RahulSChand/gpu_poor) - RahulSChand ![Star](https://img.shields.io/github/stars/RahulSChand/gpu_poor.svg?style=social&label=Star)

	 *Calculate GPU memory requirement & breakdown for training/inference of LLM models. Supports ggml/bnb quantization*
- [**leptonai**](https://github.com/leptonai/leptonai) - leptonai ![Star](https://img.shields.io/github/stars/leptonai/leptonai.svg?style=social&label=Star)

	 *A Pythonic framework to simplify AI service building*
- [**exllamav2**](https://github.com/turboderp/exllamav2) - turboderp ![Star](https://img.shields.io/github/stars/turboderp/exllamav2.svg?style=social&label=Star)

	 *A fast inference library for running LLMs locally on modern consumer-class GPUs*
- [**outlines**](https://github.com/normal-computing/outlines) - normal-computing ![Star](https://img.shields.io/github/stars/normal-computing/outlines.svg?style=social&label=Star)

	 *Generative Model Programming*
- [**one-api**](https://github.com/songquanpeng/one-api) - songquanpeng ![Star](https://img.shields.io/github/stars/songquanpeng/one-api.svg?style=social&label=Star)

	 *OpenAI 接口管理 & 分发系统，支持 Azure、Anthropic Claude、Google PaLM 2、智谱 ChatGLM、百度文心一言、讯飞星火认知以及阿里通义千问，可用于二次分发管理 key，仅单可执行文件，已打包好 Docker 镜像，一键部署，开箱即用. OpenAI key management & redistribution system, using a single API for all LLMs, and features an English UI.*
- [**LLaMA2-Accessory**](https://github.com/Alpha-VLLM/LLaMA2-Accessory) - Alpha-VLLM ![Star](https://img.shields.io/github/stars/Alpha-VLLM/LLaMA2-Accessory.svg?style=social&label=Star)

	 *An Open-source Toolkit for LLM Development*
- [**Flowise**](https://github.com/FlowiseAI/Flowise) - FlowiseAI ![Star](https://img.shields.io/github/stars/FlowiseAI/Flowise.svg?style=social&label=Star)

	 *Drag & drop UI to build your customized LLM flow*
- [**simpleaichat**](https://github.com/minimaxir/simpleaichat) - minimaxir ![Star](https://img.shields.io/github/stars/minimaxir/simpleaichat.svg?style=social&label=Star)

	 *Python package for easily interfacing with chat apps, with robust features and minimal code complexity.*
- [**TypeChat**](https://github.com/Microsoft/TypeChat) - Microsoft ![Star](https://img.shields.io/github/stars/Microsoft/TypeChat.svg?style=social&label=Star)

	 *TypeChat is a library that makes it easy to build natural language interfaces using types.*
- [**petals**](https://github.com/bigscience-workshop/petals) - bigscience-workshop ![Star](https://img.shields.io/github/stars/bigscience-workshop/petals.svg?style=social&label=Star)

	 *🌸 Run large language models at home, BitTorrent-style. Fine-tuning and inference up to 10x faster than offloading*
- [**chatbox**](https://github.com/Bin-Huang/chatbox) - Bin-Huang ![Star](https://img.shields.io/github/stars/Bin-Huang/chatbox.svg?style=social&label=Star)

	 *Chatbox is a desktop app for GPT/LLM that supports Windows, Mac, Linux & Web Online*
- [**h2o-llmstudio**](https://github.com/h2oai/h2o-llmstudio) - h2oai ![Star](https://img.shields.io/github/stars/h2oai/h2o-llmstudio.svg?style=social&label=Star)

	 *H2O LLM Studio - a framework and no-code GUI for fine-tuning LLMs*
- [**LMFlow**](https://github.com/OptimalScale/LMFlow) - OptimalScale ![Star](https://img.shields.io/github/stars/OptimalScale/LMFlow.svg?style=social&label=Star)

	 *An Extensible Toolkit for Finetuning and Inference of Large Foundation Models. Large Model for All.*
- [**FlagAI**](https://github.com/FlagAI-Open/FlagAI) - FlagAI-Open ![Star](https://img.shields.io/github/stars/FlagAI-Open/FlagAI.svg?style=social&label=Star)

	 *FlagAI (Fast LArge-scale General AI models) is a fast, easy-to-use and extensible toolkit for large-scale model.*

## Unlearning
- **To Forget or Not? Towards Practical Knowledge Unlearning for Large
  Language Models**, `arXiv, 2407.01920`, [arxiv](http://arxiv.org/abs/2407.01920v1), [pdf](http://arxiv.org/pdf/2407.01920v1.pdf), cication: [**-1**](None)

	 *Bozhong Tian, Xiaozhuan Liang, Siyuan Cheng, Qingbin Liu, Mengru Wang, Dianbo Sui, Xi Chen, Huajun Chen, Ningyu Zhang*

	 · ([KnowUnDo](https://github.com/zjunlp/KnowUnDo) - zjunlp) ![Star](https://img.shields.io/github/stars/zjunlp/KnowUnDo.svg?style=social&label=Star)
- **UnUnlearning: Unlearning is not sufficient for content regulation in
  advanced generative AI**, `arXiv, 2407.00106`, [arxiv](http://arxiv.org/abs/2407.00106v1), [pdf](http://arxiv.org/pdf/2407.00106v1.pdf), cication: [**-1**](None)

	 *Ilia Shumailov, Jamie Hayes, Eleni Triantafillou, Guillermo Ortiz-Jimenez, Nicolas Papernot, Matthew Jagielski, Itay Yona, Heidi Howard, Eugene Bagdasaryan*
- **What makes unlearning hard and what to do about it**, `arXiv, 2406.01257`, [arxiv](http://arxiv.org/abs/2406.01257v1), [pdf](http://arxiv.org/pdf/2406.01257v1.pdf), cication: [**-1**](None)

	 *Kairan Zhao, Meghdad Kurmanji, George-Octavian Bărbulescu, Eleni Triantafillou, Peter Triantafillou*
- **The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning**, `arXiv, 2403.03218`, [arxiv](http://arxiv.org/abs/2403.03218v2), [pdf](http://arxiv.org/pdf/2403.03218v2.pdf), cication: [**-1**](None)

	 *Nathaniel Li, Alexander Pan, Anjali Gopal, Summer Yue, Daniel Berrios, Alice Gatti, Justin D. Li, Ann-Kathrin Dombrowski, Shashwat Goel, Long Phan*
	- `The WMDP benchmark is a curated dataset of over 4,000 questions designed to gauge and mitigate LLMs' knowledge in areas with misuse potential, such as biosecurity and cybersecurity. `
- **Machine Unlearning of Pre-trained Large Language Models**, `arXiv, 2402.15159`, [arxiv](http://arxiv.org/abs/2402.15159v1), [pdf](http://arxiv.org/pdf/2402.15159v1.pdf), cication: [**-1**](None)

	 *Jin Yao, Eli Chien, Minxin Du, Xinyao Niu, Tianhao Wang, Zezhou Cheng, Xiang Yue* · ([Unlearning_LLM](https://github.com/yaojin17/Unlearning_LLM) - yaojin17) ![Star](https://img.shields.io/github/stars/yaojin17/Unlearning_LLM.svg?style=social&label=Star)
- **TOFU: A Task of Fictitious Unlearning for LLMs**, `arXiv, 2401.06121`, [arxiv](http://arxiv.org/abs/2401.06121v1), [pdf](http://arxiv.org/pdf/2401.06121v1.pdf), cication: [**-1**](None)

	 *Pratyush Maini, Zhili Feng, Avi Schwarzschild, Zachary C. Lipton, J. Zico Kolter*
- **Large Language Model Unlearning**, `arXiv, 2310.10683`, [arxiv](http://arxiv.org/abs/2310.10683v1), [pdf](http://arxiv.org/pdf/2310.10683v1.pdf), cication: [**-1**](None)

	 *Yuanshun Yao, Xiaojun Xu, Yang Liu*

	 · ([jiqizhixin](https://www.jiqizhixin.com/articles/2023-12-13-6)) · ([llm_unlearn](https://github.com/kevinyaobytedance/llm_unlearn) - kevinyaobytedance) ![Star](https://img.shields.io/github/stars/kevinyaobytedance/llm_unlearn.svg?style=social&label=Star)
- **Improving Language Plasticity via Pretraining with Active Forgetting**, `arXiv, 2307.01163`, [arxiv](http://arxiv.org/abs/2307.01163v2), [pdf](http://arxiv.org/pdf/2307.01163v2.pdf), cication: [**-1**](None)

	 *Yihong Chen, Kelly Marchisio, Roberta Raileanu, David Ifeoluwa Adelani, Pontus Stenetorp, Sebastian Riedel, Mikel Artetxe*
- [Announcing the first Machine Unlearning Challenge – Google Research Blog](https://ai.googleblog.com/2023/06/announcing-first-machine-unlearning.html)

## Personality
- **Large Language Models Understand and Can be Enhanced by Emotional
  Stimuli**, `arXiv, 2307.11760`, [arxiv](http://arxiv.org/abs/2307.11760v7), [pdf](http://arxiv.org/pdf/2307.11760v7.pdf), cication: [**6**](https://scholar.google.com/scholar?cites=5825846437972489885&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Cheng Li, Jindong Wang, Yixuan Zhang, Kaijie Zhu, Wenxin Hou, Jianxun Lian, Fang Luo, Qiang Yang, Xing Xie*
- **When Large Language Models Meet Personalization: Perspectives of
  Challenges and Opportunities**, `arXiv, 2307.16376`, [arxiv](http://arxiv.org/abs/2307.16376v1), [pdf](http://arxiv.org/pdf/2307.16376v1.pdf), cication: [**7**](https://scholar.google.com/scholar?cites=801122460433394373&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Jin Chen, Zheng Liu, Xu Huang, Chenwang Wu, Qi Liu, Gangwei Jiang, Yuanhao Pu, Yuxuan Lei, Xiaolong Chen, Xingmei Wang*
- **Personality Traits in Large Language Models**, `arXiv, 2307.00184`, [arxiv](http://arxiv.org/abs/2307.00184v3), [pdf](http://arxiv.org/pdf/2307.00184v3.pdf), cication: [**17**](https://scholar.google.com/scholar?cites=3059704921021154305&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Greg Serapio-García, Mustafa Safdari, Clément Crepy, Luning Sun, Stephen Fitz, Peter Romero, Marwa Abdulhai, Aleksandra Faust, Maja Matarić*

## World Model
- **Efficient World Models with Context-Aware Tokenization**, `arXiv, 2406.19320`, [arxiv](http://arxiv.org/abs/2406.19320v1), [pdf](http://arxiv.org/pdf/2406.19320v1.pdf), cication: [**-1**](None)

	 *Vincent Micheli, Eloi Alonso, François Fleuret*

	 · ([delta-iris](https://github.com/vmicheli/delta-iris) - vmicheli) ![Star](https://img.shields.io/github/stars/vmicheli/delta-iris.svg?style=social&label=Star)
- **Can Language Models Serve as Text-Based World Simulators?**, `arXiv, 2406.06485`, [arxiv](http://arxiv.org/abs/2406.06485v1), [pdf](http://arxiv.org/pdf/2406.06485v1.pdf), cication: [**-1**](None)

	 *Ruoyao Wang, Graham Todd, Ziang Xiao, Xingdi Yuan, Marc-Alexandre Côté, Peter Clark, Peter Jansen*
- **Cognitively Inspired Energy-Based World Models**, `arXiv, 2406.08862`, [arxiv](http://arxiv.org/abs/2406.08862v1), [pdf](http://arxiv.org/pdf/2406.08862v1.pdf), cication: [**-1**](None)

	 *Alexi Gladstone, Ganesh Nanduru, Md Mofijul Islam, Aman Chadha, Jundong Li, Tariq Iqbal*
- [**Pandora**](https://github.com/maitrix-org/Pandora?tab=readme-ov-file) - maitrix-org ![Star](https://img.shields.io/github/stars/maitrix-org/Pandora.svg?style=social&label=Star)

	 *Pandora: Towards General World Model with Natural Language Actions and Video States* · ([world-model.maitrix](https://world-model.maitrix.org/))
- **iVideoGPT: Interactive VideoGPTs are Scalable World Models**, `arXiv, 2405.15223`, [arxiv](http://arxiv.org/abs/2405.15223v1), [pdf](http://arxiv.org/pdf/2405.15223v1.pdf), cication: [**-1**](None)

	 *Jialong Wu, Shaofeng Yin, Ningya Feng, Xu He, Dong Li, Jianye Hao, Mingsheng Long*
- **Diffusion for World Modeling: Visual Details Matter in Atari**, `arXiv, 2405.12399`, [arxiv](http://arxiv.org/abs/2405.12399v1), [pdf](http://arxiv.org/pdf/2405.12399v1.pdf), cication: [**-1**](None)

	 *Eloi Alonso, Adam Jelley, Vincent Micheli, Anssi Kanervisto, Amos Storkey, Tim Pearce, François Fleuret* · ([diamond](https://github.com/eloialonso/diamond) - eloialonso) ![Star](https://img.shields.io/github/stars/eloialonso/diamond.svg?style=social&label=Star)
- **Robust agents learn causal world models**, `arXiv, 2402.10877`, [arxiv](http://arxiv.org/abs/2402.10877v6), [pdf](http://arxiv.org/pdf/2402.10877v6.pdf), cication: [**-1**](None)

	 *Jonathan Richens, Tom Everitt*
- **Learning and Leveraging World Models in Visual Representation Learning**, `arXiv, 2403.00504`, [arxiv](http://arxiv.org/abs/2403.00504v1), [pdf](http://arxiv.org/pdf/2403.00504v1.pdf), cication: [**-1**](None)

	 *Quentin Garrido, Mahmoud Assran, Nicolas Ballas, Adrien Bardes, Laurent Najman, Yann LeCun*
- **Video as the New Language for Real-World Decision Making**, `arXiv, 2402.17139`, [arxiv](http://arxiv.org/abs/2402.17139v1), [pdf](http://arxiv.org/pdf/2402.17139v1.pdf), cication: [**-1**](None)

	 *Sherry Yang, Jacob Walker, Jack Parker-Holder, Yilun Du, Jake Bruce, Andre Barreto, Pieter Abbeel, Dale Schuurmans*

	 · ([mp.weixin.qq](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652449854&idx=2&sn=91b372f92a2cc98f862cdc37cfaeabad))
- **Genie: Generative Interactive Environments**, `arXiv, 2402.15391`, [arxiv](http://arxiv.org/abs/2402.15391v1), [pdf](http://arxiv.org/pdf/2402.15391v1.pdf), cication: [**-1**](None)

	 *Jake Bruce, Michael Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps*
- **Diffusion World Model**, `arXiv, 2402.03570`, [arxiv](http://arxiv.org/abs/2402.03570v1), [pdf](http://arxiv.org/pdf/2402.03570v1.pdf), cication: [**-1**](None)

	 *Zihan Ding, Amy Zhang, Yuandong Tian, Qinqing Zheng*
- **The Geometry of Truth: Emergent Linear Structure in Large Language Model
  Representations of True/False Datasets**, `arXiv, 2310.06824`, [arxiv](http://arxiv.org/abs/2310.06824v1), [pdf](http://arxiv.org/pdf/2310.06824v1.pdf), cication: [**-1**](None)

	 *Samuel Marks, Max Tegmark* · ([mp.weixin.qq](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652393580&idx=2&sn=31bcdd233c49fb79e78d06045df491dc))
- **Language Models Represent Space and Time**, `arXiv, 2310.02207`, [arxiv](http://arxiv.org/abs/2310.02207v1), [pdf](http://arxiv.org/pdf/2310.02207v1.pdf), cication: [**2**](https://scholar.google.com/scholar?cites=2674847876149703750&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Wes Gurnee, Max Tegmark* · ([world-models](https://github.com/wesg52/world-models) - wesg52) ![Star](https://img.shields.io/github/stars/wesg52/world-models.svg?style=social&label=Star)
- [How far are we from AGI?](https://aisupremacy.substack.com/p/how-far-are-we-from-agi)

	 · ([mp.weixin.qq](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652366759&idx=2&sn=48422d1a7ffae891ea61108761f4d582))

- [what a world model is](https://twitter.com/ylecun/status/1759933365241921817)

- [图灵奖得主杨立昆教授在哈佛大学数学系演讲稿——关于人工智能世界新模型](https://mp.weixin.qq.com/s?__biz=MzIyMzk1MDE3Nw==&mid=2247631931&idx=1&sn=fd0a217a3302125daaceb5103639739c)
- [图灵奖得主LeCun最新专访：为什么物理世界终将成为LLM的「死穴」？](https://mp.weixin.qq.com/s?__biz=MzIyMzk1MDE3Nw==&mid=2247629387&idx=1&sn=d8246e2f1f134cb3c635f915a2d0af90)
- [OpenAI「登月计划」剑指超级AI！LeCun提出AGI之路七阶段，打造世界模型是首位](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652419855&idx=3&sn=a5f20e0a0061c01e1ec87d4a81c23e68)
## Forecasting
- **Chronos: Learning the Language of Time Series**, `arXiv, 2403.07815`, [arxiv](http://arxiv.org/abs/2403.07815v1), [pdf](http://arxiv.org/pdf/2403.07815v1.pdf), cication: [**-1**](None)

	 *Abdul Fatir Ansari, Lorenzo Stella, Caner Turkmen, Xiyuan Zhang, Pedro Mercado, Huibin Shen, Oleksandr Shchur, Syama Sundar Rangapuram, Sebastian Pineda Arango, Shubham Kapoor*
- **Lag-Llama: Towards Foundation Models for Probabilistic Time Series
  Forecasting**, `arXiv, 2310.08278`, [arxiv](http://arxiv.org/abs/2310.08278v3), [pdf](http://arxiv.org/pdf/2310.08278v3.pdf), cication: [**11**](https://scholar.google.com/scholar?cites=8753400217691232559&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Kashif Rasul, Arjun Ashok, Andrew Robert Williams, Hena Ghonia, Rishika Bhagwatkar, Arian Khorasani, Mohammad Javad Darvishi Bayazi, George Adamopoulos, Roland Riachi, Nadhir Hassen* · ([lag-llama](https://github.com/time-series-foundation-models/lag-llama) - time-series-foundation-models) ![Star](https://img.shields.io/github/stars/time-series-foundation-models/lag-llama.svg?style=social&label=Star)
- **Time-LLM: Time Series Forecasting by Reprogramming Large Language Models**, `arXiv, 2310.01728`, [arxiv](http://arxiv.org/abs/2310.01728v2), [pdf](http://arxiv.org/pdf/2310.01728v2.pdf), cication: [**17**](https://scholar.google.com/scholar?cites=4605741467245220036&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y. Zhang, Xiaoming Shi, Pin-Yu Chen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan* · ([time-llm](https://github.com/kimmeen/time-llm) - kimmeen) ![Star](https://img.shields.io/github/stars/kimmeen/time-llm.svg?style=social&label=Star)
- **A decoder-only foundation model for time-series forecasting**, `arXiv, 2310.10688`, [arxiv](http://arxiv.org/abs/2310.10688v3), [pdf](http://arxiv.org/pdf/2310.10688v3.pdf), cication: [**2**](https://scholar.google.com/scholar?cites=2603810331889084693&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Abhimanyu Das, Weihao Kong, Rajat Sen, Yichen Zhou* · ([jiqizhixin](https://www.jiqizhixin.com/articles/2024-02-05-9))

## Chat arena
- [No-code LLM fine-tuning and evaluation at scale – Airtrain.ai](https://www.airtrain.ai/?ref=producthunt)
- **Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference**, `arXiv, 2403.04132`, [arxiv](http://arxiv.org/abs/2403.04132v1), [pdf](http://arxiv.org/pdf/2403.04132v1.pdf), cication: [**-1**](None)

	 *Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E. Gonzalez*
- [**GodMode**](https://github.com/smol-ai/GodMode) - smol-ai ![Star](https://img.shields.io/github/stars/smol-ai/GodMode.svg?style=social&label=Star)

	 *AI Chat Browser: Fast, Full webapp access to ChatGPT / Claude / Bard / Bing / Llama2! I use this 20 times a day.*
- [**ChatALL**](https://github.com/sunner/ChatALL) - sunner ![Star](https://img.shields.io/github/stars/sunner/ChatALL.svg?style=social&label=Star)

	 *Concurrently chat with ChatGPT, Bing Chat, Bard, Alpaca, Vicuna, Claude, ChatGLM, MOSS, 讯飞星火, 文心一言 and more, discover the best answers*

## State Space Model
- [**Zamba2-1.2B**](https://huggingface.co/Zyphra/Zamba2-1.2B) - Zyphra 🤗
- **The Mamba in the Llama: Distilling and Accelerating Hybrid Models**, `arXiv, 2408.15237`, [arxiv](http://arxiv.org/abs/2408.15237v1), [pdf](http://arxiv.org/pdf/2408.15237v1.pdf), cication: [**-1**](None)

	 *Junxiong Wang, Daniele Paliotta, Avner May, Alexander M. Rush, Tri Dao* · ([MambaInLlama](https://github.com/jxiw/MambaInLlama) - jxiw) ![Star](https://img.shields.io/github/stars/jxiw/MambaInLlama.svg?style=social&label=Star)
- **Transformers to SSMs: Distilling Quadratic Knowledge to Subquadratic
  Models**, `arXiv, 2408.10189`, [arxiv](http://arxiv.org/abs/2408.10189v1), [pdf](http://arxiv.org/pdf/2408.10189v1.pdf), cication: [**-1**](None)

	 *Aviv Bick, Kevin Y. Li, Eric P. Xing, J. Zico Kolter, Albert Gu* · ([phi-mamba](https://github.com/goombalab/phi-mamba) - goombalab) ![Star](https://img.shields.io/github/stars/goombalab/phi-mamba.svg?style=social&label=Star)
- **A Survey of Mamba**, `arXiv, 2408.01129`, [arxiv](http://arxiv.org/abs/2408.01129v1), [pdf](http://arxiv.org/pdf/2408.01129v1.pdf), cication: [**-1**](None)

	 *Haohao Qu, Liangbo Ning, Rui An, Wenqi Fan, Tyler Derr, Xin Xu, Qing Li*
- **MoMa: Efficient Early-Fusion Pre-training with Mixture of Modality-Aware
  Experts**, `arXiv, 2407.21770`, [arxiv](http://arxiv.org/abs/2407.21770v2), [pdf](http://arxiv.org/pdf/2407.21770v2.pdf), cication: [**-1**](None)

	 *Xi Victoria Lin, Akshat Shrivastava, Liang Luo, Srinivasan Iyer, Mike Lewis, Gargi Gosh, Luke Zettlemoyer, Armen Aghajanyan*
- **Samba: Simple Hybrid State Space Models for Efficient Unlimited Context
  Language Modeling**, `arXiv, 2406.07522`, [arxiv](http://arxiv.org/abs/2406.07522v1), [pdf](http://arxiv.org/pdf/2406.07522v1.pdf), cication: [**-1**](None)

	 *Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, Chen Liang, Weizhu Chen*
- **An Empirical Study of Mamba-based Language Models**, `arXiv, 2406.07887`, [arxiv](http://arxiv.org/abs/2406.07887v1), [pdf](http://arxiv.org/pdf/2406.07887v1.pdf), cication: [**-1**](None)

	 *Roger Waleffe, Wonmin Byeon, Duncan Riach, Brandon Norick, Vijay Korthikanti, Tri Dao, Albert Gu, Ali Hatamizadeh, Sudhakar Singh, Deepak Narayanan*
- **Transformers are SSMs: Generalized Models and Efficient Algorithms
  Through Structured State Space Duality**, `arXiv, 2405.21060`, [arxiv](http://arxiv.org/abs/2405.21060v1), [pdf](http://arxiv.org/pdf/2405.21060v1.pdf), cication: [**-1**](None)

	 *Tri Dao, Albert Gu* · ([mamba](https://github.com/state-spaces/mamba) - state-spaces) ![Star](https://img.shields.io/github/stars/state-spaces/mamba.svg?style=social&label=Star) · ([goombalab.github](https://goombalab.github.io/blog/2024/mamba2-part1-model/))
- **Zamba: A Compact 7B SSM Hybrid Model**, `arXiv, 2405.16712`, [arxiv](http://arxiv.org/abs/2405.16712v1), [pdf](http://arxiv.org/pdf/2405.16712v1.pdf), cication: [**-1**](None)

	 *Paolo Glorioso, Quentin Anthony, Yury Tokpanov, James Whittington, Jonathan Pilault, Adam Ibrahim, Beren Millidge*
- [**mamba-7b-rw**](https://huggingface.co/TRI-ML/mamba-7b-rw) - TRI-ML 🤗
- **The Illusion of State in State-Space Models**, `arXiv, 2404.08819`, [arxiv](http://arxiv.org/abs/2404.08819v1), [pdf](http://arxiv.org/pdf/2404.08819v1.pdf), cication: [**-1**](None)

	 *William Merrill, Jackson Petty, Ashish Sabharwal*
- [Zamba — Zyphra](https://www.zyphra.com/zamba)

	 · ([twitter](https://twitter.com/QuentinAnthon15/status/1780280071304937978))
- **MambaMixer: Efficient Selective State Space Models with Dual Token and
  Channel Selection**, `arXiv, 2403.19888`, [arxiv](http://arxiv.org/abs/2403.19888v1), [pdf](http://arxiv.org/pdf/2403.19888v1.pdf), cication: [**-1**](None)

	 *Ali Behrouz, Michele Santacatterina, Ramin Zabih*

	 · ([mambamixer.github](https://t.co/3rYmBevXMN))
- **Jamba: A Hybrid Transformer-Mamba Language Model**, `arXiv, 2403.19887`, [arxiv](http://arxiv.org/abs/2403.19887v1), [pdf](http://arxiv.org/pdf/2403.19887v1.pdf), cication: [**-1**](None)

	 *Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi, Shaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz* · ([ai21](https://www.ai21.com/blog/announcing-jamba))
	 · ([huggingface](https://huggingface.co/ai21labs/Jamba-v0.1))
- **VideoMamba: State Space Model for Efficient Video Understanding**, `arXiv, 2403.06977`, [arxiv](http://arxiv.org/abs/2403.06977v2), [pdf](http://arxiv.org/pdf/2403.06977v2.pdf), cication: [**-1**](None)

	 *Kunchang Li, Xinhao Li, Yi Wang, Yinan He, Yali Wang, Limin Wang, Yu Qiao* · ([VideoMamba](https://github.com/OpenGVLab/VideoMamba) - OpenGVLab) ![Star](https://img.shields.io/github/stars/OpenGVLab/VideoMamba.svg?style=social&label=Star)
- **DenseMamba: State Space Models with Dense Hidden Connection for
  Efficient Large Language Models**, `arXiv, 2403.00818`, [arxiv](http://arxiv.org/abs/2403.00818v2), [pdf](http://arxiv.org/pdf/2403.00818v2.pdf), cication: [**-1**](None)

	 *Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, Yunhe Wang*
- [Mamba: The Easy Way](https://jackcook.com/2024/02/23/mamba.html)
- **Can Mamba Learn How to Learn? A Comparative Study on In-Context Learning
  Tasks**, `arXiv, 2402.04248`, [arxiv](http://arxiv.org/abs/2402.04248v1), [pdf](http://arxiv.org/pdf/2402.04248v1.pdf), cication: [**-1**](None)

	 *Jongho Park, Jaeseung Park, Zheyang Xiong, Nayoung Lee, Jaewoong Cho, Samet Oymak, Kangwook Lee, Dimitris Papailiopoulos*
- **Repeat After Me: Transformers are Better than State Space Models at
  Copying**, `arXiv, 2402.01032`, [arxiv](http://arxiv.org/abs/2402.01032v1), [pdf](http://arxiv.org/pdf/2402.01032v1.pdf), cication: [**-1**](None)

	 *Samy Jelassi, David Brandfonbrener, Sham M. Kakade, Eran Malach*
- **BlackMamba: Mixture of Experts for State-Space Models**, `arXiv, 2402.01771`, [arxiv](http://arxiv.org/abs/2402.01771v1), [pdf](http://arxiv.org/pdf/2402.01771v1.pdf), cication: [**-1**](None)

	 *Quentin Anthony, Yury Tokpanov, Paolo Glorioso, Beren Millidge* · ([BlackMamba](https://github.com/Zyphra/BlackMamba) - Zyphra) ![Star](https://img.shields.io/github/stars/Zyphra/BlackMamba.svg?style=social&label=Star) · ([zyphra](https://www.zyphra.com/blackmamba))
	· ([static1.squarespace](https://static1.squarespace.com/static/658ded386c43c219ee47caba/t/65bd73200920d050ccbac40c/1706914594353/blackMamba.pdf))
- **MambaByte: Token-free Selective State Space Model**, `arXiv, 2401.13660`, [arxiv](http://arxiv.org/abs/2401.13660v1), [pdf](http://arxiv.org/pdf/2401.13660v1.pdf), cication: [**-1**](None)

	 *Junxiong Wang, Tushaar Gangavarapu, Jing Nathan Yan, Alexander M Rush*
- **MoE-Mamba: Efficient Selective State Space Models with Mixture of
  Experts**, `arXiv, 2401.04081`, [arxiv](http://arxiv.org/abs/2401.04081v1), [pdf](http://arxiv.org/pdf/2401.04081v1.pdf), cication: [**-1**](None)

	 *Maciej Pióro, Kamil Ciebiera, Krystian Król, Jan Ludziejewski, Sebastian Jaszczur*
- [The Annotated S4](https://srush.github.io/annotated-s4/)
- **Mamba: Linear-Time Sequence Modeling with Selective State Spaces**, `arXiv, 2312.00752`, [arxiv](http://arxiv.org/abs/2312.00752v1), [pdf](http://arxiv.org/pdf/2312.00752v1.pdf), cication: [**-1**](None)

	 *Albert Gu, Tri Dao* · ([mamba](https://github.com/state-spaces/mamba) - state-spaces) ![Star](https://img.shields.io/github/stars/state-spaces/mamba.svg?style=social&label=Star)

---
- [Do we need Attention? A Mamba Primer - YouTube](https://www.youtube.com/watch?v=dVH1dRoMPBc&t=10s&ab_channel=SashaRush%F0%9F%A4%97)
- [Mamba Explained](https://thegradient.pub/mamba-explained/)
- [Recent Mamba Papers - a julien-c Collection](https://huggingface.co/collections/julien-c/recent-mamba-papers-6602855121032a86a5d9fab7)
- [Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Paper Explained) - YouTube](https://www.youtube.com/watch?v=9dSkvxS2EB0&ab_channel=YannicKilcher)

## New model
- **Discrete Flow Matching**, `arXiv, 2407.15595`, [arxiv](http://arxiv.org/abs/2407.15595v1), [pdf](http://arxiv.org/pdf/2407.15595v1.pdf), cication: [**-1**](None)

	 *Itai Gat, Tal Remez, Neta Shaul, Felix Kreuk, Ricky T. Q. Chen, Gabriel Synnaeve, Yossi Adi, Yaron Lipman*
- **Learning to (Learn at Test Time): RNNs with Expressive Hidden States**, `arXiv, 2407.04620`, [arxiv](http://arxiv.org/abs/2407.04620v1), [pdf](http://arxiv.org/pdf/2407.04620v1.pdf), cication: [**-1**](None)

	 *Yu Sun, Xinhao Li, Karan Dalal, Jiarui Xu, Arjun Vikram, Genghan Zhang, Yann Dubois, Xinlei Chen, Xiaolong Wang, Sanmi Koyejo*
- **Simple and Effective Masked Diffusion Language Models**, `arXiv, 2406.07524`, [arxiv](http://arxiv.org/abs/2406.07524v1), [pdf](http://arxiv.org/pdf/2406.07524v1.pdf), cication: [**-1**](None)

	 *Subham Sekhar Sahoo, Marianne Arriola, Yair Schiff, Aaron Gokaslan, Edgar Marroquin, Justin T Chiu, Alexander Rush, Volodymyr Kuleshov* · ([mdlm](https://github.com/kuleshov-group/mdlm) - kuleshov-group) ![Star](https://img.shields.io/github/stars/kuleshov-group/mdlm.svg?style=social&label=Star)
- **A Unified Implicit Attention Formulation for Gated-Linear Recurrent
  Sequence Models**, `arXiv, 2405.16504`, [arxiv](http://arxiv.org/abs/2405.16504v1), [pdf](http://arxiv.org/pdf/2405.16504v1.pdf), cication: [**-1**](None)

	 *Itamar Zimerman, Ameen Ali, Lior Wolf* · ([UnifiedImplicitAttnRepr](https://github.com/Itamarzimm/UnifiedImplicitAttnRepr) - Itamarzimm) ![Star](https://img.shields.io/github/stars/Itamarzimm/UnifiedImplicitAttnRepr.svg?style=social&label=Star)
- **Attention as an RNN**, `arXiv, 2405.13956`, [arxiv](http://arxiv.org/abs/2405.13956v1), [pdf](http://arxiv.org/pdf/2405.13956v1.pdf), cication: [**-1**](None)

	 *Leo Feng, Frederick Tung, Hossein Hajimirsadeghi, Mohamed Osama Ahmed, Yoshua Bengio, Greg Mori* · ([jiqizhixin](https://www.jiqizhixin.com/articles/2024-05-27-3))
- [**linear_open_lm**](https://github.com/tri-ml/linear_open_lm) - tri-ml ![Star](https://img.shields.io/github/stars/tri-ml/linear_open_lm.svg?style=social&label=Star)

	 *A repository for research on medium sized language models.*
- **xLSTM: Extended Long Short-Term Memory**, `arXiv, 2405.04517`, [arxiv](http://arxiv.org/abs/2405.04517v1), [pdf](http://arxiv.org/pdf/2405.04517v1.pdf), cication: [**-1**](None)

	 *Maximilian Beck, Korbinian Pöppel, Markus Spanring, Andreas Auer, Oleksandra Prudnikova, Michael Kopp, Günter Klambauer, Johannes Brandstetter, Sepp Hochreiter*
- **HGRN2: Gated Linear RNNs with State Expansion**, `arXiv, 2404.07904`, [arxiv](http://arxiv.org/abs/2404.07904v1), [pdf](http://arxiv.org/pdf/2404.07904v1.pdf), cication: [**-1**](None)

	 *Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, Yiran Zhong*
- **Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence**, `arXiv, 2404.05892`, [arxiv](http://arxiv.org/abs/2404.05892v2), [pdf](http://arxiv.org/pdf/2404.05892v2.pdf), cication: [**-1**](None)

	 *Bo Peng, Daniel Goldstein, Quentin Anthony, Alon Albalak, Eric Alcaide, Stella Biderman, Eugene Cheah, Xingjian Du, Teddy Ferdinan, Haowen Hou* · ([RWKV-LM](https://github.com/RWKV/RWKV-LM) - RWKV) ![Star](https://img.shields.io/github/stars/RWKV/RWKV-LM.svg?style=social&label=Star) · ([ChatRWKV](https://github.com/RWKV/ChatRWKV) - RWKV) ![Star](https://img.shields.io/github/stars/RWKV/ChatRWKV.svg?style=social&label=Star)
- [**RWKV-LM**](https://github.com/RWKV/RWKV-LM) - RWKV ![Star](https://img.shields.io/github/stars/RWKV/RWKV-LM.svg?style=social&label=Star)

	 *RWKV is an RNN with transformer-level LLM performance. It can be directly trained like a GPT (parallelizable). So it's combining the best of RNN and transformer - great performance, fast inference, saves VRAM, fast training, "infinite" ctx\_len, and free sentence embedding.*
- **Infini-gram: Scaling Unbounded n-gram Language Models to a Trillion
  Tokens**, `arXiv, 2401.17377`, [arxiv](http://arxiv.org/abs/2401.17377v1), [pdf](http://arxiv.org/pdf/2401.17377v1.pdf), cication: [**-1**](None)

	 *Jiacheng Liu, Sewon Min, Luke Zettlemoyer, Yejin Choi, Hannaneh Hajishirzi*
- **Transfer Learning for Text Diffusion Models**, `arXiv, 2401.17181`, [arxiv](http://arxiv.org/abs/2401.17181v1), [pdf](http://arxiv.org/pdf/2401.17181v1.pdf), cication: [**-1**](None)

	 *Kehang Han, Kathleen Kenealy, Aditya Barua, Noah Fiedel, Noah Constant*
- [🦅 Eagle 7B : Soaring past Transformers with 1 Trillion Tokens Across 100+ Languages (RWKV-v5)](https://blog.rwkv.com/p/eagle-7b-soaring-past-transformers)

- [Paving the way to efficient architectures: StripedHyena-7B, open source models offering a glimpse into a world beyond Transformers](https://www.together.ai/blog/stripedhyena-7b)

- **TCNCA: Temporal Convolution Network with Chunked Attention for Scalable
  Sequence Processing**, `arXiv, 2312.05605`, [arxiv](http://arxiv.org/abs/2312.05605v1), [pdf](http://arxiv.org/pdf/2312.05605v1.pdf), cication: [**-1**](None)

	 *Aleksandar Terzic, Michael Hersche, Geethan Karunaratne, Luca Benini, Abu Sebastian, Abbas Rahimi*
- **GIVT: Generative Infinite-Vocabulary Transformers**, `arXiv, 2312.02116`, [arxiv](http://arxiv.org/abs/2312.02116v1), [pdf](http://arxiv.org/pdf/2312.02116v1.pdf), cication: [**-1**](None)

	 *Michael Tschannen, Cian Eastwood, Fabian Mentzer*
- **Text Rendering Strategies for Pixel Language Models**, `arXiv, 2311.00522`, [arxiv](http://arxiv.org/abs/2311.00522v1), [pdf](http://arxiv.org/pdf/2311.00522v1.pdf), cication: [**-1**](None)

	 *Jonas F. Lotz, Elizabeth Salesky, Phillip Rust, Desmond Elliott*
- **Retentive Network: A Successor to Transformer for Large Language Models**, `arXiv, 2307.08621`, [arxiv](http://arxiv.org/abs/2307.08621v4), [pdf](http://arxiv.org/pdf/2307.08621v4.pdf), cication: [**14**](https://scholar.google.com/scholar?cites=14499954689213944503&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, Furu Wei*
- **Copy Is All You Need**, `arXiv, 2307.06962`, [arxiv](http://arxiv.org/abs/2307.06962v1), [pdf](http://arxiv.org/pdf/2307.06962v1.pdf), cication: [**217**](https://scholar.google.com/scholar?cites=15114021291138625040&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Tian Lan, Deng Cai, Yan Wang, Heyan Huang, Xian-Ling Mao*
- **BiPhone: Modeling Inter Language Phonetic Influences in Text**, `arXiv, 2307.03322`, [arxiv](http://arxiv.org/abs/2307.03322v1), [pdf](http://arxiv.org/pdf/2307.03322v1.pdf), cication: [**-1**](None)

	 *Abhirut Gupta, Ananya B. Sai, Richard Sproat, Yuri Vasilevski, James S. Ren, Ambarish Jash, Sukhdeep S. Sodhi, Aravindan Raghuveer*
- **Deep Language Networks: Joint Prompt Training of Stacked LLMs using
  Variational Inference**, `arXiv, 2306.12509`, [arxiv](http://arxiv.org/abs/2306.12509v1), [pdf](http://arxiv.org/pdf/2306.12509v1.pdf), cication: [**4**](https://scholar.google.com/scholar?cites=14503894489688541656&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Alessandro Sordoni, Xingdi Yuan, Marc-Alexandre Côté, Matheus Pereira, Adam Trischler, Ziang Xiao, Arian Hosseini, Friederike Niedtner, Nicolas Le Roux*
- **Backpack Language Models**, `arXiv, 2305.16765`, [arxiv](http://arxiv.org/abs/2305.16765v1), [pdf](http://arxiv.org/pdf/2305.16765v1.pdf), cication: [**4**](https://scholar.google.com/scholar?cites=6150502937498838062&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *John Hewitt, John Thickstun, Christopher D. Manning, Percy Liang* · ([jiqizhixin](https://www.jiqizhixin.com/articles/2023-06-25-5)) · ([mp.weixin.qq](https://mp.weixin.qq.com/s?__biz=MzI1MjQ2OTQ3Ng==&mid=2247608689&idx=2&sn=ed29c1ee1f571f98b191805472feb79a))

## LLM detection
- **MarkLLM: An Open-Source Toolkit for LLM Watermarking**, `arXiv, 2405.10051`, [arxiv](http://arxiv.org/abs/2405.10051v1), [pdf](http://arxiv.org/pdf/2405.10051v1.pdf), cication: [**-1**](None)

	 *Leyi Pan, Aiwei Liu, Zhiwei He, Zitian Gao, Xuandong Zhao, Yijian Lu, Binglin Zhou, Shuliang Liu, Xuming Hu, Lijie Wen* · ([markllm](https://github.com/thu-bpm/markllm) - thu-bpm) ![Star](https://img.shields.io/github/stars/thu-bpm/markllm.svg?style=social&label=Star)
- **Min-K%++: Improved Baseline for Detecting Pre-Training Data from Large
  Language Models**, `arXiv, 2404.02936`, [arxiv](http://arxiv.org/abs/2404.02936v1), [pdf](http://arxiv.org/pdf/2404.02936v1.pdf), cication: [**-1**](None)

	 *Jingyang Zhang, Jingwei Sun, Eric Yeats, Yang Ouyang, Martin Kuo, Jianyi Zhang, Hao Yang, Hai Li* · ([zjysteven.github](https://zjysteven.github.io/mink-plus-plus/)) · ([mink-plus-plus](https://github.com/zjysteven/mink-plus-plus) - zjysteven) ![Star](https://img.shields.io/github/stars/zjysteven/mink-plus-plus.svg?style=social&label=Star)
- **Decoding the AI Pen: Techniques and Challenges in Detecting AI-Generated
  Text**, `arXiv, 2403.05750`, [arxiv](http://arxiv.org/abs/2403.05750v1), [pdf](http://arxiv.org/pdf/2403.05750v1.pdf), cication: [**-1**](None)

	 *Sara Abdali, Richard Anarfi, CJ Barberan, Jia He*
- [AI Watermarking 101: Tools and Techniques](https://huggingface.co/blog/watermarking)
- **Watermarking Makes Language Models Radioactive**, `arXiv, 2402.14904`, [arxiv](http://arxiv.org/abs/2402.14904v1), [pdf](http://arxiv.org/pdf/2402.14904v1.pdf), cication: [**-1**](None)

	 *Tom Sander, Pierre Fernandez, Alain Durmus, Matthijs Douze, Teddy Furon*
- **HuRef: HUman-REadable Fingerprint for Large Language Models**, `arXiv, 2312.04828`, [arxiv](http://arxiv.org/abs/2312.04828v1), [pdf](http://arxiv.org/pdf/2312.04828v1.pdf), cication: [**-1**](None)

	 *Boyi Zeng, Chenghu Zhou, Xinbing Wang, Zhouhan Lin* · ([jiqizhixin](https://www.jiqizhixin.com/articles/2024-02-02-4))
- [**LLM-generated-text-detection**](https://github.com/thunlp/LLM-generated-text-detection) - thunlp ![Star](https://img.shields.io/github/stars/thunlp/LLM-generated-text-detection.svg?style=social&label=Star)
- **Adaptive Text Watermark for Large Language Models**, `arXiv, 2401.13927`, [arxiv](http://arxiv.org/abs/2401.13927v1), [pdf](http://arxiv.org/pdf/2401.13927v1.pdf), cication: [**-1**](None)

	 *Yepeng Liu, Yuheng Bu*
- **Spotting LLMs With Binoculars: Zero-Shot Detection of Machine-Generated
  Text**, `arXiv, 2401.12070`, [arxiv](http://arxiv.org/abs/2401.12070v1), [pdf](http://arxiv.org/pdf/2401.12070v1.pdf), cication: [**-1**](None)

	 *Abhimanyu Hans, Avi Schwarzschild, Valeriia Cherepanova, Hamid Kazemi, Aniruddha Saha, Micah Goldblum, Jonas Geiping, Tom Goldstein*
- **LLM-as-a-Coauthor: The Challenges of Detecting LLM-Human Mixcase**, `arXiv, 2401.05952`, [arxiv](http://arxiv.org/abs/2401.05952v1), [pdf](http://arxiv.org/pdf/2401.05952v1.pdf), cication: [**-1**](None)

	 *Chujie Gao, Dongping Chen, Qihui Zhang, Yue Huang, Yao Wan, Lichao Sun* · ([MixSet](https://github.com/Dongping-Chen/MixSet) - Dongping-Chen) ![Star](https://img.shields.io/github/stars/Dongping-Chen/MixSet.svg?style=social&label=Star)
- **A Survey of Text Watermarking in the Era of Large Language Models**, `arXiv, 2312.07913`, [arxiv](http://arxiv.org/abs/2312.07913v4), [pdf](http://arxiv.org/pdf/2312.07913v4.pdf), cication: [**-1**](None)

	 *Aiwei Liu, Leyi Pan, Yijian Lu, Jingjing Li, Xuming Hu, Xi Zhang, Lijie Wen, Irwin King, Hui Xiong, Philip S. Yu* · ([jiqizhixin](https://www.jiqizhixin.com/articles/2024-01-26))
- **Fast-DetectGPT: Efficient Zero-Shot Detection of Machine-Generated Text
  via Conditional Probability Curvature**, `arXiv, 2310.05130`, [arxiv](http://arxiv.org/abs/2310.05130v2), [pdf](http://arxiv.org/pdf/2310.05130v2.pdf), cication: [**17**](https://scholar.google.com/scholar?cites=5808426526727311301&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Guangsheng Bao, Yanbin Zhao, Zhiyang Teng, Linyi Yang, Yue Zhang* · ([fast-detect-gpt](https://github.com/baoguangsheng/fast-detect-gpt?tab=readme-ov-file) - baoguangsheng) ![Star](https://img.shields.io/github/stars/baoguangsheng/fast-detect-gpt.svg?style=social&label=Star) · ([jiqizhixin](https://www.jiqizhixin.com/articles/2024-03-18-10))
- **Ghostbuster: Detecting Text Ghostwritten by Large Language Models**, `arXiv, 2305.15047`, [arxiv](http://arxiv.org/abs/2305.15047v2), [pdf](http://arxiv.org/pdf/2305.15047v2.pdf), cication: [**6**](https://scholar.google.com/scholar?cites=13263500511172823777&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Vivek Verma, Eve Fleisig, Nicholas Tomlin, Dan Klein* · ([bair.berkeley](https://bair.berkeley.edu/blog/2023/11/14/ghostbuster/))
- [‘ChatGPT detector’ catches AI-generated papers with unprecedented accuracy](https://www.nature.com/articles/d41586-023-03479-4)
- **GPT detectors are biased against non-native English writers**, `arXiv, 2304.02819`, [arxiv](http://arxiv.org/abs/2304.02819v3), [pdf](http://arxiv.org/pdf/2304.02819v3.pdf), cication: [**42**](https://scholar.google.com/scholar?cites=517481798147034142&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Weixin Liang, Mert Yuksekgonul, Yining Mao, Eric Wu, James Zou*
- **Can LLM-Generated Misinformation Be Detected?**, `arXiv, 2309.13788`, [arxiv](http://arxiv.org/abs/2309.13788v3), [pdf](http://arxiv.org/pdf/2309.13788v3.pdf), cication: [**-1**](None)

	 *Canyu Chen, Kai Shu* · ([llm-misinformation](https://github.com/llm-misinformation/llm-misinformation/) - llm-misinformation) ![Star](https://img.shields.io/github/stars/llm-misinformation/llm-misinformation.svg?style=social&label=Star)
- **Three Bricks to Consolidate Watermarks for Large Language Models**, `arXiv, 2308.00113`, [arxiv](http://arxiv.org/abs/2308.00113v2), [pdf](http://arxiv.org/pdf/2308.00113v2.pdf), cication: [**3**](https://scholar.google.com/scholar?cites=10192059197680159637&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Pierre Fernandez, Antoine Chaffin, Karim Tit, Vivien Chappelier, Teddy Furon*
- **Robust Distortion-free Watermarks for Language Models**, `arXiv, 2307.15593`, [arxiv](http://arxiv.org/abs/2307.15593v2), [pdf](http://arxiv.org/pdf/2307.15593v2.pdf), cication: [**9**](https://scholar.google.com/scholar?cites=8195690178514933158&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Rohith Kuditipudi, John Thickstun, Tatsunori Hashimoto, Percy Liang*
- **Can AI-Generated Text be Reliably Detected?**, `arXiv, 2303.11156`, [arxiv](http://arxiv.org/abs/2303.11156v2), [pdf](http://arxiv.org/pdf/2303.11156v2.pdf), cication: [**93**](https://scholar.google.com/scholar?cites=6956709800612024780&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Vinu Sankar Sadasivan, Aounon Kumar, Sriram Balasubramanian, Wenxiao Wang, Soheil Feizi* · ([mp.weixin.qq](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652358133&idx=4&sn=79515be33a6c66221480408134193581))
- [Digital tool spots academic text spawned by ChatGPT with 99% accuracy | The University of Kansas](http://today.ku.edu/2023/05/19/digital-tool-spots-academic-text-spawned-chatgpt-99-percent-accuracy)

	 · ([mp.weixin.qq](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652345558&idx=4&sn=95686435d5cad05d44cbe02bc26d6859))

## Interpretability 
- **Towards flexible perception with visual memory**, `arXiv, 2408.08172`, [arxiv](http://arxiv.org/abs/2408.08172v1), [pdf](http://arxiv.org/pdf/2408.08172v1.pdf), cication: [**-1**](None)

	 *Robert Geirhos, Priyank Jaini, Austin Stone, Sourabh Medapati, Xi Yi, George Toderici, Abhijit Ogale, Jonathon Shlens*
- **Does Liking Yellow Imply Driving a School Bus? Semantic Leakage in
  Language Models**, `arXiv, 2408.06518`, [arxiv](http://arxiv.org/abs/2408.06518v1), [pdf](http://arxiv.org/pdf/2408.06518v1.pdf), cication: [**-1**](None)

	 *Hila Gonen, Terra Blevins, Alisa Liu, Luke Zettlemoyer, Noah A. Smith*
- **Gemma Scope: Open Sparse Autoencoders Everywhere All At Once on Gemma 2**, `arXiv, 2408.05147`, [arxiv](http://arxiv.org/abs/2408.05147v1), [pdf](http://arxiv.org/pdf/2408.05147v1.pdf), cication: [**-1**](None)

	 *Tom Lieberum, Senthooran Rajamanoharan, Arthur Conmy, Lewis Smith, Nicolas Sonnerat, Vikrant Varma, János Kramár, Anca Dragan, Rohin Shah, Neel Nanda*
- **Transformer Explainer: Interactive Learning of Text-Generative Models**, `arXiv, 2408.04619`, [arxiv](http://arxiv.org/abs/2408.04619v1), [pdf](http://arxiv.org/pdf/2408.04619v1.pdf), cication: [**-1**](None)

	 *Aeree Cho, Grace C. Kim, Alexander Karpekov, Alec Helbling, Zijie J. Wang, Seongmin Lee, Benjamin Hoover, Duen Horng Chau* · ([poloclub.github](https://poloclub.github.io/transformer-explainer/))
- **Knowledge Mechanisms in Large Language Models: A Survey and Perspective**, `arXiv, 2407.15017`, [arxiv](http://arxiv.org/abs/2407.15017v1), [pdf](http://arxiv.org/pdf/2407.15017v1.pdf), cication: [**-1**](None)

	 *Mengru Wang, Yunzhi Yao, Ziwen Xu, Shuofei Qiao, Shumin Deng, Peng Wang, Xiang Chen, Jia-Chen Gu, Yong Jiang, Pengjun Xie*
- [Fetching Title#6ef9](https://icml.cc/virtual/2024/tutorial/35223)
- **Self-Recognition in Language Models**, `arXiv, 2407.06946`, [arxiv](http://arxiv.org/abs/2407.06946v1), [pdf](http://arxiv.org/pdf/2407.06946v1.pdf), cication: [**-1**](None)

	 *Tim R. Davidson, Viacheslav Surkov, Veniamin Veselovsky, Giuseppe Russo, Robert West, Caglar Gulcehre*
- **Do Vision and Language Models Share Concepts? A Vector Space Alignment
  Study**, `arXiv, 2302.06555`, [arxiv](http://arxiv.org/abs/2302.06555v2), [pdf](http://arxiv.org/pdf/2302.06555v2.pdf), cication: [**4**](https://scholar.google.com/scholar?cites=14801822930693996891&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Jiaang Li, Yova Kementchedjhieva, Constanza Fierro, Anders Søgaard* · ([VLCA](https://github.com/jiaangli/VLCA) - jiaangli) ![Star](https://img.shields.io/github/stars/jiaangli/VLCA.svg?style=social&label=Star)
- **Can LLMs Learn by Teaching? A Preliminary Study**, `arXiv, 2406.14629`, [arxiv](http://arxiv.org/abs/2406.14629v1), [pdf](http://arxiv.org/pdf/2406.14629v1.pdf), cication: [**-1**](None)

	 *Xuefei Ning, Zifu Wang, Shiyao Li, Zinan Lin, Peiran Yao, Tianyu Fu, Matthew B. Blaschko, Guohao Dai, Huazhong Yang, Yu Wang*

	 · ([lbt](https://github.com/imagination-research/lbt) - imagination-research) ![Star](https://img.shields.io/github/stars/imagination-research/lbt.svg?style=social&label=Star)
- **The Missing Curve Detectors of InceptionV1: Applying Sparse Autoencoders
  to InceptionV1 Early Vision**, `arXiv, 2406.03662`, [arxiv](http://arxiv.org/abs/2406.03662v1), [pdf](http://arxiv.org/pdf/2406.03662v1.pdf), cication: [**-1**](None)

	 *Liv Gorton*

	 · ([livgorton](https://livgorton.com/missing-curve-detectors/))
- **From RAGs to rich parameters: Probing how language models utilize
  external knowledge over parametric information for factual queries**, `arXiv, 2406.12824`, [arxiv](http://arxiv.org/abs/2406.12824v1), [pdf](http://arxiv.org/pdf/2406.12824v1.pdf), cication: [**-1**](None)

	 *Hitesh Wadhwa, Rahul Seetharaman, Somyaa Aggarwal, Reshmi Ghosh, Samyadeep Basu, Soundararajan Srinivasan, Wenlong Zhao, Shreyas Chaudhari, Ehsan Aghazadeh*
- **How Do Large Language Models Acquire Factual Knowledge During
  Pretraining?**, `arXiv, 2406.11813`, [arxiv](http://arxiv.org/abs/2406.11813v1), [pdf](http://arxiv.org/pdf/2406.11813v1.pdf), cication: [**-1**](None)

	 *Hoyeon Chang, Jinho Park, Seonghyeon Ye, Sohee Yang, Youngkyung Seo, Du-Seong Chang, Minjoon Seo*
- **How Do Large Language Models Acquire Factual Knowledge During
  Pretraining?**, `arXiv, 2406.11813`, [arxiv](http://arxiv.org/abs/2406.11813v1), [pdf](http://arxiv.org/pdf/2406.11813v1.pdf), cication: [**-1**](None)

	 *Hoyeon Chang, Jinho Park, Seonghyeon Ye, Sohee Yang, Youngkyung Seo, Du-Seong Chang, Minjoon Seo*
- [What Do Neural Networks Really Learn? Exploring the Brain of an AI Model - YouTube](https://www.youtube.com/watch?v=jGCvY4gNnA8&ab_channel=RationalAnimations)
- [Scaling interpretability - YouTube](https://www.youtube.com/watch?v=sQar5NNGbw4&ab_channel=Anthropic)
- **Large Language Model Confidence Estimation via Black-Box Access**, `arXiv, 2406.04370`, [arxiv](http://arxiv.org/abs/2406.04370v1), [pdf](http://arxiv.org/pdf/2406.04370v1.pdf), cication: [**-1**](None)

	 *Tejaswini Pedapati, Amit Dhurandhar, Soumya Ghosh, Soham Dan, Prasanna Sattigeri*
- [sparse-autoencoders.pdf](https://cdn.openai.com/papers/sparse-autoencoders.pdf)
- [llm.c by Hand](https://x.com/ProfTomYeh/status/1798042265883156651)
- **Not All Language Model Features Are Linear**, `arXiv, 2405.14860`, [arxiv](http://arxiv.org/abs/2405.14860v1), [pdf](http://arxiv.org/pdf/2405.14860v1.pdf), cication: [**-1**](None)

	 *Joshua Engels, Isaac Liao, Eric J. Michaud, Wes Gurnee, Max Tegmark*
- **Your Transformer is Secretly Linear**, `arXiv, 2405.12250`, [arxiv](http://arxiv.org/abs/2405.12250v1), [pdf](http://arxiv.org/pdf/2405.12250v1.pdf), cication: [**-1**](None)

	 *Anton Razzhigaev, Matvey Mikhalchuk, Elizaveta Goncharova, Nikolai Gerasimenko, Ivan Oseledets, Denis Dimitrov, Andrey Kuznetsov*
- [Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet](https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html)

	 · ([anthropic](https://www.anthropic.com/research/mapping-mind-language-model?utm_source=ainews&utm_medium=email&utm_campaign=ainews-anthropic-cracks-the-llm-genome-project))
- **The Platonic Representation Hypothesis**, `arXiv, 2405.07987`, [arxiv](http://arxiv.org/abs/2405.07987v1), [pdf](http://arxiv.org/pdf/2405.07987v1.pdf), cication: [**-1**](None)

	 *Minyoung Huh, Brian Cheung, Tongzhou Wang, Phillip Isola* · ([platonic-rep](https://github.com/minyoungg/platonic-rep?tab=readme-ov-file) - minyoungg) ![Star](https://img.shields.io/github/stars/minyoungg/platonic-rep.svg?style=social&label=Star) · ([phillipi.github](https://phillipi.github.io/prh/))
- **Fishing for Magikarp: Automatically Detecting Under-trained Tokens in
  Large Language Models**, `arXiv, 2405.05417`, [arxiv](http://arxiv.org/abs/2405.05417v1), [pdf](http://arxiv.org/pdf/2405.05417v1.pdf), cication: [**-1**](None)

	 *Sander Land, Max Bartolo* · ([magikarp](https://github.com/cohere-ai/magikarp/) - cohere-ai) ![Star](https://img.shields.io/github/stars/cohere-ai/magikarp.svg?style=social&label=Star)
- **A Primer on the Inner Workings of Transformer-based Language Models**, `arXiv, 2405.00208`, [arxiv](http://arxiv.org/abs/2405.00208v2), [pdf](http://arxiv.org/pdf/2405.00208v2.pdf), cication: [**-1**](None)

	 *Javier Ferrando, Gabriele Sarti, Arianna Bisazza, Marta R. Costa-jussà*
- **Understanding Emergent Abilities of Language Models from the Loss
  Perspective**, `arXiv, 2403.15796`, [arxiv](http://arxiv.org/abs/2403.15796v2), [pdf](http://arxiv.org/pdf/2403.15796v2.pdf), cication: [**-1**](None)

	 *Zhengxiao Du, Aohan Zeng, Yuxiao Dong, Jie Tang*
- [Circuits Updates - April 2024](https://transformer-circuits.pub/2024/april-update/index.html#scaling-laws)
- **Transformers Can Represent $n$-gram Language Models**, `arXiv, 2404.14994`, [arxiv](http://arxiv.org/abs/2404.14994v1), [pdf](http://arxiv.org/pdf/2404.14994v1.pdf), cication: [**-1**](None)

	 *Anej Svete, Ryan Cotterell*
- **A Multimodal Automated Interpretability Agent**, `arXiv, 2404.14394`, [arxiv](http://arxiv.org/abs/2404.14394v1), [pdf](http://arxiv.org/pdf/2404.14394v1.pdf), cication: [**-1**](None)

	 *Tamar Rott Shaham, Sarah Schwettmann, Franklin Wang, Achyuta Rajaram, Evan Hernandez, Jacob Andreas, Antonio Torralba*
- [**llm-transparency-tool**](https://github.com/facebookresearch/llm-transparency-tool) - facebookresearch ![Star](https://img.shields.io/github/stars/facebookresearch/llm-transparency-tool.svg?style=social&label=Star)
- **Compression Represents Intelligence Linearly**, `arXiv, 2404.09937`, [arxiv](http://arxiv.org/abs/2404.09937v1), [pdf](http://arxiv.org/pdf/2404.09937v1.pdf), cication: [**-1**](None)

	 *Yuzhen Huang, Jinghan Zhang, Zifei Shan, Junxian He*

	 · ([huggingface](https://huggingface.co/datasets/hkust-nlp/llm-compression)) · ([llm-compression-intelligence](https://github.com/hkust-nlp/llm-compression-intelligence?tab=readme-ov-file) - hkust-nlp) ![Star](https://img.shields.io/github/stars/hkust-nlp/llm-compression-intelligence.svg?style=social&label=Star)
- [**color-coded-text-generation**](https://huggingface.co/spaces/joaogante/color-coded-text-generation) - joaogante 🤗
- **LVLM-Intrepret: An Interpretability Tool for Large Vision-Language
  Models**, `arXiv, 2404.03118`, [arxiv](http://arxiv.org/abs/2404.03118v1), [pdf](http://arxiv.org/pdf/2404.03118v1.pdf), cication: [**-1**](None)

	 *Gabriela Ben Melech Stan, Raanan Yehezkel Rohekar, Yaniv Gurwicz, Matthew Lyle Olson, Anahita Bhiwandiwalla, Estelle Aflalo, Chenfei Wu, Nan Duan, Shao-Yen Tseng, Vasudev Lal*
- **Understanding Emergent Abilities of Language Models from the Loss
  Perspective**, `arXiv, 2403.15796`, [arxiv](http://arxiv.org/abs/2403.15796v2), [pdf](http://arxiv.org/pdf/2403.15796v2.pdf), cication: [**-1**](None)

	 *Zhengxiao Du, Aohan Zeng, Yuxiao Dong, Jie Tang*
- **Source-Aware Training Enables Knowledge Attribution in Language Models**, `arXiv, 2404.01019`, [arxiv](http://arxiv.org/abs/2404.01019v1), [pdf](http://arxiv.org/pdf/2404.01019v1.pdf), cication: [**-1**](None)

	 *Muhammad Khalifa, David Wadden, Emma Strubell, Honglak Lee, Lu Wang, Iz Beltagy, Hao Peng*
- **Future Lens: Anticipating Subsequent Tokens from a Single Hidden State**, `arXiv, 2311.04897`, [arxiv](http://arxiv.org/abs/2311.04897v1), [pdf](http://arxiv.org/pdf/2311.04897v1.pdf), cication: [**-1**](None)

	 *Koyena Pal, Jiuding Sun, Andrew Yuan, Byron C. Wallace, David Bau*
- **Localizing Paragraph Memorization in Language Models**, `arXiv, 2403.19851`, [arxiv](http://arxiv.org/abs/2403.19851v1), [pdf](http://arxiv.org/pdf/2403.19851v1.pdf), cication: [**-1**](None)

	 *Niklas Stoehr, Mitchell Gordon, Chiyuan Zhang, Owen Lewis*
- [SAE-VIS: Announcement Post — LessWrong](https://www.lesswrong.com/posts/nAhy6ZquNY7AD3RkD/sae-vis-announcement-post-1)
- [Circuits Updates - March 2024](https://transformer-circuits.pub/2024/march-update/index.html#external-research)
- **pyvene: A Library for Understanding and Improving PyTorch Models via
  Interventions**, `arXiv, 2403.07809`, [arxiv](http://arxiv.org/abs/2403.07809v1), [pdf](http://arxiv.org/pdf/2403.07809v1.pdf), cication: [**-1**](None)

	 *Zhengxuan Wu, Atticus Geiger, Aryaman Arora, Jing Huang, Zheng Wang, Noah D. Goodman, Christopher D. Manning, Christopher Potts* · ([pyvene](https://github.com/stanfordnlp/pyvene) - stanfordnlp) ![Star](https://img.shields.io/github/stars/stanfordnlp/pyvene.svg?style=social&label=Star)
- [**transformer-debugger**](https://github.com/openai/transformer-debugger) - openai ![Star](https://img.shields.io/github/stars/openai/transformer-debugger.svg?style=social&label=Star)

	 · ([jiqizhixin](https://www.jiqizhixin.com/articles/2024-03-12-3))
- **Logits of API-Protected LLMs Leak Proprietary Information**, `arXiv, 2403.09539`, [arxiv](http://arxiv.org/abs/2403.09539v2), [pdf](http://arxiv.org/pdf/2403.09539v2.pdf), cication: [**-1**](None)

	 *Matthew Finlayson, Xiang Ren, Swabha Swayamdipta*

	 · ([qbitai](https://www.qbitai.com/2024/03/128307.html))
- **Stealing Part of a Production Language Model**, `arXiv, 2403.06634`, [arxiv](http://arxiv.org/abs/2403.06634v1), [pdf](http://arxiv.org/pdf/2403.06634v1.pdf), cication: [**-1**](None)

	 *Nicholas Carlini, Daniel Paleka, Krishnamurthy Dj Dvijotham, Thomas Steinke, Jonathan Hayase, A. Feder Cooper, Katherine Lee, Matthew Jagielski, Milad Nasr, Arthur Conmy*

	 · ([qbitai](https://www.qbitai.com/2024/03/127168.html))
	- `extracting information from black-box language models like OpenAI's ChatGPT and Google's PaLM-2 (revealing for the first time the hidden dimensions of these models)`
- [Reflections on Qualitative Research](https://transformer-circuits.pub/2024/qualitative-essay/index.html)
- [Claude-3's uncanny "awareness"](https://twitter.com/DrJimFan/status/1765076396404363435)
- **AtP*: An efficient and scalable method for localizing LLM behaviour to
  components**, `arXiv, 2403.00745`, [arxiv](http://arxiv.org/abs/2403.00745v1), [pdf](http://arxiv.org/pdf/2403.00745v1.pdf), cication: [**-1**](None)

	 *János Kramár, Tom Lieberum, Rohin Shah, Neel Nanda*
- [Circuits Updates - February 2024](https://transformer-circuits.pub/2024/feb-update/index.html)
- **A phase transition between positional and semantic learning in a
  solvable model of dot-product attention**, `arXiv, 2402.03902`, [arxiv](http://arxiv.org/abs/2402.03902v1), [pdf](http://arxiv.org/pdf/2402.03902v1.pdf), cication: [**-1**](None)

	 *Hugo Cui, Freya Behrens, Florent Krzakala, Lenka Zdeborová*
- [**fractal**](https://github.com/sohl-dickstein/fractal) - sohl-dickstein ![Star](https://img.shields.io/github/stars/sohl-dickstein/fractal.svg?style=social&label=Star)

	 *The boundary of neural network trainability is fractal*
- **Rethinking Interpretability in the Era of Large Language Models**, `arXiv, 2402.01761`, [arxiv](http://arxiv.org/abs/2402.01761v1), [pdf](http://arxiv.org/pdf/2402.01761v1.pdf), cication: [**-1**](None)

	 *Chandan Singh, Jeevana Priya Inala, Michel Galley, Rich Caruana, Jianfeng Gao*
- **Can Large Language Models Understand Context?**, `arXiv, 2402.00858`, [arxiv](http://arxiv.org/abs/2402.00858v1), [pdf](http://arxiv.org/pdf/2402.00858v1.pdf), cication: [**-1**](None)

	 *Yilun Zhu, Joel Ruben Antony Moniz, Shruti Bhargava, Jiarui Lu, Dhivya Piraviperumal, Site Li, Yuan Zhang, Hong Yu, Bo-Hsiang Tseng*
- [Circuits Updates - January 2024](https://transformer-circuits.pub/2024/jan-update/index.html)
- **Patchscope: A Unifying Framework for Inspecting Hidden Representations
  of Language Models**, `arXiv, 2401.06102`, [arxiv](http://arxiv.org/abs/2401.06102v1), [pdf](http://arxiv.org/pdf/2401.06102v1.pdf), cication: [**-1**](None)

	 *Asma Ghandeharioun, Avi Caciularu, Adam Pearce, Lucas Dixon, Mor Geva*
- [Vayu Robotics Blog - Interpretable End-to-End Robot Navigation](https://www.vayurobotics.com/blog/interpretable-end-to-end-driving-agent-for-robotic-mobility)
- [Fact Finding: Attempting to Reverse-Engineer Factual Recall on the Neuron Level (Post 1) — AI Alignment Forum](https://www.alignmentforum.org/s/hpWHhjvjn67LJ4xXX/p/iGuwZTHWb6DFY3sKB)
- [deep learning does approximate Solomonoff induction](https://twitter.com/johnschulman2/status/1741178475946602979)
- [**awesome-llm-interpretability**](https://github.com/JShollaj/awesome-llm-interpretability) - JShollaj ![Star](https://img.shields.io/github/stars/JShollaj/awesome-llm-interpretability.svg?style=social&label=Star)

	 *A curated list of Large Language Model (LLM) Interpretability resources.*

- [Site Unreachable](https://windowsontheory.org/2023/12/22/emergent-abilities-and-grokking-fundamental-mirage-or-both/)
- **Challenges with unsupervised LLM knowledge discovery**, `arXiv, 2312.10029`, [arxiv](http://arxiv.org/abs/2312.10029v1), [pdf](http://arxiv.org/pdf/2312.10029v1.pdf), cication: [**-1**](None)

	 *Sebastian Farquhar, Vikrant Varma, Zachary Kenton, Johannes Gasteiger, Vladimir Mikulik, Rohin Shah*
- **Using Captum to Explain Generative Language Models**, `arXiv, 2312.05491`, [arxiv](http://arxiv.org/abs/2312.05491v1), [pdf](http://arxiv.org/pdf/2312.05491v1.pdf), cication: [**-1**](None)

	 *Vivek Miglani, Aobo Yang, Aram H. Markosyan, Diego Garcia-Olano, Narine Kokhlikyan*
- **Beyond Surface: Probing LLaMA Across Scales and Layers**, `arXiv, 2312.04333`, [arxiv](http://arxiv.org/abs/2312.04333v1), [pdf](http://arxiv.org/pdf/2312.04333v1.pdf), cication: [**-1**](None)

	 *Nuo Chen, Ning Wu, Shining Liang, Ming Gong, Linjun Shou, Dongmei Zhang, Jia Li*
- [**llm-viz**](https://github.com/bbycroft/llm-viz) - bbycroft ![Star](https://img.shields.io/github/stars/bbycroft/llm-viz.svg?style=social&label=Star)

	 *3D Visualization of an GPT-style LLM*
- **White-Box Transformers via Sparse Rate Reduction: Compression Is All
  There Is?**, `arXiv, 2311.13110`, [arxiv](http://arxiv.org/abs/2311.13110v2), [pdf](http://arxiv.org/pdf/2311.13110v2.pdf), cication: [**-1**](None)

	 *Yaodong Yu, Sam Buchanan, Druv Pai, Tianzhe Chu, Ziyang Wu, Shengbang Tong, Hao Bai, Yuexiang Zhai, Benjamin D. Haeffele, Yi Ma* · ([mp.weixin.qq](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652411597&idx=1&sn=a110c3008a6725d424c9baff337cdb16&poc_token=HEt8Y2WjW94eS5ngCe9T1PO8B0ABkk44PoUjOvZj))
- **Pretraining Data Mixtures Enable Narrow Model Selection Capabilities in
  Transformer Models**, `arXiv, 2311.00871`, [arxiv](http://arxiv.org/abs/2311.00871v1), [pdf](http://arxiv.org/pdf/2311.00871v1.pdf), cication: [**-1**](None)

	 *Steve Yadlowsky, Lyric Doshi, Nilesh Tripuraneni* · ([jiqizhixin](https://www.jiqizhixin.com/articles/2023-11-07-6))
- **The Generative AI Paradox: "What It Can Create, It May Not Understand"**, `arXiv, 2311.00059`, [arxiv](http://arxiv.org/abs/2311.00059v1), [pdf](http://arxiv.org/pdf/2311.00059v1.pdf), cication: [**-1**](None)

	 *Peter West, Ximing Lu, Nouha Dziri, Faeze Brahman, Linjie Li, Jena D. Hwang, Liwei Jiang, Jillian Fisher, Abhilasha Ravichander, Khyathi Chandu*
- **The Impact of Depth and Width on Transformer Language Model
  Generalization**, `arXiv, 2310.19956`, [arxiv](http://arxiv.org/abs/2310.19956v1), [pdf](http://arxiv.org/pdf/2310.19956v1.pdf), cication: [**-1**](None)

	 *Jackson Petty, Sjoerd van Steenkiste, Ishita Dasgupta, Fei Sha, Dan Garrette, Tal Linzen*
- **Can Large Language Models Explain Themselves? A Study of LLM-Generated
  Self-Explanations**, `arXiv, 2310.11207`, [arxiv](http://arxiv.org/abs/2310.11207v1), [pdf](http://arxiv.org/pdf/2310.11207v1.pdf), cication: [**-1**](None)

	 *Shiyuan Huang, Siddarth Mamidanna, Shreedhar Jangam, Yilun Zhou, Leilani H. Gilpin*
- [Towards Monosemanticity: Decomposing Language Models With Dictionary Learning](https://transformer-circuits.pub/2023/monosemantic-features/index.html)

	 · ([qbitai](https://www.qbitai.com/2023/10/87969.html))
- **Representation Engineering: A Top-Down Approach to AI Transparency**, `arXiv, 2310.01405`, [arxiv](http://arxiv.org/abs/2310.01405v3), [pdf](http://arxiv.org/pdf/2310.01405v3.pdf), cication: [**5**](https://scholar.google.com/scholar?cites=7486178775253953945&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski* · ([representation-engineering](https://github.com/andyzoujm/representation-engineering) - andyzoujm) ![Star](https://img.shields.io/github/stars/andyzoujm/representation-engineering.svg?style=social&label=Star) · ([mp.weixin.qq](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652386851&idx=1&sn=0c828b9880d272e367c5810f82120bdc))
- **Attention Satisfies: A Constraint-Satisfaction Lens on Factual Errors of
  Language Models**, `arXiv, 2309.15098`, [arxiv](http://arxiv.org/abs/2309.15098v1), [pdf](http://arxiv.org/pdf/2309.15098v1.pdf), cication: [**-1**](None)

	 *Mert Yuksekgonul, Varun Chandrasekaran, Erik Jones, Suriya Gunasekar, Ranjita Naik, Hamid Palangi, Ece Kamar, Besmira Nushi*
- **Language Modeling Is Compression**, `arXiv, 2309.10668`, [arxiv](http://arxiv.org/abs/2309.10668v1), [pdf](http://arxiv.org/pdf/2309.10668v1.pdf), cication: [**7**](https://scholar.google.com/scholar?cites=8098408536892148709&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Grégoire Delétang, Anian Ruoss, Paul-Ambroise Duquenne, Elliot Catt, Tim Genewein, Christopher Mattern, Jordi Grau-Moya, Li Kevin Wenliang, Matthew Aitchison, Laurent Orseau*
- **Sorted LLaMA: Unlocking the Potential of Intermediate Layers of Large
  Language Models for Dynamic Inference Using Sorted Fine-Tuning (SoFT)**, `arXiv, 2309.08968`, [arxiv](http://arxiv.org/abs/2309.08968v1), [pdf](http://arxiv.org/pdf/2309.08968v1.pdf), cication: [**-1**](None)

	 *Parsa Kavehzadeh, Mojtaba Valipour, Marzieh Tahaei, Ali Ghodsi, Boxing Chen, Mehdi Rezagholizadeh*
- **Sparse Autoencoders Find Highly Interpretable Features in Language
  Models**, `arXiv, 2309.08600`, [arxiv](http://arxiv.org/abs/2309.08600v3), [pdf](http://arxiv.org/pdf/2309.08600v3.pdf), cication: [**5**](https://scholar.google.com/scholar?cites=3171773312943220036&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Hoagy Cunningham, Aidan Ewart, Logan Riggs, Robert Huben, Lee Sharkey*
- [Human Language Understanding & Reasoning](https://direct.mit.edu/daed/article/151/2/127/110621/Human-Language-Understanding-amp-Reasoning)

	 · ([mp.weixin.qq](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652372005&idx=2&sn=9d5025db51ff939bf39e8cf861938a34))
- [Do Machine Learning Models Memorize or Generalize?](https://pair.withgoogle.com/explorables/grokking/)

	 · ([mp.weixin.qq](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652371869&idx=5&sn=e05c029912da214a6592d9e5f2418aa9))
- [**CIMI**](https://github.com/Daftstone/CIMI) - Daftstone ![Star](https://img.shields.io/github/stars/Daftstone/CIMI.svg?style=social&label=Star)

	 · ([jiqizhixin](https://www.jiqizhixin.com/articles/2023-08-11-5))
- [Do Machine Learning Models Memorize or Generalize?](https://pair.withgoogle.com/explorables/grokking/)

	 · ([qbitai](https://www.qbitai.com/2023/08/76083.html))
- **Studying Large Language Model Generalization with Influence Functions**, `arXiv, 2308.03296`, [arxiv](http://arxiv.org/abs/2308.03296v1), [pdf](http://arxiv.org/pdf/2308.03296v1.pdf), cication: [**12**](https://scholar.google.com/scholar?cites=4154155767169928682&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Roger Grosse, Juhan Bae, Cem Anil, Nelson Elhage, Alex Tamkin, Amirhossein Tajdini, Benoit Steiner, Dustin Li, Esin Durmus, Ethan Perez*
- [Can foundation models label data like humans?](https://huggingface.co/blog/llm-leaderboard)
- **Scan and Snap: Understanding Training Dynamics and Token Composition in
  1-layer Transformer**, `arXiv, 2305.16380`, [arxiv](http://arxiv.org/abs/2305.16380v4), [pdf](http://arxiv.org/pdf/2305.16380v4.pdf), cication: [**6**](https://scholar.google.com/scholar?cites=10559864520549789725&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Yuandong Tian, Yiping Wang, Beidi Chen, Simon Du* · ([mp.weixin.qq](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652339033&idx=3&sn=7c45e1f38e8ce0bd91869ac2a47078fa))
- **Label Words are Anchors: An Information Flow Perspective for
  Understanding In-Context Learning**, `arXiv, 2305.14160`, [arxiv](http://arxiv.org/abs/2305.14160v2), [pdf](http://arxiv.org/pdf/2305.14160v2.pdf), cication: [**-1**](None)

	 *Lean Wang, Lei Li, Damai Dai, Deli Chen, Hao Zhou, Fandong Meng, Jie Zhou, Xu Sun* · ([qbitai](https://www.qbitai.com/2023/12/105631.html))
- **How Much Does Attention Actually Attend? Questioning the Importance of
  Attention in Pretrained Transformers**, `arXiv, 2211.03495`, [arxiv](http://arxiv.org/abs/2211.03495v1), [pdf](http://arxiv.org/pdf/2211.03495v1.pdf), cication: [**16**](https://scholar.google.com/scholar?cites=2519257119659989818&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Michael Hassid, Hao Peng, Daniel Rotem, Jungo Kasai, Ivan Montero, Noah A. Smith, Roy Schwartz*

## Generaliazation
- **Time is Encoded in the Weights of Finetuned Language Models**, `arXiv, 2312.13401`, [arxiv](http://arxiv.org/abs/2312.13401v1), [pdf](http://arxiv.org/pdf/2312.13401v1.pdf), cication: [**-1**](None)

	 *Kai Nylund, Suchin Gururangan, Noah A. Smith*
- **Pretraining Data Mixtures Enable Narrow Model Selection Capabilities in
  Transformer Models**, `arXiv, 2311.00871`, [arxiv](http://arxiv.org/abs/2311.00871v1), [pdf](http://arxiv.org/pdf/2311.00871v1.pdf), cication: [**-1**](None)

	 *Steve Yadlowsky, Lyric Doshi, Nilesh Tripuraneni*

## LLM editting
- **Model Surgery: Modulating LLM's Behavior Via Simple Parameter Editing**, `arXiv, 2407.08770`, [arxiv](http://arxiv.org/abs/2407.08770v1), [pdf](http://arxiv.org/pdf/2407.08770v1.pdf), cication: [**-1**](None)

	 *Huanqian Wang, Yang Yue, Rui Lu, Jingxin Shi, Andrew Zhao, Shenzhi Wang, Shiji Song, Gao Huang*
- **Composable Interventions for Language Models**, `arXiv, 2407.06483`, [arxiv](http://arxiv.org/abs/2407.06483v1), [pdf](http://arxiv.org/pdf/2407.06483v1.pdf), cication: [**-1**](None)

	 *Arinbjorn Kolbeinsson, Kyle O'Brien, Tianjin Huang, Shanghua Gao, Shiwei Liu, Jonathan Richard Schwarz, Anurag Vaidya, Faisal Mahmood, Marinka Zitnik, Tianlong Chen*
- **Breaking Boundaries: Investigating the Effects of Model Editing on
  Cross-linguistic Performance**, `arXiv, 2406.11139`, [arxiv](http://arxiv.org/abs/2406.11139v1), [pdf](http://arxiv.org/pdf/2406.11139v1.pdf), cication: [**-1**](None)

	 *Somnath Banerjee, Avik Halder, Rajarshi Mandal, Sayan Layek, Ian Soboroff, Rima Hazra, Animesh Mukherjee*
- **Is Bigger Edit Batch Size Always Better? -- An Empirical Study on Model
  Editing with Llama-3**, `arXiv, 2405.00664`, [arxiv](http://arxiv.org/abs/2405.00664v1), [pdf](http://arxiv.org/pdf/2405.00664v1.pdf), cication: [**-1**](None)

	 *Junsang Yoon, Akshat Gupta, Gopala Anumanchipalli*
- **Robust and Scalable Model Editing for Large Language Models**, `arXiv, 2403.17431`, [arxiv](http://arxiv.org/abs/2403.17431v1), [pdf](http://arxiv.org/pdf/2403.17431v1.pdf), cication: [**-1**](None)

	 *Yingfa Chen, Zhengyan Zhang, Xu Han, Chaojun Xiao, Zhiyuan Liu, Chen Chen, Kuai Li, Tao Yang, Maosong Sun* · ([EREN](https://github.com/thunlp/EREN?tab=readme-ov-file) - thunlp) ![Star](https://img.shields.io/github/stars/thunlp/EREN.svg?style=social&label=Star)
- **Editing Conceptual Knowledge for Large Language Models**, `arXiv, 2403.06259`, [arxiv](http://arxiv.org/abs/2403.06259v1), [pdf](http://arxiv.org/pdf/2403.06259v1.pdf), cication: [**-1**](None)

	 *Xiaohan Wang, Shengyu Mao, Ningyu Zhang, Shumin Deng, Yunzhi Yao, Yue Shen, Lei Liang, Jinjie Gu, Huajun Chen*

	 · ([zjukg](https://www.zjukg.org/project/ConceptEdit/)) · ([EasyEdit](https://github.com/zjunlp/EasyEdit) - zjunlp) ![Star](https://img.shields.io/github/stars/zjunlp/EasyEdit.svg?style=social&label=Star)
- **A Comprehensive Study of Knowledge Editing for Large Language Models**, `arXiv, 2401.01286`, [arxiv](http://arxiv.org/abs/2401.01286v2), [pdf](http://arxiv.org/pdf/2401.01286v2.pdf), cication: [**-1**](None)

	 *Ningyu Zhang, Yunzhi Yao, Bozhong Tian, Peng Wang, Shumin Deng, Mengru Wang, Zekun Xi, Shengyu Mao, Jintian Zhang, Yuansheng Ni*
- **Evaluating the Ripple Effects of Knowledge Editing in Language Models**, `arXiv, 2307.12976`, [arxiv](http://arxiv.org/abs/2307.12976v1), [pdf](http://arxiv.org/pdf/2307.12976v1.pdf), cication: [**5**](https://scholar.google.com/scholar?cites=3039858131654435599&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Roi Cohen, Eden Biran, Ori Yoran, Amir Globerson, Mor Geva*
- **Editing Large Language Models: Problems, Methods, and Opportunities**, `arXiv, 2305.13172`, [arxiv](http://arxiv.org/abs/2305.13172v2), [pdf](http://arxiv.org/pdf/2305.13172v2.pdf), cication: [**12**](https://scholar.google.com/scholar?cites=15387184595402526264&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Yunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng, Zhoubo Li, Shumin Deng, Huajun Chen, Ningyu Zhang* · ([easyedit](https://github.com/zjunlp/easyedit) - zjunlp) ![Star](https://img.shields.io/github/stars/zjunlp/easyedit.svg?style=social&label=Star)
- [**ModelEditingPapers**](https://github.com/zjunlp/ModelEditingPapers) - zjunlp ![Star](https://img.shields.io/github/stars/zjunlp/ModelEditingPapers.svg?style=social&label=Star)

	 *Must-read Papers on Model Editing.*

## AGI insights
- **Open-Endedness is Essential for Artificial Superhuman Intelligence**, `arXiv, 2406.04268`, [arxiv](http://arxiv.org/abs/2406.04268v1), [pdf](http://arxiv.org/pdf/2406.04268v1.pdf), cication: [**-1**](None)

	 *Edward Hughes, Michael Dennis, Jack Parker-Holder, Feryal Behbahani, Aditi Mavalankar, Yuge Shi, Tom Schaul, Tim Rocktaschel*
- **Artificial Generational Intelligence: Cultural Accumulation in
  Reinforcement Learning**, `arXiv, 2406.00392`, [arxiv](http://arxiv.org/abs/2406.00392v1), [pdf](http://arxiv.org/pdf/2406.00392v1.pdf), cication: [**-1**](None)

	 *Jonathan Cook, Chris Lu, Edward Hughes, Joel Z. Leibo, Jakob Foerster*
- [My AI Timelines Have Sped Up (Again)](https://www.alexirpan.com/2024/01/10/ai-timelines-2024.html)

	 · ([mp.weixin.qq](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652453308&idx=2&sn=9101acc27aeff38e2f6f732357011b9c))
- **Computing Power and the Governance of Artificial Intelligence**, `arXiv, 2402.08797`, [arxiv](http://arxiv.org/abs/2402.08797v1), [pdf](http://arxiv.org/pdf/2402.08797v1.pdf), cication: [**-1**](None)

	 *Girish Sastry, Lennart Heim, Haydn Belfield, Markus Anderljung, Miles Brundage, Julian Hazell, Cullen O'Keefe, Gillian K. Hadfield, Richard Ngo, Konstantin Pilz*
- [Self-driving as a case study for AGI](https://web.archive.org/web/20240122062223/https://karpathy.github.io/2024/01/21/selfdriving-agi/)
- **Perspectives on the State and Future of Deep Learning -- 2023**, `arXiv, 2312.09323`, [arxiv](http://arxiv.org/abs/2312.09323v1), [pdf](http://arxiv.org/pdf/2312.09323v1.pdf), cication: [**-1**](None)

	 *Micah Goldblum, Anima Anandkumar, Richard Baraniuk, Tom Goldstein, Kyunghyun Cho, Zachary C Lipton, Melanie Mitchell, Preetum Nakkiran, Max Welling, Andrew Gordon Wilson*
- [AI and Open Source in 2023 - by Sebastian Raschka, PhD](https://magazine.sebastianraschka.com/p/ai-and-open-source-in-2023)

	 · ([mp.weixin.qq](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652414828&idx=4&sn=7780ba9182ef40d0dac9b076a2d40003))
- [Some intuitions about large language models](https://docs.google.com/presentation/d/1hQUd3pF8_2Gr2Obc89LKjmHL0DlH-uof9M0yFVd3FA4/edit#slide=id.g16197112905_0_0)
- [Role play with large language models | Nature](https://www.nature.com/articles/s41586-023-06647-8)

	 · ([qbitai](https://www.qbitai.com/2023/11/99062.html))
- **Levels of AGI: Operationalizing Progress on the Path to AGI**, `arXiv, 2311.02462`, [arxiv](http://arxiv.org/abs/2311.02462v1), [pdf](http://arxiv.org/pdf/2311.02462v1.pdf), cication: [**-1**](None)

	 *Meredith Ringel Morris, Jascha Sohl-dickstein, Noah Fiedel, Tris Warkentin, Allan Dafoe, Aleksandra Faust, Clement Farabet, Shane Legg*
- **Consciousness in Artificial Intelligence: Insights from the Science of
  Consciousness**, `arXiv, 2308.08708`, [arxiv](http://arxiv.org/abs/2308.08708v3), [pdf](http://arxiv.org/pdf/2308.08708v3.pdf), cication: [**15**](https://scholar.google.com/scholar?cites=8239061011717183910&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Patrick Butlin, Robert Long, Eric Elmoznino, Yoshua Bengio, Jonathan Birch, Axel Constant, George Deane, Stephen M. Fleming, Chris Frith, Xu Ji* · ([jiqizhixin](https://www.jiqizhixin.com/articles/2023-08-22-4))
- [Collective Intelligence for Deep Learning: A Survey of Recent Developments | 大トロ](https://blog.otoro.net/2022/10/01/collectiveintelligence/)

---
- [融合RL与LLM思想，探寻世界模型以迈向AGI「中·下篇」](https://mp.weixin.qq.com/s?__biz=MzAxMTk4NDkwNw==&mid=2247495832&idx=3&sn=5eff30cfeb95402ba39ab27158f6a9da)
- [融合RL与LLM思想，探寻世界模型以迈向AGI「上篇」](https://mp.weixin.qq.com/s?__biz=MzAxMTk4NDkwNw==&mid=2247495832&idx=2&sn=423e09552e24520af132c0e58fb9835c)
- [强化学习之父Richard Sutton：通往AGI的另一种可能](https://mp.weixin.qq.com/s?__biz=MzAxMTk4NDkwNw==&mid=2247495731&idx=1&sn=c00430a203adb5e89cab3c036b62ee5e)
- [图灵奖得主、神经网络之父Hinton最新公开演讲：数字智能会取代生物智能吗？（全文及PPT）](https://mp.weixin.qq.com/s?__biz=MzIyMzk1MDE3Nw==&mid=2247629387&idx=2&sn=fe485c8faa0cc42b23d1a91ded7cdb66)
- [好问题比好答案更重要｜沈向洋大模型五问](https://mp.weixin.qq.com/s?__biz=MzAxMTk4NDkwNw==&mid=2247494275&idx=1&sn=4860205e6043435fe7ff871c8fb4c9c1)

## Callibration
- **Llamas Know What GPTs Don't Show: Surrogate Models for Confidence
  Estimation**, `arXiv, 2311.08877`, [arxiv](http://arxiv.org/abs/2311.08877v1), [pdf](http://arxiv.org/pdf/2311.08877v1.pdf), cication: [**-1**](None)

	 *Vaishnavi Shrivastava, Percy Liang, Ananya Kumar*
- **Do Large Language Models Know What They Don't Know?**, `arXiv, 2305.18153`, [arxiv](http://arxiv.org/abs/2305.18153v2), [pdf](http://arxiv.org/pdf/2305.18153v2.pdf), cication: [**16**](https://scholar.google.com/scholar?cites=18069947573459721243&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Zhangyue Yin, Qiushi Sun, Qipeng Guo, Jiawen Wu, Xipeng Qiu, Xuanjing Huang*

## Tokenization
- **Data Mixture Inference: What do BPE Tokenizers Reveal about their
  Training Data?**, `arXiv, 2407.16607`, [arxiv](http://arxiv.org/abs/2407.16607v2), [pdf](http://arxiv.org/pdf/2407.16607v2.pdf), cication: [**-1**](None)

	 *Jonathan Hayase, Alisa Liu, Yejin Choi, Sewoong Oh, Noah A. Smith*
- **Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies**, `arXiv, 2407.13623`, [arxiv](http://arxiv.org/abs/2407.13623v2), [pdf](http://arxiv.org/pdf/2407.13623v2.pdf), cication: [**-1**](None)

	 *Chaofan Tao, Qian Liu, Longxu Dou, Niklas Muennighoff, Zhongwei Wan, Ping Luo, Min Lin, Ngai Wong* · ([scaling-with-vocab](https://github.com/sail-sg/scaling-with-vocab) - sail-sg) ![Star](https://img.shields.io/github/stars/sail-sg/scaling-with-vocab.svg?style=social&label=Star)
- **T-FREE: Tokenizer-Free Generative LLMs via Sparse Representations for
  Memory-Efficient Embeddings**, `arXiv, 2406.19223`, [arxiv](http://arxiv.org/abs/2406.19223v1), [pdf](http://arxiv.org/pdf/2406.19223v1.pdf), cication: [**-1**](None)

	 *Björn Deiseroth, Manuel Brack, Patrick Schramowski, Kristian Kersting, Samuel Weinbach*
- **Tokenization Falling Short: The Curse of Tokenization**, `arXiv, 2406.11687`, [arxiv](http://arxiv.org/abs/2406.11687v1), [pdf](http://arxiv.org/pdf/2406.11687v1.pdf), cication: [**-1**](None)

	 *Yekun Chai, Yewei Fang, Qiwei Peng, Xuhong Li*
- **Zero-Shot Tokenizer Transfer**, `arXiv, 2405.07883`, [arxiv](http://arxiv.org/abs/2405.07883v1), [pdf](http://arxiv.org/pdf/2405.07883v1.pdf), cication: [**-1**](None)

	 *Benjamin Minixhofer, Edoardo Maria Ponti, Ivan Vulić*
	- 
- **Toward a Theory of Tokenization in LLMs**, `arXiv, 2404.08335`, [arxiv](http://arxiv.org/abs/2404.08335v1), [pdf](http://arxiv.org/pdf/2404.08335v1.pdf), cication: [**-1**](None)

	 *Nived Rajaraman, Jiantao Jiao, Kannan Ramchandran*
- **Training LLMs over Neurally Compressed Text**, `arXiv, 2404.03626`, [arxiv](http://arxiv.org/abs/2404.03626v1), [pdf](http://arxiv.org/pdf/2404.03626v1.pdf), cication: [**-1**](None)

	 *Brian Lester, Jaehoon Lee, Alex Alemi, Jeffrey Pennington, Adam Roberts, Jascha Sohl-Dickstein, Noah Constant*
- **Greed is All You Need: An Evaluation of Tokenizer Inference Methods**, `arXiv, 2403.01289`, [arxiv](http://arxiv.org/abs/2403.01289v1), [pdf](http://arxiv.org/pdf/2403.01289v1.pdf), cication: [**-1**](None)

	 *Omri Uzan, Craig W. Schmidt, Chris Tanner, Yuval Pinter*
- **xT: Nested Tokenization for Larger Context in Large Images**, `arXiv, 2403.01915`, [arxiv](http://arxiv.org/abs/2403.01915v1), [pdf](http://arxiv.org/pdf/2403.01915v1.pdf), cication: [**-1**](None)

	 *Ritwik Gupta, Shufan Li, Tyler Zhu, Jitendra Malik, Trevor Darrell, Karttikeya Mangalam* · ([xT](https://github.com/bair-climate-initiative/xT) - bair-climate-initiative) ![Star](https://img.shields.io/github/stars/bair-climate-initiative/xT.svg?style=social&label=Star)

## Books
- [GenAI Handbook](https://genai-handbook.github.io/)
- [大规模语言模型：从理论到实践](https://intro-llm.github.io/)

## Privacy
- [PIISA](http://piisa.org/)

## Misc
- **Prompt2Model: Generating Deployable Models from Natural Language
  Instructions**, `arXiv, 2308.12261`, [arxiv](http://arxiv.org/abs/2308.12261v1), [pdf](http://arxiv.org/pdf/2308.12261v1.pdf), cication: [**-1**](None)

	 *Vijay Viswanathan, Chenyang Zhao, Amanda Bertsch, Tongshuang Wu, Graham Neubig* · ([mp.weixin.qq](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652381740&idx=3&sn=ea1973275259430a24f3a086a82ca617))
- **xVal: A Continuous Number Encoding for Large Language Models**, `arXiv, 2310.02989`, [arxiv](http://arxiv.org/abs/2310.02989v1), [pdf](http://arxiv.org/pdf/2310.02989v1.pdf), cication: [**-1**](None)

	 *Siavash Golkar, Mariel Pettee, Michael Eickenberg, Alberto Bietti, Miles Cranmer, Geraud Krawezik, Francois Lanusse, Michael McCabe, Ruben Ohana, Liam Parker* · ([mp.weixin.qq](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652393209&idx=5&sn=d2bd7eabf982afe4b8643b0ef9ff467c))
- **GraphGPT: Graph Instruction Tuning for Large Language Models**, `arXiv, 2310.13023`, [arxiv](http://arxiv.org/abs/2310.13023v1), [pdf](http://arxiv.org/pdf/2310.13023v1.pdf), cication: [**2**](https://scholar.google.com/scholar?cites=2388076328359031272&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Jiabin Tang, Yuhao Yang, Wei Wei, Lei Shi, Lixin Su, Suqi Cheng, Dawei Yin, Chao Huang* · ([mp.weixin.qq](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652396739&idx=2&sn=c992b43cb881ac14b6b6402248568854))
- [A taxonomy and review of generalization research in NLP | Nature Machine Intelligence](https://www.nature.com/articles/s42256-023-00729-y?utm_source=twitter&utm_medium=organic_social&utm_campaign=research&utm_content=link)
- **Neurons in Large Language Models: Dead, N-gram, Positional**, `arXiv, 2309.04827`, [arxiv](http://arxiv.org/abs/2309.04827v1), [pdf](http://arxiv.org/pdf/2309.04827v1.pdf), cication: [**-1**](None)

	 *Elena Voita, Javier Ferrando, Christoforos Nalmpantis*
	 
- [模型融合、混合专家、更小的LLM，几篇论文看懂2024年LLM发展方向 | 机器之心](https://www.jiqizhixin.com/articles/2024-02-22)
- [ACL 2023最佳论文出炉！CMU西交大等摘桂冠，杰出论文奖华人学者占半壁江](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652350213&idx=3&sn=01843319b9d89fcfda5096af994b2050)山


## Impacts
- **On the Societal Impact of Open Foundation Models**, `arXiv, 2403.07918`, [arxiv](http://arxiv.org/abs/2403.07918v1), [pdf](http://arxiv.org/pdf/2403.07918v1.pdf), cication: [**-1**](None)

	 *Sayash Kapoor, Rishi Bommasani, Kevin Klyman, Shayne Longpre, Ashwin Ramaswami, Peter Cihon, Aspen Hopkins, Kevin Bankston, Stella Biderman, Miranda Bogen* · ([crfm.stanford](https://crfm.stanford.edu/open-fms/))
- [MIT新研究：打工人不用担心被AI淘汰！成本巨贵，视觉工作只有23%可替代](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652436863&idx=3&sn=cd329df965557870646c7dd6fcb460cc)

## Course & Tutorial
- [Lectures](https://dlsyscourse.org/lectures/)
- [**cookbook**](https://github.com/EleutherAI/cookbook) - EleutherAI ![Star](https://img.shields.io/github/stars/EleutherAI/cookbook.svg?style=social&label=Star)

	 *Deep learning for dummies. All the practical details and useful utilities that go into working with real models.*
- [Physics of Language Models](https://icml.cc/virtual/2024/tutorial/35223)
- [**llama3-from-scratch**](https://github.com/naklecha/llama3-from-scratch) - naklecha ![Star](https://img.shields.io/github/stars/naklecha/llama3-from-scratch.svg?style=social&label=Star)

	 *llama3 implementation one matrix multiplication at a time*
- [**cookbook**](https://huggingface.co/learn/cookbook/index) - learn 🤗
- [GPT in 60 Lines of NumPy | Jay Mody](https://jaykmody.com/blog/gpt-from-scratch/)
- [Fetching Title#4fnm](https://web.stanford.edu/class/cs25/)
- [Stanford CS25 - Transformers United - YouTube](https://www.youtube.com/playlist?list=PLoROMvodv4rNiJRchCzutFw5ItR_Z27CM)
- [Let's build the GPT Tokenizer - YouTube](https://youtu.be/zduSFxRajkE)
- [**minbpe**](https://github.com/karpathy/minbpe) - karpathy ![Star](https://img.shields.io/github/stars/karpathy/minbpe.svg?style=social&label=Star)

	 *Minimal, clean, code for the Byte Pair Encoding (BPE) algorithm commonly used in LLM tokenization.*
- [**LLMs-from-scratch**](https://github.com/rasbt/LLMs-from-scratch) - rasbt ![Star](https://img.shields.io/github/stars/rasbt/LLMs-from-scratch.svg?style=social&label=Star)

	 *Implementing a ChatGPT-like LLM from scratch, step by step*
- [**MachineLearning-QandAI-book**](https://github.com/rasbt/MachineLearning-QandAI-book/tree/main) - rasbt ![Star](https://img.shields.io/github/stars/rasbt/MachineLearning-QandAI-book.svg?style=social&label=Star)

	 *Machine Learning Q and AI book*
- [**ML-YouTube-Courses**](https://github.com/dair-ai/ML-YouTube-Courses) - dair-ai ![Star](https://img.shields.io/github/stars/dair-ai/ML-YouTube-Courses.svg?style=social&label=Star)

	 *📺 Discover the latest machine learning / AI courses on YouTube.*
- [**llm-course**](https://github.com/mlabonne/llm-course) - mlabonne ![Star](https://img.shields.io/github/stars/mlabonne/llm-course.svg?style=social&label=Star)

	 *Course to get into Large Language Models (LLMs) with roadmaps and Colab notebooks.*
- [[1hr Talk] Intro to Large Language Models - YouTube](https://www.youtube.com/watch?v=zjkBMFhNj_g&ab_channel=AndrejKarpathy)

	 · ([drive.google](https://drive.google.com/file/d/1pxx_ZI7O-Nwl7ZLNk5hI3WzAsTLwvNU7/view?pli=1)) · ([drive.google](https://drive.google.com/file/d/1FPUpFMiCkMRKPFjhi9MAhby68MHVqe8u/view?pli=1))

	 · ([mp.weixin.qq](https://mp.weixin.qq.com/s?__biz=MzIyMzk1MDE3Nw==&mid=2247620373&idx=1&sn=f319bf1583d025e4f191b2f4a3d156ac))
	 
- [ML 2023 Spring](https://speech.ee.ntu.edu.tw/~hylee/ml/2023-spring.php)
- [80分鐘快速了解大型語言模型 (5:30 有咒術迴戰雷) - YouTube](https://www.youtube.com/watch?v=wG8-IUtqu-s&t=5s&ab_channel=Hung-yiLee)
- [Stanford CS224N: Natural Language Processing with Deep Learning | 2023 - YouTube](https://www.youtube.com/playlist?list=PLoROMvodv4rMFqRtEuo6SGjY4XbRIVRd4)

### CUDA
- [**lectures**](https://github.com/cuda-mode/lectures) - cuda-mode ![Star](https://img.shields.io/github/stars/cuda-mode/lectures.svg?style=social&label=Star)

	 *Material for cuda-mode lectures*
- [introduce CUDA in a way that will be accessible to Python folks](https://twitter.com/jeremyphoward/status/1752071227228008471)

	 · ([youtu](https://youtu.be/nOxKexn3iBo))

### Videos
- [Unreasonably Effective AI with Demis Hassabis - YouTube](https://www.youtube.com/watch?v=pZybROKrj2Q)
- [Low Level Technicals of LLMs: Daniel Han - YouTube](https://www.youtube.com/watch?v=pRM_P6UfdIc&ab_channel=AIEngineer)
- [Fixing bugs in Gemma, Llama, & Phi 3: Daniel Han - YouTube](https://www.youtube.com/watch?v=TKmfBnW0mQA&ab_channel=AIEngineer)
- [Interviewing Sebastian Raschka on the state of open LLMs, Llama 3.1, and AI education](https://www.interconnects.ai/p/interviewing-sebastian-raschka)
- [one transistor to a small CPU](https://x.com/karpathy/status/1818897688571920514)
- [Before you continue to YouTube](https://www.youtube.com/@srush_nlp/videos)
- [Yann Lecun | Objective-Driven AI: Towards AI systems that can learn, remember, reason, and plan - YouTube](https://www.youtube.com/watch?v=MiqLoAZFRSE&ab_channel=HarvardCMSA)

	 · ([twitter](https://twitter.com/ylecun/status/1776151785624801336))
- [Deep Learning Foundations by Soheil Feizi : Large Language Models - YouTube](https://www.youtube.com/watch?v=2yjzZfDQxy8&ab_channel=SoheilFeizi)
- [But what is a GPT? Visual intro to Transformers | Deep learning, chapter 5 - YouTube](https://www.youtube.com/watch?v=wjZofJX0v4M&t=2s&ab_channel=3Blue1Brown)
- [Unsupervised Learning: Redpoint's AI Podcast - YouTube](https://www.youtube.com/@RedpointAI)

	 · ([youtube](https://www.youtube.com/watch?v=_N2KPEdh69s&ab_channel=UnsupervisedLearning%3ARedpoint%27sAIPodcast))
- [Sequoia Capital - YouTube](https://www.youtube.com/@sequoiacapital/podcasts)
- [Making AI accessible with Andrej Karpathy and Stephanie Zhan - YouTube](https://www.youtube.com/watch?v=c3b-JASoPi0&list=PLOhHNjZItNnOoPxOF3dmq30UxYqFuxXKn&index=4&ab_channel=SequoiaCapital)

	 · ([mp.weixin.qq](https://mp.weixin.qq.com/s/bsclvziceHPM4qiO4R3HdA))
- [Yann Lecun: Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI | Lex Fridman Podcast #416 - YouTube](https://www.youtube.com/watch?v=5t1vTLU7s40)
- [Sam Altman: OpenAI, GPT-5, Sora, Board Saga, Elon Musk, Ilya, Power & AGI | Lex Fridman Podcast #419 - YouTube](https://www.youtube.com/watch?v=jvqFAi7vkBc)
- [No Priors: AI, Machine Learning, Tech, & Startups - YouTube](https://www.youtube.com/@NoPriorsPodcast)
- [Eye on AI - YouTube](https://www.youtube.com/@eyeonai3425/podcasts)

### Blogs
- [**ML-Papers-Explained**](https://github.com/dair-ai/ML-Papers-Explained?tab=readme-ov-file) - dair-ai ![Star](https://img.shields.io/github/stars/dair-ai/ML-Papers-Explained.svg?style=social&label=Star)

	 *Explanation to key concepts in ML*
- [What happened to BERT & T5? On Transformer Encoders, PrefixLM and Denoising Objectives — Yi Tay](https://www.yitay.net/blog/model-architecture-blogpost-encoders-prefixlm-denoising)

	 · ([x](https://x.com/srush_nlp/status/1779938508578165198))
- [The 10,000x Yolo Researcher Metagame — with Yi Tay of Reka](https://www.latent.space/p/yitay)
- [Deep Dive into Transformers by Hand ✍︎ | by Srijanie Dey, PhD | Towards Data Science](https://towardsdatascience.com/deep-dive-into-transformers-by-hand-%EF%B8%8E-68b8be4bd813)
- [**Transformers-Tutorials**](https://github.com/NielsRogge/Transformers-Tutorials) - NielsRogge ![Star](https://img.shields.io/github/stars/NielsRogge/Transformers-Tutorials.svg?style=social&label=Star)

	 *This repository contains demos I made with the Transformers library by HuggingFace.*
- [How I got into deep learning - Vikas Paruchuri](https://www.vikas.sh/post/how-i-got-into-deep-learning)
- [NLP Newsletter | elvis | Substack](https://nlp.elvissaravia.com/)
- [Archive • AI News • Buttondown](https://buttondown.email/ainews/archive/)
- [Sebastian Raschka, PhD | Substack](https://substack.com/@rasbt)
- [Lil'Log](https://lilianweng.github.io/)

- [网传Ilya Sutskever的推荐清单火了，掌握当前AI 90% | 机器之心](https://www.jiqizhixin.com/articles/2024-05-09-7)

## Extra reference
- [**mamba_state_space_model_paper_list**](https://github.com/event-ahu/mamba_state_space_model_paper_list) - event-ahu ![Star](https://img.shields.io/github/stars/event-ahu/mamba_state_space_model_paper_list.svg?style=social&label=Star)

	 *[Mamba-Survey-2024] Paper list for State-Space-Model/Mamba and it's Applications*
- [**Awesome-Mamba-Papers**](https://github.com/yyyujintang/Awesome-Mamba-Papers) - yyyujintang ![Star](https://img.shields.io/github/stars/yyyujintang/Awesome-Mamba-Papers.svg?style=social&label=Star)

	 *Awesome Papers related to Mamba.*
- [**awesome-generative-ai-guide**](https://github.com/aishwaryanr/awesome-generative-ai-guide) - aishwaryanr ![Star](https://img.shields.io/github/stars/aishwaryanr/awesome-generative-ai-guide.svg?style=social&label=Star)

	 *A one stop repository for generative AI research updates, interview resources, notebooks and much more!*
- [**awesome-local-ai**](https://github.com/janhq/awesome-local-ai) - janhq ![Star](https://img.shields.io/github/stars/janhq/awesome-local-ai.svg?style=social&label=Star)

	 *An awesome repository of local AI tools*
- [**how-to-optim-algorithm-in-cuda**](https://github.com/BBuf/how-to-optim-algorithm-in-cuda) - BBuf ![Star](https://img.shields.io/github/stars/BBuf/how-to-optim-algorithm-in-cuda.svg?style=social&label=Star)

	 *how to optimize some algorithm in cuda.*

---
- [机器之心SOTA！模型](https://sota.jiqizhixin.com/)
- [MLC-LLM 支持RWKV-5推理以及对RWKV-5的一些思考](https://mp.weixin.qq.com/s?__biz=Mzg4OTEwNjMzMA==&mid=2247534749&idx=2&sn=74ee2be683af033e5e39422240ca6e39)