# LLM Alignment
- [LLM Alignment](#llm-alignment)
	- [Survey](#survey)
	- [Paper \& Projects](#paper--projects)
	- [Other](#other)
- [Awesome RLHF](#awesome-rlhf)
	- [Survey](#survey-1)
	- [Papers](#papers)
	- [Projects](#projects)
	- [Other](#other-1)
	- [Extra reference](#extra-reference)

## Survey
- **AI Alignment: A Comprehensive Survey**, `arXiv, 2310.19852`, [arxiv](http://arxiv.org/abs/2310.19852v2), [pdf](http://arxiv.org/pdf/2310.19852v2.pdf), cication: [**1**](https://scholar.google.com/scholar?cites=2143607605171939849&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Jiaming Ji, Tianyi Qiu, Boyuan Chen, Borong Zhang, Hantao Lou, Kaile Wang, Yawen Duan, Zhonghao He, Jiayi Zhou, Zhaowei Zhang*
- **Instruction Tuning for Large Language Models: A Survey**, `arXiv, 2308.10792`, [arxiv](http://arxiv.org/abs/2308.10792v4), [pdf](http://arxiv.org/pdf/2308.10792v4.pdf), cication: [**19**](https://scholar.google.com/scholar?cites=10868294172009796896&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu* ¬∑ ([mp.weixin.qq](https://mp.weixin.qq.com/s?__biz=MzAxMTk4NDkwNw==&mid=2247494678&idx=2&sn=8f5065a21416564f369666744d144452))
- **Large Language Model Alignment: A Survey**, `arXiv, 2309.15025`, [arxiv](http://arxiv.org/abs/2309.15025v1), [pdf](http://arxiv.org/pdf/2309.15025v1.pdf), cication: [**3**](https://scholar.google.com/scholar?cites=12166333814159585377&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Tianhao Shen, Renren Jin, Yufei Huang, Chuang Liu, Weilong Dong, Zishan Guo, Xinwei Wu, Yan Liu, Deyi Xiong* ¬∑ ([jiqizhixin](https://www.jiqizhixin.com/articles/2023-10-07-11)) ¬∑ ([llm-alignment-survey](https://github.com/Magnetic2014/llm-alignment-survey) - Magnetic2014) ![Star](https://img.shields.io/github/stars/Magnetic2014/llm-alignment-survey.svg?style=social&label=Star)
- **Aligning Large Language Models with Human: A Survey**, `arXiv, 2307.12966`, [arxiv](http://arxiv.org/abs/2307.12966v1), [pdf](http://arxiv.org/pdf/2307.12966v1.pdf), cication: [**29**](https://scholar.google.com/scholar?cites=2762352632434587623&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Yufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xingshan Zeng, Wenyong Huang, Lifeng Shang, Xin Jiang, Qun Liu* ¬∑ ([AlignLLMHumanSurvey](https://github.com/GaryYufei/AlignLLMHumanSurvey) - GaryYufei) ![Star](https://img.shields.io/github/stars/GaryYufei/AlignLLMHumanSurvey.svg?style=social&label=Star)

## Paper & Projects

- **Alignment Studio: Aligning Large Language Models to Particular
  Contextual Regulations**, `arXiv, 2403.09704`, [arxiv](http://arxiv.org/abs/2403.09704v1), [pdf](http://arxiv.org/pdf/2403.09704v1.pdf), cication: [**-1**](None)

	 *Swapnaja Achintalwar, Ioana Baldini, Djallel Bouneffouf, Joan Byamugisha, Maria Chang, Pierre Dognin, Eitan Farchi, Ndivhuwo Makondo, Aleksandra Mojsilovic, Manish Nagireddy*
- **Instruction-tuned Language Models are Better Knowledge Learners**, `arXiv, 2402.12847`, [arxiv](http://arxiv.org/abs/2402.12847v1), [pdf](http://arxiv.org/pdf/2402.12847v1.pdf), cication: [**-1**](None)

	 *Zhengbao Jiang, Zhiqing Sun, Weijia Shi, Pedro Rodriguez, Chunting Zhou, Graham Neubig, Xi Victoria Lin, Wen-tau Yih, Srinivasan Iyer*
- **Reformatted Alignment**, `arXiv, 2402.12219`, [arxiv](http://arxiv.org/abs/2402.12219v1), [pdf](http://arxiv.org/pdf/2402.12219v1.pdf), cication: [**-1**](None)

	 *Run-Ze Fan, Xuefeng Li, Haoyang Zou, Junlong Li, Shwai He, Ethan Chern, Jiewen Hu, Pengfei Liu* ¬∑ ([ReAlign](https://github.com/GAIR-NLP/ReAlign) - GAIR-NLP) ![Star](https://img.shields.io/github/stars/GAIR-NLP/ReAlign.svg?style=social&label=Star) ¬∑ ([gair-nlp.github](https://gair-nlp.github.io/ReAlign/))

	 ¬∑ ([qbitai](https://www.qbitai.com/2024/03/127514.html))
- **Aligner: Achieving Efficient Alignment through Weak-to-Strong Correction**, `arXiv, 2402.02416`, [arxiv](http://arxiv.org/abs/2402.02416v2), [pdf](http://arxiv.org/pdf/2402.02416v2.pdf), cication: [**-1**](None)

	 *Jiaming Ji, Boyuan Chen, Hantao Lou, Donghai Hong, Borong Zhang, Xuehai Pan, Juntao Dai, Yaodong Yang* ¬∑ ([jiqizhixin](https://www.jiqizhixin.com/articles/2024-02-08-6)) ¬∑ ([aligner2024.github](https://aligner2024.github.io))
- **LESS: Selecting Influential Data for Targeted Instruction Tuning**, `arXiv, 2402.04333`, [arxiv](http://arxiv.org/abs/2402.04333v1), [pdf](http://arxiv.org/pdf/2402.04333v1.pdf), cication: [**-1**](None)

	 *Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, Danqi Chen* ¬∑ ([less](https://github.com/princeton-nlp/less) - princeton-nlp) ![Star](https://img.shields.io/github/stars/princeton-nlp/less.svg?style=social&label=Star)

	 ¬∑ ([qbitai](https://www.qbitai.com/2024/02/120290.html))
- **Generative Representational Instruction Tuning**, `arXiv, 2402.09906`, [arxiv](http://arxiv.org/abs/2402.09906v1), [pdf](http://arxiv.org/pdf/2402.09906v1.pdf), cication: [**-1**](None)

	 *Niklas Muennighoff, Hongjin Su, Liang Wang, Nan Yang, Furu Wei, Tao Yu, Amanpreet Singh, Douwe Kiela*
- **DeAL: Decoding-time Alignment for Large Language Models**, `arXiv, 2402.06147`, [arxiv](http://arxiv.org/abs/2402.06147v1), [pdf](http://arxiv.org/pdf/2402.06147v1.pdf), cication: [**-1**](None)

	 *James Y. Huang, Sailik Sengupta, Daniele Bonadiman, Yi-an Lai, Arshit Gupta, Nikolaos Pappas, Saab Mansour, Katrin Kirchoff, Dan Roth*
- **Direct Language Model Alignment from Online AI Feedback**, `arXiv, 2402.04792`, [arxiv](http://arxiv.org/abs/2402.04792v1), [pdf](http://arxiv.org/pdf/2402.04792v1.pdf), cication: [**-1**](None)

	 *Shangmin Guo, Biao Zhang, Tianlin Liu, Tianqi Liu, Misha Khalman, Felipe Llinares, Alexandre Rame, Thomas Mesnard, Yao Zhao, Bilal Piot*
- **Specialized Language Models with Cheap Inference from Limited Domain
  Data**, `arXiv, 2402.01093`, [arxiv](http://arxiv.org/abs/2402.01093v1), [pdf](http://arxiv.org/pdf/2402.01093v1.pdf), cication: [**-1**](None)

	 *David Grangier, Angelos Katharopoulos, Pierre Ablin, Awni Hannun*
- **Human-Instruction-Free LLM Self-Alignment with Limited Samples**, `arXiv, 2401.06785`, [arxiv](http://arxiv.org/abs/2401.06785v1), [pdf](http://arxiv.org/pdf/2401.06785v1.pdf), cication: [**-1**](None)

	 *Hongyi Guo, Yuanshun Yao, Wei Shen, Jiaheng Wei, Xiaoying Zhang, Zhaoran Wang, Yang Liu*
- **WaveCoder: Widespread And Versatile Enhanced Instruction Tuning with
  Refined Data Generation**, `arXiv, 2312.14187`, [arxiv](http://arxiv.org/abs/2312.14187v2), [pdf](http://arxiv.org/pdf/2312.14187v2.pdf), cication: [**-1**](None)

	 *Zhaojian Yu, Xin Zhang, Ning Shang, Yangyu Huang, Can Xu, Yishujie Zhao, Wenxiang Hu, Qiufeng Yin*
- [Teach Llamas to Talk: Recent Progress in Instruction Tuning](https://gaotianyu.xyz/blog/2023/11/30/instruction-tuning/)

	 ¬∑ ([mp.weixin.qq](https://mp.weixin.qq.com/s?__biz=MzIyMzk1MDE3Nw==&mid=2247621353&idx=5&sn=2f132a987c097fa38c8c374283903165))
- [**weak-to-strong**](https://github.com/openai/weak-to-strong) - openai ![Star](https://img.shields.io/github/stars/openai/weak-to-strong.svg?style=social&label=Star)

	 ¬∑ ([openai](https://openai.com/research/weak-to-strong-generalization)) ¬∑ ([cdn.openai](https://cdn.openai.com/papers/weak-to-strong-generalization.pdf))
	 ¬∑ ([jiqizhixin](https://www.jiqizhixin.com/articles/2023-12-15-5))
	 ¬∑ ([mp.weixin.qq](https://mp.weixin.qq.com/s?__biz=MzAxMTk4NDkwNw==&mid=2247495310&idx=1&sn=0711309c1fbf12c3baf304f19195d1d5))
- **Alignment for Honesty**, `arXiv, 2312.07000`, [arxiv](http://arxiv.org/abs/2312.07000v1), [pdf](http://arxiv.org/pdf/2312.07000v1.pdf), cication: [**-1**](None)

	 *Yuqing Yang, Ethan Chern, Xipeng Qiu, Graham Neubig, Pengfei Liu* ¬∑ ([alignment-for-honesty](https://github.com/GAIR-NLP/alignment-for-honesty) - GAIR-NLP) ![Star](https://img.shields.io/github/stars/GAIR-NLP/alignment-for-honesty.svg?style=social&label=Star)
- **The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context
  Learning**, `arXiv, 2312.01552`, [arxiv](http://arxiv.org/abs/2312.01552v1), [pdf](http://arxiv.org/pdf/2312.01552v1.pdf), cication: [**-1**](None)

	 *Bill Yuchen Lin, Abhilasha Ravichander, Ximing Lu, Nouha Dziri, Melanie Sclar, Khyathi Chandu, Chandra Bhagavatula, Yejin Choi* ¬∑ ([allenai.github](https://allenai.github.io/re-align/))

	 ¬∑ ([jiqizhixin](https://www.jiqizhixin.com/articles/2023-12-06-8))

	 ¬∑ ([URIAL](https://github.com/Re-Align/URIAL) - Re-Align) ![Star](https://img.shields.io/github/stars/Re-Align/URIAL.svg?style=social&label=Star)
- **Instruction-tuning Aligns LLMs to the Human Brain**, `arXiv, 2312.00575`, [arxiv](http://arxiv.org/abs/2312.00575v1), [pdf](http://arxiv.org/pdf/2312.00575v1.pdf), cication: [**-1**](None)

	 *Khai Loong Aw, Syrielle Montariol, Badr AlKhamissi, Martin Schrimpf, Antoine Bosselut*
- [**wizardlm**](https://github.com/nlpxucan/wizardlm) - nlpxucan ![Star](https://img.shields.io/github/stars/nlpxucan/wizardlm.svg?style=social&label=Star)

	 *Family of instruction-following LLMs powered by Evol-Instruct: WizardLM, WizardCoder*
- **Trusted Source Alignment in Large Language Models**, `arXiv, 2311.06697`, [arxiv](http://arxiv.org/abs/2311.06697v1), [pdf](http://arxiv.org/pdf/2311.06697v1.pdf), cication: [**-1**](None)

	 *Vasilisa Bashlovkina, Zhaobin Kuang, Riley Matthews, Edward Clifford, Yennie Jun, William W. Cohen, Simon Baumgartner*
- **AlignBench: Benchmarking Chinese Alignment of Large Language Models**, `arXiv, 2311.18743`, [arxiv](http://arxiv.org/abs/2311.18743v3), [pdf](http://arxiv.org/pdf/2311.18743v3.pdf), cication: [**8**](https://scholar.google.com/scholar?cites=16582734869098555099&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Xiao Liu, Xuanyu Lei, Shengyuan Wang, Yue Huang, Zhuoer Feng, Bosi Wen, Jiale Cheng, Pei Ke, Yifan Xu, Weng Lam Tam* ¬∑ ([AlignBench](https://github.com/THUDM/AlignBench) - THUDM) ![Star](https://img.shields.io/github/stars/THUDM/AlignBench.svg?style=social&label=Star)
- **Zephyr: Direct Distillation of LM Alignment**, `arXiv, 2310.16944`, [arxiv](http://arxiv.org/abs/2310.16944v1), [pdf](http://arxiv.org/pdf/2310.16944v1.pdf), cication: [**1**](https://scholar.google.com/scholar?cites=5826276281263581161&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Cl√©mentine Fourrier, Nathan Habib* ¬∑ ([alignment-handbook](https://github.com/huggingface/alignment-handbook) - huggingface) ![Star](https://img.shields.io/github/stars/huggingface/alignment-handbook.svg?style=social&label=Star)

- **Controlled Decoding from Language Models**, `arXiv, 2310.17022`, [arxiv](http://arxiv.org/abs/2310.17022v1), [pdf](http://arxiv.org/pdf/2310.17022v1.pdf), cication: [**-1**](None)

	 *Sidharth Mudgal, Jong Lee, Harish Ganapathy, YaGuang Li, Tao Wang, Yanping Huang, Zhifeng Chen, Heng-Tze Cheng, Michael Collins, Trevor Strohman*
- **Auto-Instruct: Automatic Instruction Generation and Ranking for
  Black-Box Language Models**, `arXiv, 2310.13127`, [arxiv](http://arxiv.org/abs/2310.13127v1), [pdf](http://arxiv.org/pdf/2310.13127v1.pdf), cication: [**-1**](None)

	 *Zhihan Zhang, Shuohang Wang, Wenhao Yu, Yichong Xu, Dan Iter, Qingkai Zeng, Yang Liu, Chenguang Zhu, Meng Jiang*
- **An Emulator for Fine-Tuning Large Language Models using Small Language
  Models**, `arXiv, 2310.12962`, [arxiv](http://arxiv.org/abs/2310.12962v1), [pdf](http://arxiv.org/pdf/2310.12962v1.pdf), cication: [**-1**](None)

	 *Eric Mitchell, Rafael Rafailov, Archit Sharma, Chelsea Finn, Christopher D. Manning*
- **NEFTune: Noisy Embeddings Improve Instruction Finetuning**, `arXiv, 2310.05914`, [arxiv](http://arxiv.org/abs/2310.05914v2), [pdf](http://arxiv.org/pdf/2310.05914v2.pdf), cication: [**-1**](None)

	 *Neel Jain, Ping-yeh Chiang, Yuxin Wen, John Kirchenbauer, Hong-Min Chu, Gowthami Somepalli, Brian R. Bartoldson, Bhavya Kailkhura, Avi Schwarzschild, Aniruddha Saha* ¬∑ ([qbitai](https://www.qbitai.com/2023/10/91833.html))
- [**alignment-handbook**](https://github.com/huggingface/alignment-handbook) - huggingface ![Star](https://img.shields.io/github/stars/huggingface/alignment-handbook.svg?style=social&label=Star)

	 *Robust recipes for to align language models with human and AI preferences*
- [**Xwin-LM**](https://github.com/Xwin-LM/Xwin-LM) - Xwin-LM ![Star](https://img.shields.io/github/stars/Xwin-LM/Xwin-LM.svg?style=social&label=Star)

	 *Xwin-LM: Powerful, Stable, and Reproducible LLM Alignment* ¬∑ ([mp.weixin.qq](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652380317&idx=2&sn=24c7c3d53878d939d60a53b38c525869))
- **Self-Alignment with Instruction Backtranslation**, `arXiv, 2308.06259`, [arxiv](http://arxiv.org/abs/2308.06259v2), [pdf](http://arxiv.org/pdf/2308.06259v2.pdf), cication: [**13**](https://scholar.google.com/scholar?cites=14196853842712224571&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Luke Zettlemoyer, Omer Levy, Jason Weston, Mike Lewis* ¬∑ ([jiqizhixin](https://www.jiqizhixin.com/articles/2023-08-15-13))
- **Simple synthetic data reduces sycophancy in large language models**, `arXiv, 2308.03958`, [arxiv](http://arxiv.org/abs/2308.03958v1), [pdf](http://arxiv.org/pdf/2308.03958v1.pdf), cication: [**7**](https://scholar.google.com/scholar?cites=15897243719772431083&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Jerry Wei, Da Huang, Yifeng Lu, Denny Zhou, Quoc V. Le*
- [**alignllmhumansurvey**](https://github.com/garyyufei/alignllmhumansurvey) - garyyufei ![Star](https://img.shields.io/github/stars/garyyufei/alignllmhumansurvey.svg?style=social&label=Star)

	 *Aligning Large Language Models with Human: A Survey*
- **RLCD: Reinforcement Learning from Contrast Distillation for Language
  Model Alignment**, `arXiv, 2307.12950`, [arxiv](http://arxiv.org/abs/2307.12950v2), [pdf](http://arxiv.org/pdf/2307.12950v2.pdf), cication: [**5**](https://scholar.google.com/scholar?cites=2633095818852088890&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Kevin Yang, Dan Klein, Asli Celikyilmaz, Nanyun Peng, Yuandong Tian*
- **AlpaGasus: Training A Better Alpaca with Fewer Data**, `arXiv, 2307.08701`, [arxiv](http://arxiv.org/abs/2307.08701v4), [pdf](http://arxiv.org/pdf/2307.08701v4.pdf), cication: [**11**](https://scholar.google.com/scholar?cites=5256571148060741678&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang* ¬∑ ([lichang-chen.github](https://lichang-chen.github.io/AlpaGasus/))
- **Instruction Mining: When Data Mining Meets Large Language Model
  Finetuning**, `arXiv, 2307.06290`, [arxiv](http://arxiv.org/abs/2307.06290v2), [pdf](http://arxiv.org/pdf/2307.06290v2.pdf), cication: [**3**](https://scholar.google.com/scholar?cites=18128512597851597026&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Yihan Cao, Yanbin Kang, Chi Wang, Lichao Sun*
- **Becoming self-instruct: introducing early stopping criteria for minimal
  instruct tuning**, `arXiv, 2307.03692`, [arxiv](http://arxiv.org/abs/2307.03692v1), [pdf](http://arxiv.org/pdf/2307.03692v1.pdf), cication: [**2**](https://scholar.google.com/scholar?cites=8663508115372331640&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Waseem AlShikh, Manhal Daaboul, Kirk Goddard, Brock Imel, Kiran Kamble, Parikshith Kulkarni, Melisa Russak*
- **Training Models to Generate, Recognize, and Reframe Unhelpful Thoughts**, `arXiv, 2307.02768`, [arxiv](http://arxiv.org/abs/2307.02768v1), [pdf](http://arxiv.org/pdf/2307.02768v1.pdf), cication: [**2**](https://scholar.google.com/scholar?cites=8502039692890912470&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Mounica Maddela, Megan Ung, Jing Xu, Andrea Madotto, Heather Foran, Y-Lan Boureau*
- **Goal Representations for Instruction Following: A Semi-Supervised
  Language Interface to Control**, `arXiv, 2307.00117`, [arxiv](http://arxiv.org/abs/2307.00117v2), [pdf](http://arxiv.org/pdf/2307.00117v2.pdf), cication: [**3**](https://scholar.google.com/scholar?cites=9646982711872255783&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Vivek Myers, Andre He, Kuan Fang, Homer Walke, Philippe Hansen-Estruch, Ching-An Cheng, Mihai Jalobeanu, Andrey Kolobov, Anca Dragan, Sergey Levine*
- **On the Exploitability of Instruction Tuning**, `arXiv, 2306.17194`, [arxiv](http://arxiv.org/abs/2306.17194v2), [pdf](http://arxiv.org/pdf/2306.17194v2.pdf), cication: [**4**](https://scholar.google.com/scholar?cites=14521585596237389103&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Manli Shu, Jiongxiao Wang, Chen Zhu, Jonas Geiping, Chaowei Xiao, Tom Goldstein*
- **Are aligned neural networks adversarially aligned?**, `arXiv, 2306.15447`, [arxiv](http://arxiv.org/abs/2306.15447v1), [pdf](http://arxiv.org/pdf/2306.15447v1.pdf), cication: [**30**](https://scholar.google.com/scholar?cites=3768131676399480172&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Nicholas Carlini, Milad Nasr, Christopher A. Choquette-Choo, Matthew Jagielski, Irena Gao, Anas Awadalla, Pang Wei Koh, Daphne Ippolito, Katherine Lee, Florian Tramer*
- **Constitutional AI: Harmlessness from AI Feedback**, `arXiv, 2212.08073`, [arxiv](http://arxiv.org/abs/2212.08073v1), [pdf](http://arxiv.org/pdf/2212.08073v1.pdf), cication: [**249**](https://scholar.google.com/scholar?cites=4609886220529082012&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon*
- **A General Language Assistant as a Laboratory for Alignment**, `arXiv, 2112.00861`, [arxiv](http://arxiv.org/abs/2112.00861v3), [pdf](http://arxiv.org/pdf/2112.00861v3.pdf), cication: [**61**](https://scholar.google.com/scholar?cites=7633810469345389198&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma*

## Other
- [Stanford CS25: V3 I Recipe for Training Helpful Chatbots - YouTube](https://youtu.be/mcep6W8oB1I)
- [The History of Open-Source LLMs: Imitation and Alignment (Part Three)](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-imitation)
- [Teach Llamas to Talk: Recent Progress in Instruction Tuning](https://gaotianyu.xyz/blog/2023/11/30/instruction-tuning/)

	 ¬∑ ([jiqizhixin](https://www.jiqizhixin.com/articles/2024-01-22-5))

- [ÊÉ≥Á†îÁ©∂Â§ßÊ®°ÂûãAlignmentÔºå‰Ω†Âè™ÈúÄË¶ÅÁúãÊáÇËøôÂá†ÁØápaper - Áü•‰πé](https://zhuanlan.zhihu.com/p/681642685#showWechatShareTip?utm_source=wechat_timeline&utm_medium=social&wechatShare=1&s_r=0)
- [OpenAIË∂ÖÁ∫ßÂØπÈΩêË¥üË¥£‰∫∫Jan LeikeÔºöÂ¶Ç‰ΩïÁ†¥Ëß£ÂØπÈΩêÈöæÈ¢òÔºüÁî®ÂèØÊâ©Â±ïÁõëÁù£](https://mp.weixin.qq.com/s?__biz=MzIyMzk1MDE3Nw==&mid=2247619616&idx=1&sn=fb6bbb82ad3514091fe390e3302834eb)
- [ÊúâË¢´Ê∑∑ÂêàÂêéÁöÑSFTÊï∞ÊçÆ‰º§Âà∞](https://mp.weixin.qq.com/s?__biz=MzAxMTk4NDkwNw==&mid=2247494859&idx=1&sn=d92a88e6554b86a16e67ec89529916ce)
- [OpenAIÁöÑSuperalignmentÁ≠ñÁï•ÔºöËÆ°ÁÆó‰∏∫Áéã](https://mp.weixin.qq.com/s?__biz=MzAxMTk4NDkwNw==&mid=2247494388&idx=1&sn=ef2dc97c9f05564114b6df6746d16460)
- [ÂΩì OpenAI ËØ¥ Superalignment ËØ¥ÁöÑÊòØ‰ªÄ‰πà](https://mp.weixin.qq.com/s?__biz=MzAxMTk4NDkwNw==&mid=2247494216&idx=2&sn=404ea6eeb9d10baf0abfceeb04939c27)
- [Áî®AIÂØπÈΩêAIÔºüË∂ÖÁ∫ßÂØπÈΩêÂõ¢ÈòüÈ¢ÜÂØº‰∫∫ËØ¶Ëß£OpenAIÂØπÈΩêË∂ÖÁ∫ßÊô∫ËÉΩÂõõÂπ¥ËÆ°Âàí | Êú∫Âô®‰πãÂøÉ](https://www.jiqizhixin.com/articles/2023-08-11-3)
- [È¢ÜÂüüÂ§ßÊ®°Âûã-ËÆ≠ÁªÉTrick&ËêΩÂú∞ÊÄùËÄÉ](https://mp.weixin.qq.com/s?__biz=MzAxMTk4NDkwNw==&mid=2247494320&idx=2&sn=8edd6ea37bbd73768caa306d9daa2cf7)

# Awesome RLHF

## Survey
- **A Survey of Reinforcement Learning from Human Feedback**, `arXiv, 2312.14925`, [arxiv](http://arxiv.org/abs/2312.14925v1), [pdf](http://arxiv.org/pdf/2312.14925v1.pdf), cication: [**5**](https://scholar.google.com/scholar?cites=2931703534150485296&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Timo Kaufmann, Paul Weng, Viktor Bengs, Eyke H√ºllermeier*

## Papers
- **RewardBench: Evaluating Reward Models for Language Modeling**, `arXiv, 2403.13787`, [arxiv](http://arxiv.org/abs/2403.13787v1), [pdf](http://arxiv.org/pdf/2403.13787v1.pdf), cication: [**-1**](None)

	 *Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi*
	- `a benchmark dataset and toolkit designed for the comprehensive evaluation of reward models used in RLHF`
- [**reward-bench**](https://github.com/allenai/reward-bench?tab=readme-ov-file) - allenai ![Star](https://img.shields.io/github/stars/allenai/reward-bench.svg?style=social&label=Star)

	 *RewardBench: the first evaluation tool for reward models.* ¬∑ ([huggingface](https://huggingface.co/spaces/allenai/reward-bench)) ¬∑ ([twitter](https://twitter.com/natolambert/status/1770488846360428782))
- **ChatGLM-RLHF: Practices of Aligning Large Language Models with Human
  Feedback**, `arXiv, 2404.00934`, [arxiv](http://arxiv.org/abs/2404.00934v2), [pdf](http://arxiv.org/pdf/2404.00934v2.pdf), cication: [**-1**](None)

	 *Zhenyu Hou, Yilin Niu, Zhengxiao Du, Xiaohan Zhang, Xiao Liu, Aohan Zeng, Qinkai Zheng, Minlie Huang, Hongning Wang, Jie Tang*
- **sDPO: Don't Use Your Data All at Once**, `arXiv, 2403.19270`, [arxiv](http://arxiv.org/abs/2403.19270v1), [pdf](http://arxiv.org/pdf/2403.19270v1.pdf), cication: [**-1**](None)

	 *Dahyun Kim, Yungi Kim, Wonho Song, Hyeonwoo Kim, Yunsu Kim, Sanghoon Kim, Chanjun Park*
- **The N+ Implementation Details of RLHF with PPO: A Case Study on TL;DR
  Summarization**, `arXiv, 2403.17031`, [arxiv](http://arxiv.org/abs/2403.17031v1), [pdf](http://arxiv.org/pdf/2403.17031v1.pdf), cication: [**-1**](None)

	 *Shengyi Huang, Michael Noukhovitch, Arian Hosseini, Kashif Rasul, Weixun Wang, Lewis Tunstall*

	 ¬∑ ([summarize_from_feedback_details](https://github.com/vwxyzjn/summarize_from_feedback_details) - vwxyzjn) ![Star](https://img.shields.io/github/stars/vwxyzjn/summarize_from_feedback_details.svg?style=social&label=Star) ¬∑ ([huggingface](https://huggingface.co/blog/the_n_implementation_details_of_rlhf_with_ppo)) ¬∑ ([twitter](https://twitter.com/vwxyzjn/status/1773011925666050313))
- **PERL: Parameter Efficient Reinforcement Learning from Human Feedback**, `arXiv, 2403.10704`, [arxiv](http://arxiv.org/abs/2403.10704v1), [pdf](http://arxiv.org/pdf/2403.10704v1.pdf), cication: [**-1**](None)

	 *Hakim Sidahmed, Samrat Phatale, Alex Hutcheson, Zhuonan Lin, Zhang Chen, Zac Yu, Jarvis Jin, Roman Komarytsia, Christiane Ahlheim, Yonghao Zhu*
	- `(PERL) using Low-Rank Adaptation (LoRA) for training models with Reinforcement Learning from Human Feedback (RLHF), a method that aligns pretrained base LLMs with human preferences efficiently.`
- **ORPO: Monolithic Preference Optimization without Reference Model**, `arXiv, 2403.07691`, [arxiv](http://arxiv.org/abs/2403.07691v2), [pdf](http://arxiv.org/pdf/2403.07691v2.pdf), cication: [**-1**](None)

	 *Jiwoo Hong, Noah Lee, James Thorne* ¬∑ ([orpo](https://github.com/xfactlab/orpo) - xfactlab) ![Star](https://img.shields.io/github/stars/xfactlab/orpo.svg?style=social&label=Star)
- **Teaching Large Language Models to Reason with Reinforcement Learning**, `arXiv, 2403.04642`, [arxiv](http://arxiv.org/abs/2403.04642v1), [pdf](http://arxiv.org/pdf/2403.04642v1.pdf), cication: [**-1**](None)

	 *Alex Havrilla, Yuqing Du, Sharath Chandra Raparthy, Christoforos Nalmpantis, Jane Dwivedi-Yu, Maksym Zhuravinskyi, Eric Hambro, Sainbayar Sukhbaatar, Roberta Raileanu*
- **Back to Basics: Revisiting REINFORCE Style Optimization for Learning
  from Human Feedback in LLMs**, `arXiv, 2402.14740`, [arxiv](http://arxiv.org/abs/2402.14740v1), [pdf](http://arxiv.org/pdf/2402.14740v1.pdf), cication: [**-1**](None)

	 *Arash Ahmadian, Chris Cremer, Matthias Gall√©, Marzieh Fadaee, Julia Kreutzer, Ahmet √úst√ºn, Sara Hooker*
- **Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive**, `arXiv, 2402.13228`, [arxiv](http://arxiv.org/abs/2402.13228v1), [pdf](http://arxiv.org/pdf/2402.13228v1.pdf), cication: [**-1**](None)

	 *Arka Pal, Deep Karkhanis, Samuel Dooley, Manley Roberts, Siddartha Naidu, Colin White*
- **A Critical Evaluation of AI Feedback for Aligning Large Language Models**, `arXiv, 2402.12366`, [arxiv](http://arxiv.org/abs/2402.12366v1), [pdf](http://arxiv.org/pdf/2402.12366v1.pdf), cication: [**-1**](None)

	 *Archit Sharma, Sedrick Keh, Eric Mitchell, Chelsea Finn, Kushal Arora, Thomas Kollar*
- **RLVF: Learning from Verbal Feedback without Overgeneralization**, `arXiv, 2402.10893`, [arxiv](http://arxiv.org/abs/2402.10893v1), [pdf](http://arxiv.org/pdf/2402.10893v1.pdf), cication: [**-1**](None)

	 *Moritz Stephan, Alexander Khazatsky, Eric Mitchell, Annie S Chen, Sheryl Hsu, Archit Sharma, Chelsea Finn*
- **A Minimaximalist Approach to Reinforcement Learning from Human Feedback**, `arXiv, 2401.04056`, [arxiv](http://arxiv.org/abs/2401.04056v1), [pdf](http://arxiv.org/pdf/2401.04056v1.pdf), cication: [**4**](https://scholar.google.com/scholar?cites=18273842745928489276&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Gokul Swamy, Christoph Dann, Rahul Kidambi, Zhiwei Steven Wu, Alekh Agarwal* ¬∑ ([jiqizhixin](https://www.jiqizhixin.com/articles/2024-02-15-3))
- **Suppressing Pink Elephants with Direct Principle Feedback**, `arXiv, 2402.07896`, [arxiv](http://arxiv.org/abs/2402.07896v2), [pdf](http://arxiv.org/pdf/2402.07896v2.pdf), cication: [**-1**](None)

	 *Louis Castricato, Nathan Lile, Suraj Anand, Hailey Schoelkopf, Siddharth Verma, Stella Biderman*
- **ODIN: Disentangled Reward Mitigates Hacking in RLHF**, `arXiv, 2402.07319`, [arxiv](http://arxiv.org/abs/2402.07319v1), [pdf](http://arxiv.org/pdf/2402.07319v1.pdf), cication: [**-1**](None)

	 *Lichang Chen, Chen Zhu, Davit Soselia, Jiuhai Chen, Tianyi Zhou, Tom Goldstein, Heng Huang, Mohammad Shoeybi, Bryan Catanzaro*
- **LiPO: Listwise Preference Optimization through Learning-to-Rank**, `arXiv, 2402.01878`, [arxiv](http://arxiv.org/abs/2402.01878v1), [pdf](http://arxiv.org/pdf/2402.01878v1.pdf), cication: [**-1**](None)

	 *Tianqi Liu, Zhen Qin, Junru Wu, Jiaming Shen, Misha Khalman, Rishabh Joshi, Yao Zhao, Mohammad Saleh, Simon Baumgartner, Jialu Liu*
- **StepCoder: Improve Code Generation with Reinforcement Learning from
  Compiler Feedback**, `arXiv, 2402.01391`, [arxiv](http://arxiv.org/abs/2402.01391v2), [pdf](http://arxiv.org/pdf/2402.01391v2.pdf), cication: [**-1**](None)

	 *Shihan Dou, Yan Liu, Haoxiang Jia, Limao Xiong, Enyu Zhou, Wei Shen, Junjie Shan, Caishuang Huang, Xiao Wang, Xiaoran Fan*
- **Transforming and Combining Rewards for Aligning Large Language Models**, `arXiv, 2402.00742`, [arxiv](http://arxiv.org/abs/2402.00742v1), [pdf](http://arxiv.org/pdf/2402.00742v1.pdf), cication: [**-1**](None)

	 *Zihao Wang, Chirag Nagpal, Jonathan Berant, Jacob Eisenstein, Alex D'Amour, Sanmi Koyejo, Victor Veitch*
- **Aligning Large Language Models with Counterfactual DPO**, `arXiv, 2401.09566`, [arxiv](http://arxiv.org/abs/2401.09566v2), [pdf](http://arxiv.org/pdf/2401.09566v2.pdf), cication: [**-1**](None)

	 *Bradley Butcher*
- **WARM: On the Benefits of Weight Averaged Reward Models**, `arXiv, 2401.12187`, [arxiv](http://arxiv.org/abs/2401.12187v1), [pdf](http://arxiv.org/pdf/2401.12187v1.pdf), cication: [**-1**](None)

	 *Alexandre Ram√©, Nino Vieillard, L√©onard Hussenot, Robert Dadashi, Geoffrey Cideron, Olivier Bachem, Johan Ferret*
- **A Mechanistic Understanding of Alignment Algorithms: A Case Study on DPO
  and Toxicity**, `arXiv, 2401.01967`, [arxiv](http://arxiv.org/abs/2401.01967v1), [pdf](http://arxiv.org/pdf/2401.01967v1.pdf), cication: [**11**](https://scholar.google.com/scholar?cites=6168801330158049880&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Andrew Lee, Xiaoyan Bai, Itamar Pres, Martin Wattenberg, Jonathan K. Kummerfeld, Rada Mihalcea*
- **ReFT: Reasoning with Reinforced Fine-Tuning**, `arXiv, 2401.08967`, [arxiv](http://arxiv.org/abs/2401.08967v1), [pdf](http://arxiv.org/pdf/2401.08967v1.pdf), cication: [**-1**](None)

	 *Trung Quoc Luong, Xinbo Zhang, Zhanming Jie, Peng Sun, Xiaoran Jin, Hang Li*
- **Self-Rewarding Language Models**, `arXiv, 2401.10020`, [arxiv](http://arxiv.org/abs/2401.10020v1), [pdf](http://arxiv.org/pdf/2401.10020v1.pdf), cication: [**-1**](None)

	 *Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, Jason Weston*
- **Contrastive Preference Optimization: Pushing the Boundaries of LLM
  Performance in Machine Translation**, `arXiv, 2401.08417`, [arxiv](http://arxiv.org/abs/2401.08417v1), [pdf](http://arxiv.org/pdf/2401.08417v1.pdf), cication: [**-1**](None)

	 *Haoran Xu, Amr Sharaf, Yunmo Chen, Weiting Tan, Lingfeng Shen, Benjamin Van Durme, Kenton Murray, Young Jin Kim*
- **Secrets of RLHF in Large Language Models Part II: Reward Modeling**, `arXiv, 2401.06080`, [arxiv](http://arxiv.org/abs/2401.06080v1), [pdf](http://arxiv.org/pdf/2401.06080v1.pdf), cication: [**-1**](None)

	 *Binghai Wang, Rui Zheng, Lu Chen, Yan Liu, Shihan Dou, Caishuang Huang, Wei Shen, Senjie Jin, Enyu Zhou, Chenyu Shi*

	 ¬∑ ([jiqizhixin](https://www.jiqizhixin.com/articles/2024-01-15-19))
- **ICE-GRT: Instruction Context Enhancement by Generative Reinforcement
  based Transformers**, `arXiv, 2401.02072`, [arxiv](http://arxiv.org/abs/2401.02072v1), [pdf](http://arxiv.org/pdf/2401.02072v1.pdf), cication: [**-1**](None)

	 *Chen Zheng, Ke Sun, Da Tang, Yukun Ma, Yuyu Zhang, Chenguang Xi, Xun Zhou*
- **InstructVideo: Instructing Video Diffusion Models with Human Feedback**, `arXiv, 2312.12490`, [arxiv](http://arxiv.org/abs/2312.12490v1), [pdf](http://arxiv.org/pdf/2312.12490v1.pdf), cication: [**-1**](None)

	 *Hangjie Yuan, Shiwei Zhang, Xiang Wang, Yujie Wei, Tao Feng, Yining Pan, Yingya Zhang, Ziwei Liu, Samuel Albanie, Dong Ni*
- **Silkie: Preference Distillation for Large Visual Language Models**, `arXiv, 2312.10665`, [arxiv](http://arxiv.org/abs/2312.10665v1), [pdf](http://arxiv.org/pdf/2312.10665v1.pdf), cication: [**-1**](None)

	 *Lei Li, Zhihui Xie, Mukai Li, Shunian Chen, Peiyi Wang, Liang Chen, Yazheng Yang, Benyou Wang, Lingpeng Kong*
- **Align on the Fly: Adapting Chatbot Behavior to Established Norms**, `arXiv, 2312.15907`, [arxiv](http://arxiv.org/abs/2312.15907v1), [pdf](http://arxiv.org/pdf/2312.15907v1.pdf), cication: [**-1**](None)

	 *Chunpu Xu, Steffi Chern, Ethan Chern, Ge Zhang, Zekun Wang, Ruibo Liu, Jing Li, Jie Fu, Pengfei Liu* ¬∑ ([jiqizhixin](https://www.jiqizhixin.com/articles/2024-01-24)) ¬∑ ([OPO](https://github.com/GAIR-NLP/OPO) - GAIR-NLP) ![Star](https://img.shields.io/github/stars/GAIR-NLP/OPO.svg?style=social&label=Star) ¬∑ ([gair-nlp.github](https://gair-nlp.github.io/OPO/))
- **Helping or Herding? Reward Model Ensembles Mitigate but do not Eliminate
  Reward Hacking**, `arXiv, 2312.09244`, [arxiv](http://arxiv.org/abs/2312.09244v1), [pdf](http://arxiv.org/pdf/2312.09244v1.pdf), cication: [**-1**](None)

	 *Jacob Eisenstein, Chirag Nagpal, Alekh Agarwal, Ahmad Beirami, Alex D'Amour, DJ Dvijotham, Adam Fisch, Katherine Heller, Stephen Pfohl, Deepak Ramachandran*
- **Beyond Human Data: Scaling Self-Training for Problem-Solving with
  Language Models**, `arXiv, 2312.06585`, [arxiv](http://arxiv.org/abs/2312.06585v1), [pdf](http://arxiv.org/pdf/2312.06585v1.pdf), cication: [**-1**](None)

	 *Avi Singh, John D. Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil, Peter J. Liu, James Harrison, Jaehoon Lee, Kelvin Xu, Aaron Parisi*
- [**HALOs**](https://github.com/ContextualAI/HALOs?tab=readme-ov-file) - ContextualAI ![Star](https://img.shields.io/github/stars/ContextualAI/HALOs?tab=readme-ov-file.svg?style=social&label=Star)

	 *Human-Centered Loss Functions (HALOs)* ¬∑ ([HALOs](https://github.com/ContextualAI/HALOs/blob/main/assets/report.pdf) - ContextualAI) ![Star](https://img.shields.io/github/stars/ContextualAI/HALOs.svg?style=social&label=Star)
- **Axiomatic Preference Modeling for Longform Question Answering**, `arXiv, 2312.02206`, [arxiv](http://arxiv.org/abs/2312.02206v1), [pdf](http://arxiv.org/pdf/2312.02206v1.pdf), cication: [**-1**](None)

	 *Corby Rosset, Guoqing Zheng, Victor Dibia, Ahmed Awadallah, Paul Bennett* ¬∑ ([huggingface](https://huggingface.co/corbyrosset/axiomatic_preference_model))
- **Nash Learning from Human Feedback**, `arXiv, 2312.00886`, [arxiv](http://arxiv.org/abs/2312.00886v2), [pdf](http://arxiv.org/pdf/2312.00886v2.pdf), cication: [**-1**](None)

	 *R√©mi Munos, Michal Valko, Daniele Calandriello, Mohammad Gheshlaghi Azar, Mark Rowland, Zhaohan Daniel Guo, Yunhao Tang, Matthieu Geist, Thomas Mesnard, Andrea Michi*
- **RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from
  Fine-grained Correctional Human Feedback**, `arXiv, 2312.00849`, [arxiv](http://arxiv.org/abs/2312.00849v1), [pdf](http://arxiv.org/pdf/2312.00849v1.pdf), cication: [**-1**](None)

	 *Tianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng Han, Ganqu Cui, Jinyi Hu, Zhiyuan Liu, Hai-Tao Zheng, Maosong Sun* ¬∑ ([RLHF-V](https://github.com/RLHF-V/RLHF-V) - RLHF-V) ![Star](https://img.shields.io/github/stars/RLHF-V/RLHF-V.svg?style=social&label=Star)
- [Starling-7B: Increasing LLM Helpfulness & Harmlessness with RLAIF](https://starling.cs.berkeley.edu/)
- **Adversarial Preference Optimization**, `arXiv, 2311.08045`, [arxiv](http://arxiv.org/abs/2311.08045v1), [pdf](http://arxiv.org/pdf/2311.08045v1.pdf), cication: [**-1**](None)

	 *Pengyu Cheng, Yifan Yang, Jian Li, Yong Dai, Nan Du*

	 ¬∑ ([mp.weixin.qq](https://mp.weixin.qq.com/s?__biz=MzAxMTk4NDkwNw==&mid=2247495391&idx=1&sn=51407a77e2d0277d958adaf533f6ca5b))
- **Diffusion Model Alignment Using Direct Preference Optimization**, `arXiv, 2311.12908`, [arxiv](http://arxiv.org/abs/2311.12908v1), [pdf](http://arxiv.org/pdf/2311.12908v1.pdf), cication: [**-1**](None)

	 *Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, Nikhil Naik*
- **Black-Box Prompt Optimization: Aligning Large Language Models without
  Model Training**, `arXiv, 2311.04155`, [arxiv](http://arxiv.org/abs/2311.04155v2), [pdf](http://arxiv.org/pdf/2311.04155v2.pdf), cication: [**-1**](None)

	 *Jiale Cheng, Xiao Liu, Kehan Zheng, Pei Ke, Hongning Wang, Yuxiao Dong, Jie Tang, Minlie Huang* ¬∑ ([bpo](https://github.com/thu-coai/bpo) - thu-coai) ![Star](https://img.shields.io/github/stars/thu-coai/bpo.svg?style=social&label=Star)
- **Towards Understanding Sycophancy in Language Models**, `arXiv, 2310.13548`, [arxiv](http://arxiv.org/abs/2310.13548v3), [pdf](http://arxiv.org/pdf/2310.13548v3.pdf), cication: [**-1**](None)

	 *Mrinank Sharma, Meg Tong, Tomasz Korbak, David Duvenaud, Amanda Askell, Samuel R. Bowman, Newton Cheng, Esin Durmus, Zac Hatfield-Dodds, Scott R. Johnston* ¬∑ ([jiqizhixin](https://www.jiqizhixin.com/articles/2023-10-25-4))
- **Contrastive Preference Learning: Learning from Human Feedback without RL**, `arXiv, 2310.13639`, [arxiv](http://arxiv.org/abs/2310.13639v2), [pdf](http://arxiv.org/pdf/2310.13639v2.pdf), cication: [**-1**](None)

	 *Joey Hejna, Rafael Rafailov, Harshit Sikchi, Chelsea Finn, Scott Niekum, W. Bradley Knox, Dorsa Sadigh* ¬∑ ([jiqizhixin](https://www.jiqizhixin.com/articles/2023-11-13-6))
- **Don't throw away your value model! Making PPO even better via
  Value-Guided Monte-Carlo Tree Search decoding**, `arXiv, 2309.15028`, [arxiv](http://arxiv.org/abs/2309.15028v2), [pdf](http://arxiv.org/pdf/2309.15028v2.pdf), cication: [**1**](https://scholar.google.com/scholar?cites=12825384516892808854&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Jiacheng Liu, Andrew Cohen, Ramakanth Pasunuru, Yejin Choi, Hannaneh Hajishirzi, Asli Celikyilmaz* ¬∑ ([jiqizhixin](https://www.jiqizhixin.com/articles/2023-10-27-3))
- [The N Implementation Details of RLHF with PPO](https://huggingface.co/blog/the_n_implementation_details_of_rlhf_with_ppo)
- **Specific versus General Principles for Constitutional AI**, `arXiv, 2310.13798`, [arxiv](http://arxiv.org/abs/2310.13798v1), [pdf](http://arxiv.org/pdf/2310.13798v1.pdf), cication: [**1**](https://scholar.google.com/scholar?cites=10472606437287020754&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Sandipan Kundu, Yuntao Bai, Saurav Kadavath, Amanda Askell, Andrew Callahan, Anna Chen, Anna Goldie, Avital Balwit, Azalia Mirhoseini, Brayden McLean*
- **Contrastive Preference Learning: Learning from Human Feedback without RL**, `arXiv, 2310.13639`, [arxiv](http://arxiv.org/abs/2310.13639v2), [pdf](http://arxiv.org/pdf/2310.13639v2.pdf), cication: [**-1**](None)

	 *Joey Hejna, Rafael Rafailov, Harshit Sikchi, Chelsea Finn, Scott Niekum, W. Bradley Knox, Dorsa Sadigh*
- **A General Theoretical Paradigm to Understand Learning from Human
  Preferences**, `arXiv, 2310.12036`, [arxiv](http://arxiv.org/abs/2310.12036v2), [pdf](http://arxiv.org/pdf/2310.12036v2.pdf), cication: [**1**](https://scholar.google.com/scholar?cites=16028671700226284164&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Mohammad Gheshlaghi Azar, Mark Rowland, Bilal Piot, Daniel Guo, Daniele Calandriello, Michal Valko, R√©mi Munos*
- **Tuna: Instruction Tuning using Feedback from Large Language Models**, `arXiv, 2310.13385`, [arxiv](http://arxiv.org/abs/2310.13385v1), [pdf](http://arxiv.org/pdf/2310.13385v1.pdf), cication: [**-1**](None)

	 *Haoran Li, Yiran Liu, Xingxing Zhang, Wei Lu, Furu Wei*
- **Safe RLHF: Safe Reinforcement Learning from Human Feedback**, `arXiv, 2310.12773`, [arxiv](http://arxiv.org/abs/2310.12773v1), [pdf](http://arxiv.org/pdf/2310.12773v1.pdf), cication: [**1**](https://scholar.google.com/scholar?cites=10151978917046355982&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Josef Dai, Xuehai Pan, Ruiyang Sun, Jiaming Ji, Xinbo Xu, Mickel Liu, Yizhou Wang, Yaodong Yang*
- **ReMax: A Simple, Effective, and Efficient Reinforcement Learning Method
  for Aligning Large Language Models**, `arXiv, 2310.10505`, [arxiv](http://arxiv.org/abs/2310.10505v2), [pdf](http://arxiv.org/pdf/2310.10505v2.pdf), cication: [**-1**](None)

	 *Ziniu Li, Tian Xu, Yushun Zhang, Yang Yu, Ruoyu Sun, Zhi-Quan Luo* ¬∑ ([jiqizhixin](https://www.jiqizhixin.com/articles/2023-10-20-9))
- [Rethinking the Role of PPO in RLHF ‚Äì The Berkeley Artificial Intelligence Research Blog](https://bair.berkeley.edu/blog/2023/10/16/p3o/)
- **Reinforcement Learning in the Era of LLMs: What is Essential? What is
  needed? An RL Perspective on RLHF, Prompting, and Beyond**, `arXiv, 2310.06147`, [arxiv](http://arxiv.org/abs/2310.06147v1), [pdf](http://arxiv.org/pdf/2310.06147v1.pdf), cication: [**-1**](None)

	 *Hao Sun*
- **A Long Way to Go: Investigating Length Correlations in RLHF**, `arXiv, 2310.03716`, [arxiv](http://arxiv.org/abs/2310.03716v1), [pdf](http://arxiv.org/pdf/2310.03716v1.pdf), cication: [**3**](https://scholar.google.com/scholar?cites=17792312030938285213&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Prasann Singhal, Tanya Goyal, Jiacheng Xu, Greg Durrett*
- **Aligning Large Multimodal Models with Factually Augmented RLHF**, `arXiv, 2309.14525`, [arxiv](http://arxiv.org/abs/2309.14525v1), [pdf](http://arxiv.org/pdf/2309.14525v1.pdf), cication: [**4**](https://scholar.google.com/scholar?cites=17054470781093797244&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang*
- **Stabilizing RLHF through Advantage Model and Selective Rehearsal**, `arXiv, 2309.10202`, [arxiv](http://arxiv.org/abs/2309.10202v1), [pdf](http://arxiv.org/pdf/2309.10202v1.pdf), cication: [**1**](https://scholar.google.com/scholar?cites=16456025046699375201&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Baolin Peng, Linfeng Song, Ye Tian, Lifeng Jin, Haitao Mi, Dong Yu*
- **Statistical Rejection Sampling Improves Preference Optimization**, `arXiv, 2309.06657`, [arxiv](http://arxiv.org/abs/2309.06657v1), [pdf](http://arxiv.org/pdf/2309.06657v1.pdf), cication: [**-1**](None)

	 *Tianqi Liu, Yao Zhao, Rishabh Joshi, Misha Khalman, Mohammad Saleh, Peter J. Liu, Jialu Liu*
- **Efficient RLHF: Reducing the Memory Usage of PPO**, `arXiv, 2309.00754`, [arxiv](http://arxiv.org/abs/2309.00754v1), [pdf](http://arxiv.org/pdf/2309.00754v1.pdf), cication: [**1**](https://scholar.google.com/scholar?cites=6843908222161428317&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Michael Santacroce, Yadong Lu, Han Yu, Yuanzhi Li, Yelong Shen*
- **RLAIF: Scaling Reinforcement Learning from Human Feedback with AI
  Feedback**, `arXiv, 2309.00267`, [arxiv](http://arxiv.org/abs/2309.00267v1), [pdf](http://arxiv.org/pdf/2309.00267v1.pdf), cication: [**24**](https://scholar.google.com/scholar?cites=7995210232742152683&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune, Abhinav Rastogi* ¬∑ ([mp.weixin.qq](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652372872&idx=1&sn=c1a543c792bf3dfc891729f04236c549))
- **Reinforced Self-Training (ReST) for Language Modeling**, `arXiv, 2308.08998`, [arxiv](http://arxiv.org/abs/2308.08998v2), [pdf](http://arxiv.org/pdf/2308.08998v2.pdf), cication: [**12**](https://scholar.google.com/scholar?cites=3263533902860525796&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu* ¬∑ ([jiqizhixin](https://www.jiqizhixin.com/articles/2023-08-22-6))
- **DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like
  Models at All Scales**, `arXiv, 2308.01320`, [arxiv](http://arxiv.org/abs/2308.01320v1), [pdf](http://arxiv.org/pdf/2308.01320v1.pdf), cication: [**4**](https://scholar.google.com/scholar?cites=9524636698512222272&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Zhewei Yao, Reza Yazdani Aminabadi, Olatunji Ruwase, Samyam Rajbhandari, Xiaoxia Wu, Ammar Ahmad Awan, Jeff Rasley, Minjia Zhang, Conglong Li, Connor Holmes*
- **Open Problems and Fundamental Limitations of Reinforcement Learning from
  Human Feedback**, `arXiv, 2307.15217`, [arxiv](http://arxiv.org/abs/2307.15217v2), [pdf](http://arxiv.org/pdf/2307.15217v2.pdf), cication: [**36**](https://scholar.google.com/scholar?cites=2043453316558651204&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, J√©r√©my Scheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire* ¬∑ ([jiqizhixin](https://www.jiqizhixin.com/articles/2023-08-01-9))
- [ICML '23 Tutorial on Reinforcement Learning from Human Feedback](https://docs.google.com/presentation/d/1b_ymNDU0WRQ1-rcQDK45_bH9F0giNyRmdi0iKso6G5E/edit#slide=id.g259dff58475_0_44)

	 ¬∑ ([openlmlab.github](https://openlmlab.github.io/MOSS-RLHF/)) ¬∑ ([mp.weixin.qq](https://mp.weixin.qq.com/s?__biz=MzAxMTk4NDkwNw==&mid=2247494225&idx=1&sn=7bae77595b4e2f43cc8a4b94ddb4646c))
- **Fine-Tuning Language Models with Advantage-Induced Policy Alignment**, `arXiv, 2306.02231`, [arxiv](http://arxiv.org/abs/2306.02231v3), [pdf](http://arxiv.org/pdf/2306.02231v3.pdf), cication: [**5**](https://scholar.google.com/scholar?cites=7726101249171983080&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Banghua Zhu, Hiteshi Sharma, Felipe Vieira Frujeri, Shi Dong, Chenguang Zhu, Michael I. Jordan, Jiantao Jiao*
- **System-Level Natural Language Feedback**, `arXiv, 2306.13588`, [arxiv](http://arxiv.org/abs/2306.13588v1), [pdf](http://arxiv.org/pdf/2306.13588v1.pdf), cication: [**1**](https://scholar.google.com/scholar?cites=5523428644029476523&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Weizhe Yuan, Kyunghyun Cho, Jason Weston*
- **Fine-Grained Human Feedback Gives Better Rewards for Language Model
  Training**, `arXiv, 2306.01693`, [arxiv](http://arxiv.org/abs/2306.01693v2), [pdf](http://arxiv.org/pdf/2306.01693v2.pdf), cication: [**7**](https://scholar.google.com/scholar?cites=9400790265193597011&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah A. Smith, Mari Ostendorf, Hannaneh Hajishirzi* ¬∑ ([finegrainedrlhf.github](https://finegrainedrlhf.github.io/)) ¬∑ ([qbitai](https://www.qbitai.com/2023/06/61691.html))
- **Direct Preference Optimization: Your Language Model is Secretly a Reward
  Model**, `arXiv, 2305.18290`, [arxiv](http://arxiv.org/abs/2305.18290v2), [pdf](http://arxiv.org/pdf/2305.18290v2.pdf), cication: [**-1**](None)

	 *Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, Chelsea Finn*
- **Let's Verify Step by Step**, `arXiv, 2305.20050`, [arxiv](http://arxiv.org/abs/2305.20050v1), [pdf](http://arxiv.org/pdf/2305.20050v1.pdf), cication: [**76**](https://scholar.google.com/scholar?cites=3594089577812846684&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, Karl Cobbe*
- **Training a Helpful and Harmless Assistant with Reinforcement Learning
  from Human Feedback**, `arXiv, 2204.05862`, [arxiv](http://arxiv.org/abs/2204.05862v1), [pdf](http://arxiv.org/pdf/2204.05862v1.pdf), cication: [**109**](https://scholar.google.com/scholar?cites=11199782510491151350&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan* ¬∑ ([hh-rlhf](https://github.com/anthropics/hh-rlhf) - anthropics) ![Star](https://img.shields.io/github/stars/anthropics/hh-rlhf.svg?style=social&label=Star)
## Projects
- [**PairRM**](https://huggingface.co/llm-blender/PairRM) - llm-blender ü§ó
- [**OpenRLHF**](https://github.com/OpenLLMAI/OpenRLHF) - OpenLLMAI ![Star](https://img.shields.io/github/stars/OpenLLMAI/OpenRLHF.svg?style=social&label=Star)

	 *A Ray-based High-performance RLHF framework (for 7B on RTX4090 and 34B on A100)*
- [**direct-preference-optimization**](https://github.com/eric-mitchell/direct-preference-optimization) - eric-mitchell ![Star](https://img.shields.io/github/stars/eric-mitchell/direct-preference-optimization.svg?style=social&label=Star)

	 *Reference implementation for DPO (Direct Preference Optimization)*
- [**trl**](https://github.com/huggingface/trl) - huggingface ![Star](https://img.shields.io/github/stars/huggingface/trl.svg?style=social&label=Star)

	 *Train transformer language models with reinforcement learning.*
- [**tril**](https://github.com/cornell-rl/tril) - cornell-rl ![Star](https://img.shields.io/github/stars/cornell-rl/tril.svg?style=social&label=Star)

## Other
- [History of Open Alignment](https://magnetic-share-282.notion.site/History-of-Open-Alignment-a7ef20aefb34438185336df68147809e)
- [Alignment Guidebook](https://www.notion.so/Alignment-Guidebook-e5c64df77c0a4b528b7951e87337fa78)
- [Constitutional AI with Open LLMs](https://huggingface.co/blog/constitutional_ai)
- [Preference Tuning LLMs with Direct Preference Optimization Methods](https://huggingface.co/blog/pref-tuning)

	 ¬∑ ([jiqizhixin](https://www.jiqizhixin.com/articles/2024-02-19))
- [Reinforcement Learning from Human Feedback - DeepLearning.AI](https://www.deeplearning.ai/short-courses/reinforcement-learning-from-human-feedback/)
- [**Reinforcement Learning for Language Models**](https://gist.github.com/yoavg/6bff0fecd65950898eba1bb321cfbd81) - yoavg ![Star](https://img.shields.io/github/stars/yoavg/6bff0fecd65950898eba1bb321cfbd81.svg?style=social&label=Star)
- [The Q\* hypothesis: Tree-of-thoughts reasoning, process reward models, and supercharging synthetic data](https://www.interconnects.ai/p/q-star)
- [reverse engineer the Q* fantasy](https://twitter.com/DrJimFan/status/1728100123862004105)
- [Fine-tune Llama 2 with DPO](https://huggingface.co/blog/dpo-trl)
- [LLM Training: RLHF and Its Alternatives](https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives)

	 ¬∑ ([mp.weixin.qq](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652379509&idx=5&sn=910a60449c276ac9d0f29b8f71a60327))
- [ICML '23 Tutorial on Reinforcement Learning from Human Feedback](https://docs.google.com/presentation/d/1b_ymNDU0WRQ1-rcQDK45_bH9F0giNyRmdi0iKso6G5E/edit#slide=id.g259dff58475_0_44)

- [Â§ßÊ®°ÂûãÂØπÈΩêÈò∂ÊÆµÁöÑScaling Laws](https://mp.weixin.qq.com/s?__biz=MzAxMTk4NDkwNw==&mid=2247495763&idx=1&sn=1fd3e1a70e188dcab7e480cc153e32ff)
- [RLHF‰∏≠Reward modelÁöÑtrick](https://mp.weixin.qq.com/s?__biz=MzAxMTk4NDkwNw==&mid=2247495465&idx=1&sn=b0690e841f36d154024c0f6a9e922daf)
- [ÊÄéÊ†∑ËÆ© PPO ËÆ≠ÁªÉÊõ¥Á®≥ÂÆöÔºüÊó©Êúü‰∫∫Á±ªÂæÅÊúç RLHF ÁöÑÈ©ØÂåñÁªèÈ™å](https://mp.weixin.qq.com/s?__biz=MzAxMTk4NDkwNw==&mid=2247495219&idx=2&sn=bcd6cf222963493fed29fd44838b5aa2)
- [RLHFÂÆûË∑µ - Áü•‰πé](https://zhuanlan.zhihu.com/p/635569455)

	 ¬∑ ([mp.weixin.qq](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652371219&idx=5&sn=63c3294a9c08dde7e63444c6110e9c34))
- [LLMÊàêÂäü‰∏çÂèØÊàñÁº∫ÁöÑÂü∫Áü≥ÔºöRLHFÂèäÂÖ∂Êõø‰ª£ÊäÄÊúØ | Êú∫Âô®‰πãÂøÉ](https://www.jiqizhixin.com/articles/2023-10-07-7)


## Extra reference

- [**awesome-RLHF**](https://github.com/opendilab/awesome-RLHF) - opendilab ![Star](https://img.shields.io/github/stars/opendilab/awesome-RLHF.svg?style=social&label=Star)

	 *A curated list of reinforcement learning with human feedback resources (continually updated)*