# LLM Eval

- [LLM Eval](#llm-eval)
  - [Survey](#survey)
  - [LLM Evaluation](#llm-evaluation)
  - [Leaderboard](#leaderboard)
  - [Projects](#projects)
  - [Misc](#misc)


## Survey


## LLM Evaluation

- [Paper page - LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive  Memory](https://huggingface.co/papers/2410.10813)
- [Paper page - CompassJudger-1: All-in-one Judge Model Helps Model Evaluation and  Evolution](https://huggingface.co/papers/2410.16256)
   - [github.com](https://github.com/open-compass/CompassJudger)
- [Paper page - UCFE: A User-Centric Financial Expertise Benchmark for Large Language  Models](https://huggingface.co/papers/2410.14059)
- [Paper page - MixEval-X: Any-to-Any Evaluations from Real-World Data Mixtures](https://huggingface.co/papers/2410.13754)
- [Paper page - JudgeBench: A Benchmark for Evaluating LLM-based Judges](https://huggingface.co/papers/2410.12784)
   - [github.com](https://github.com/ScalerLab/JudgeBench)
- [Paper page - Large Language Model Evaluation via Matrix Nuclear-Norm](https://huggingface.co/papers/2410.10672)
   - [github.com](https://github.com/MLGroupJLU/MatrixNuclearNorm)

## Leaderboard

- [TIGER-Lab / MMLU-Pro](https://huggingface.co/spaces/TIGER-Lab/MMLU-Pro/tree/main)

## Projects


## Misc


