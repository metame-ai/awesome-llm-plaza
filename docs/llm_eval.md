# LLM Eval

- [LLM Eval](#llm-eval) 
  - [Survey](#survey)
  - [LLM Evaluation](#llm-evaluation)
  - [Leaderboard](#leaderboard)
  - [Projects](#projects)
  - [Misc](#misc)


## Survey

- **When Benchmarks are Targets: Revealing the Sensitivity of Large Language
  Model Leaderboards**, `arXiv, 2402.01781`, [arxiv](http://arxiv.org/abs/2402.01781v2), [pdf](http://arxiv.org/pdf/2402.01781v2.pdf), cication: [**-1**](None) 

	 *Norah Alzahrani, Hisham Abdullah Alyahya, Yazeed Alnumay, ..., M Saiful Bari, Haidar Khan* 路 ([lm-evaluation-harness](https://github.com/National-Center-for-AI-Saudi-Arabia/lm-evaluation-harness) - National-Center-for-AI-Saudi-Arabia) ![Star](https://img.shields.io/github/stars/National-Center-for-AI-Saudi-Arabia/lm-evaluation-harness.svg?style=social&label=Star)

## LLM Evaluation

- **FrontierMath: A Benchmark for Evaluating Advanced Mathematical Reasoning
  in AI**, `arXiv, 2411.04872`, [arxiv](http://arxiv.org/abs/2411.04872v1), [pdf](http://arxiv.org/pdf/2411.04872v1.pdf), cication: [**-1**](None) 

	 *Elliot Glazer, Ege Erdil, Tamay Besiroglu, ..., Tetiana Grechuk, Shreepranav Varma Enugandla* 路 ([epochai](https://epochai.org/frontiermath)) 路 ([](https://x.com/EpochAIResearch/status/1854993676524831046))
- [SimpleQA that measures the ability for language models to answer short, fact-seeking questions.](https://openai.com/index/introducing-simpleqa/) 

	 路 ([cdn.openai](https://cdn.openai.com/papers/simpleqa.pdf)) 路 ([simple-evals](https://github.com/openai/simple-evals/) - openai) ![Star](https://img.shields.io/github/stars/openai/simple-evals.svg?style=social&label=Star)
- **Are LLMs Better than Reported? Detecting Label Errors and Mitigating 
  Their Effect on Model Performance**, `arXiv, 2410.18889`, [arxiv](http://arxiv.org/abs/2410.18889v1), [pdf](http://arxiv.org/pdf/2410.18889v1.pdf), cication: [**-1**](None)

	 *Omer Nahum, Nitay Calderon, Orgad Keller, ..., Idan Szpektor, Roi Reichart*
- :clapper: [How to Construct Domain Specific LLM Evaluation Systems: Hamel Husain and Emil Sedgh](https://www.youtube.com/watch?v=eLXF0VojuSs) 
-  [Creating a LLM-as-a-Judge That Drives Business Results](https://hamel.dev/blog/posts/llm-judge/) 
- **LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive 
  Memory**, `arXiv, 2410.10813`, [arxiv](http://arxiv.org/abs/2410.10813v1), [pdf](http://arxiv.org/pdf/2410.10813v1.pdf), cication: [**-1**](None)

	 *Di Wu, Hongwei Wang, Wenhao Yu, ..., Kai-Wei Chang, Dong Yu*
- **CompassJudger-1: All-in-one Judge Model Helps Model Evaluation and 
  Evolution**, `arXiv, 2410.16256`, [arxiv](http://arxiv.org/abs/2410.16256v1), [pdf](http://arxiv.org/pdf/2410.16256v1.pdf), cication: [**-1**](None)

	 *Maosong Cao, Alexander Lam, Haodong Duan, ..., Songyang Zhang, Kai Chen* 路 ([CompassJudger](https://github.com/open-compass/CompassJudger) - open-compass) ![Star](https://img.shields.io/github/stars/open-compass/CompassJudger.svg?style=social&label=Star)
- **UCFE: A User-Centric Financial Expertise Benchmark for Large Language 
  Models**, `arXiv, 2410.14059`, [arxiv](http://arxiv.org/abs/2410.14059v2), [pdf](http://arxiv.org/pdf/2410.14059v2.pdf), cication: [**-1**](None)

	 *Yuzhe Yang, Yifei Zhang, Yan Hu, ..., Honghai Yu, Benyou Wang*
- **MixEval-X: Any-to-Any Evaluations from Real-World Data Mixtures**, `arXiv, 2410.13754`, [arxiv](http://arxiv.org/abs/2410.13754v2), [pdf](http://arxiv.org/pdf/2410.13754v2.pdf), cication: [**-1**](None) 

	 *Jinjie Ni, Yifan Song, Deepanway Ghosal, ..., Yang You, Michael Shieh*
- **JudgeBench: A Benchmark for Evaluating LLM-based Judges**, `arXiv, 2410.12784`, [arxiv](http://arxiv.org/abs/2410.12784v1), [pdf](http://arxiv.org/pdf/2410.12784v1.pdf), cication: [**-1**](None) 

	 *Sijun Tan, Siyuan Zhuang, Kyle Montgomery, ..., Raluca Ada Popa, Ion Stoica* 路 ([JudgeBench](https://github.com/ScalerLab/JudgeBench) - ScalerLab) ![Star](https://img.shields.io/github/stars/ScalerLab/JudgeBench.svg?style=social&label=Star)
- **Large Language Model Evaluation via Matrix Nuclear-Norm**, `arXiv, 2410.10672`, [arxiv](http://arxiv.org/abs/2410.10672v1), [pdf](http://arxiv.org/pdf/2410.10672v1.pdf), cication: [**-1**](None) 

	 *Yahan Li, Tingyu Xia, Yi Chang, ..., Yuan Wu* 路 ([MatrixNuclearNorm](https://github.com/MLGroupJLU/MatrixNuclearNorm) - MLGroupJLU) ![Star](https://img.shields.io/github/stars/MLGroupJLU/MatrixNuclearNorm.svg?style=social&label=Star)

## Leaderboard

- [TIGER-Lab / MMLU-Pro](https://huggingface.co/spaces/TIGER-Lab/MMLU-Pro/tree/main)  

## Projects

-  [**simple-evals**](https://github.com/openai/simple-evals) - openai ![Star](https://img.shields.io/github/stars/openai/simple-evals.svg?style=social&label=Star) 

## Misc

- [Chatbot Arena Categories](https://lmarena.github.io/blog/2024/arena-category/) 

	 路 ([x](https://x.com/lmarena_ai/status/1852400728935150017))