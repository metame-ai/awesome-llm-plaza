# LLM Eval

- [LLM Eval](#llm-eval) 
  - [Survey](#survey)
  - [LLM Evaluation](#llm-evaluation)
  - [Leaderboard](#leaderboard)
  - [Projects](#projects)
  - [Misc](#misc)


## Survey

- **A Survey on LLM-as-a-Judge**, `arXiv, 2411.15594`, [arxiv](http://arxiv.org/abs/2411.15594v1), [pdf](http://arxiv.org/pdf/2411.15594v1.pdf), cication: [**-1**](None) 

	 *Jiawei Gu, Xuhui Jiang, Zhichao Shi, ..., Yuanzhuo Wang, Jian Guo*
- **When Benchmarks are Targets: Revealing the Sensitivity of Large Language 
  Model Leaderboards**, `arXiv, 2402.01781`, [arxiv](http://arxiv.org/abs/2402.01781v2), [pdf](http://arxiv.org/pdf/2402.01781v2.pdf), cication: [**-1**](None) 

	 *Norah Alzahrani, Hisham Abdullah Alyahya, Yazeed Alnumay, ..., M Saiful Bari, Haidar Khan* · ([lm-evaluation-harness](https://github.com/National-Center-for-AI-Saudi-Arabia/lm-evaluation-harness) - National-Center-for-AI-Saudi-Arabia) ![Star](https://img.shields.io/github/stars/National-Center-for-AI-Saudi-Arabia/lm-evaluation-harness.svg?style=social&label=Star)

## LLM Evaluation

- **Recitation over Reasoning: How Cutting-Edge Language Models Can Fail on
  Elementary School-Level Reasoning Problems?**, `arXiv, 2504.00509`, [arxiv](http://arxiv.org/abs/2504.00509v2), [pdf](http://arxiv.org/pdf/2504.00509v2.pdf), cication: [**-1**](None) 

	 *Kai Yan, Yufei Xu, Zhengyin Du, ..., Xiaowen Guo, Jiecao Chen*
- [Evaluating AI’s Ability to Replicate AI Research.](https://openai.com/index/paperbench/) 
- [Quickly Start Evaluating LLMs With OpenEvals](https://blog.langchain.dev/evaluating-llms-with-openevals/) 

	 · ([openevals](https://github.com/langchain-ai/openevals) - langchain-ai) ![Star](https://img.shields.io/github/stars/langchain-ai/openevals.svg?style=social&label=Star) · ([𝕏](https://x.com/LangChainAI/status/1894821108018262297))
- **BIG-Bench Extra Hard**, `arXiv, 2502.19187`, [arxiv](http://arxiv.org/abs/2502.19187v1), [pdf](http://arxiv.org/pdf/2502.19187v1.pdf), cication: [**-1**](None) 

	 *Mehran Kazemi, Bahare Fatemi, Hritik Bansal, ..., Quoc V. Le, Orhan Firat* · ([𝕏](https://x.com/kazemi_sm/status/1894935166952349955)) · ([bbeh](https://github.com/google-deepmind/bbeh) - google-deepmind) ![Star](https://img.shields.io/github/stars/google-deepmind/bbeh.svg?style=social&label=Star)
- **SuperGPQA: Scaling LLM Evaluation across 285 Graduate Disciplines**, `arXiv, 2502.14739`, [arxiv](http://arxiv.org/abs/2502.14739v3), [pdf](http://arxiv.org/pdf/2502.14739v3.pdf), cication: [**-1**](None) 

	 *M-A-P Team, Xinrun Du, Yifan Yao, ..., Wenhao Huang, Ge Zhang* · ([supergpqa.github](https://supergpqa.github.io/))
- 🌟 [EnigmaEval, a benchmark derived from puzzle hunts—a repository of sophisticated problems from the global puzzle-solving community.](https://scale.com/leaderboard/enigma_eval) 

	 · ([𝕏](https://x.com/alexandr_wang/status/1891208692751638939?s=46))
- 🌟 **Expect the Unexpected: FailSafe Long Context QA for Finance**, `arXiv, 2502.06329`, [arxiv](http://arxiv.org/abs/2502.06329v1), [pdf](http://arxiv.org/pdf/2502.06329v1.pdf), cication: [**-1**](None) 

	 *Kiran Kamble, Melisa Russak, Dmytro Mozolevskyi, ..., Mateusz Russak, Waseem AlShikh* · ([huggingface](https://huggingface.co/datasets/Writer/FailSafeQA))
- **BenchMAX: A Comprehensive Multilingual Evaluation Suite for Large 
  Language Models**, `arXiv, 2502.07346`, [arxiv](http://arxiv.org/abs/2502.07346v1), [pdf](http://arxiv.org/pdf/2502.07346v1.pdf), cication: [**-1**](None) 

	 *Xu Huang, Wenhao Zhu, Hanxu Hu, ..., Shujian Huang, Fei Yuan* · ([huggingface](https://huggingface.co/collections/LLaMAX/benchmax-674d7a815a57baf97b5539f4))) · ([BenchMAX.git)](https://github.com/CONE-MT/BenchMAX.git)) - CONE-MT) ![Star](https://img.shields.io/github/stars/CONE-MT/BenchMAX.git).svg?style=social&label=Star)
- **Atla Selene Mini: A General Purpose Evaluation Model**, `arXiv, 2501.17195`, [arxiv](http://arxiv.org/abs/2501.17195v1), [pdf](http://arxiv.org/pdf/2501.17195v1.pdf), cication: [**-1**](None) 

	 *Andrei Alexandru, Antonia Calvi, Henry Broomfield, ..., Toby Drane, Young Sun Park* · ([huggingface](https://huggingface.co/AtlaAI/Selene-1-Mini-Llama-3.1-8B))
- [MathArena: Evaluating LLMs on Uncontaminated Math Competitions](https://matharena.ai/) 
- 🌟 [Humanity's Last Exam](https://lastexam.ai/) 

	 · ([hle](https://github.com/centerforaisafety/hle) - centerforaisafety) ![Star](https://img.shields.io/github/stars/centerforaisafety/hle.svg?style=social&label=Star)
- [Holistic Evaluation of Vision-Language Models](https://crfm.stanford.edu/helm/vhelm/v2.1.1/) 
- **The FACTS Grounding Leaderboard: Benchmarking LLMs' Ability to Ground 
  Responses to Long-Form Input**, `arXiv, 2501.03200`, [arxiv](http://arxiv.org/abs/2501.03200v1), [pdf](http://arxiv.org/pdf/2501.03200v1.pdf), cication: [**-1**](None) 

	 *Alon Jacovi, Andrew Wang, Chris Alberti, ..., Sasha Goldshtein, Dipanjan Das* · ([kaggle](https://www.kaggle.com/facts-leaderboard))
- [Scalable Evaluation of Large Language Models](https://llm-class.github.io/speakers.html) 
- **MMLU-CF: A Contamination-free Multi-task Language Understanding 
  Benchmark**, `arXiv, 2412.15194`, [arxiv](http://arxiv.org/abs/2412.15194v1), [pdf](http://arxiv.org/pdf/2412.15194v1.pdf), cication: [**1**](https://scholar.google.com/scholar?cites=7641613824716151032&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII) 

	 *Qihao Zhao, Yangyu Huang, Tengchao Lv, ..., Scarlett Li, Furu Wei* · ([MMLU-CF](https://github.com/microsoft/MMLU-CF) - microsoft) ![Star](https://img.shields.io/github/stars/microsoft/MMLU-CF.svg?style=social&label=Star) · ([huggingface](https://huggingface.co/datasets/microsoft/MMLU-CF.))
- **LLM-as-an-Interviewer: Beyond Static Testing Through Dynamic LLM 
  Evaluation**, `arXiv, 2412.10424`, [arxiv](http://arxiv.org/abs/2412.10424v2), [pdf](http://arxiv.org/pdf/2412.10424v2.pdf), cication: [**-1**](None) 

	 *Eunsu Kim, Juyoung Suk, Seungone Kim, ..., Dongkwan Kim, Alice Oh* · ([𝕏](https://x.com/euns0o_kim/status/1874653915771539744))
- **LongDocURL: a Comprehensive Multimodal Long Document Benchmark 
  Integrating Understanding, Reasoning, and Locating**, `arXiv, 2412.18424`, [arxiv](http://arxiv.org/abs/2412.18424v2), [pdf](http://arxiv.org/pdf/2412.18424v2.pdf), cication: [**-1**](None) 

	 *Chao Deng, Jiale Yuan, Pi Bu, ..., Bo Zheng, Cheng-Lin Liu*
- **Smaller Language Models Are Better Instruction Evolvers**, `arXiv, 2412.11231`, [arxiv](http://arxiv.org/abs/2412.11231v1), [pdf](http://arxiv.org/pdf/2412.11231v1.pdf), cication: [**-1**](None) 

	 *Tingfeng Hui, Lulu Zhao, Guanting Dong, ..., Hua Zhou, Sen Su* · ([Evolution-Analysis](https://github.com/HypherX/Evolution-Analysis) - HypherX) ![Star](https://img.shields.io/github/stars/HypherX/Evolution-Analysis.svg?style=social&label=Star) · ([𝕏](https://x.com/kakakbibibi/status/1868867253594472804))
- **In Case You Missed It: ARC 'Challenge' Is Not That Challenging**, `arXiv, 2412.17758`, [arxiv](http://arxiv.org/abs/2412.17758v1), [pdf](http://arxiv.org/pdf/2412.17758v1.pdf), cication: [**-1**](None) 

	 *Łukasz Borchmann*
- [FACTS Grounding: A new benchmark for evaluating the factuality of large language models](https://deepmind.google/discover/blog/facts-grounding-a-new-benchmark-for-evaluating-the-factuality-of-large-language-models/) 
- **OmniDocBench: Benchmarking Diverse PDF Document Parsing with 
  Comprehensive Annotations**, `arXiv, 2412.07626`, [arxiv](http://arxiv.org/abs/2412.07626v1), [pdf](http://arxiv.org/pdf/2412.07626v1.pdf), cication: [**-1**](None) 

	 *Linke Ouyang, Yuan Qu, Hongbin Zhou, ..., Zhongying Tu, Conghui He* · ([OmniDocBench.](https://github.com/opendatalab/OmniDocBench.) - opendatalab) ![Star](https://img.shields.io/github/stars/opendatalab/OmniDocBench..svg?style=social&label=Star)
- **INCLUDE: Evaluating Multilingual Language Understanding with Regional 
  Knowledge**, `arXiv, 2411.19799`, [arxiv](http://arxiv.org/abs/2411.19799v1), [pdf](http://arxiv.org/pdf/2411.19799v1.pdf), cication: [**-1**](None) 

	 *Angelika Romanou, Negar Foroutan, Anna Sotnikova, ..., Sara Hooker, Antoine Bosselut* · ([𝕏](https://x.com/agromanou/status/1863604966180479187))
- [Finetuning LLM Judges for Evaluation](https://substack.com/home/post/p-151216391) 
- **Multi-IF: Benchmarking LLMs on Multi-Turn and Multilingual Instructions 
  Following**, `arXiv, 2410.15553`, [arxiv](http://arxiv.org/abs/2410.15553v2), [pdf](http://arxiv.org/pdf/2410.15553v2.pdf), cication: [**-1**](None) 

	 *Yun He, Di Jin, Chaoqi Wang, ..., Han Fang, Sinong Wang*
- **From Generation to Judgment: Opportunities and Challenges of 
  LLM-as-a-judge**, `arXiv, 2411.16594`, [arxiv](http://arxiv.org/abs/2411.16594v1), [pdf](http://arxiv.org/pdf/2411.16594v1.pdf), cication: [**-1**](None) 

	 *Dawei Li, Bohan Jiang, Liangjie Huang, ..., Lu Cheng, Huan Liu* · ([llm-as-a-judge.github](https://llm-as-a-judge.github.io)) · ([Awesome-LLM-as-a-judge](https://github.com/llm-as-a-judge/Awesome-LLM-as-a-judge) - llm-as-a-judge) ![Star](https://img.shields.io/github/stars/llm-as-a-judge/Awesome-LLM-as-a-judge.svg?style=social&label=Star)
- **All Languages Matter: Evaluating LMMs on Culturally Diverse 100 
  Languages**, `arXiv, 2411.16508`, [arxiv](http://arxiv.org/abs/2411.16508v2), [pdf](http://arxiv.org/pdf/2411.16508v2.pdf), cication: [**-1**](None) 

	 *Ashmal Vayani, Dinura Dissanayake, Hasindri Watawana, ..., Salman Khan, Fahad Khan*
- [Enhancing Evaluation Coverage and Validation with Language Models](https://www.youtube.com/watch?v=Gp6Na2mdKDs&list=PLWRU-w8UhT6jNg64UfBB0VtlvI4Upe914&index=7)  :clapper: 
- [Emotional Intelligence Benchmark for LLMs](https://eqbench.com/judgemark.html) 
- **Adding Error Bars to Evals: A Statistical Approach to Language Model 
  Evaluations**, `arXiv, 2411.00640`, [arxiv](http://arxiv.org/abs/2411.00640v1), [pdf](http://arxiv.org/pdf/2411.00640v1.pdf), cication: [**-1**](None) 

	 *Evan Miller*

	 · ([anthropic](https://www.anthropic.com/research/statistical-approach-to-model-evals))
- **Chinese SimpleQA: A Chinese Factuality Evaluation for Large Language 
  Models**, `arXiv, 2411.07140`, [arxiv](http://arxiv.org/abs/2411.07140v2), [pdf](http://arxiv.org/pdf/2411.07140v2.pdf), cication: [**-1**](None) 

	 *Yancheng He, Shilong Li, Jiaheng Liu, ..., Wenbo Su, Bo Zheng*
- [Mira: A Decentralized Network for Trustless AI Output Verification](https://mira.network/research/mira-whitepaper.pdf) 

	 · ([mira](https://mira.network/)) · ([huggingface](https://huggingface.co/datasets/Mira-Network/ensemble-validation?row=0))
- **FrontierMath: A Benchmark for Evaluating Advanced Mathematical Reasoning 
  in AI**, `arXiv, 2411.04872`, [arxiv](http://arxiv.org/abs/2411.04872v1), [pdf](http://arxiv.org/pdf/2411.04872v1.pdf), cication: [**-1**](None) 

	 *Elliot Glazer, Ege Erdil, Tamay Besiroglu, ..., Tetiana Grechuk, Shreepranav Varma Enugandla* · ([epochai](https://epochai.org/frontiermath)) · ([𝕏](https://x.com/EpochAIResearch/status/1854993676524831046))
- [SimpleQA that measures the ability for language models to answer short, fact-seeking questions.](https://openai.com/index/introducing-simpleqa/) 

	 · ([cdn.openai](https://cdn.openai.com/papers/simpleqa.pdf)) · ([simple-evals](https://github.com/openai/simple-evals/) - openai) ![Star](https://img.shields.io/github/stars/openai/simple-evals.svg?style=social&label=Star)
- **Are LLMs Better than Reported? Detecting Label Errors and Mitigating 
  Their Effect on Model Performance**, `arXiv, 2410.18889`, [arxiv](http://arxiv.org/abs/2410.18889v1), [pdf](http://arxiv.org/pdf/2410.18889v1.pdf), cication: [**-1**](None)

	 *Omer Nahum, Nitay Calderon, Orgad Keller, ..., Idan Szpektor, Roi Reichart*
- :clapper: [How to Construct Domain Specific LLM Evaluation Systems: Hamel Husain and Emil Sedgh](https://www.youtube.com/watch?v=eLXF0VojuSs) 
- 🌟 [Creating a LLM-as-a-Judge That Drives Business Results](https://hamel.dev/blog/posts/llm-judge/) 
- **LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive 
  Memory**, `arXiv, 2410.10813`, [arxiv](http://arxiv.org/abs/2410.10813v1), [pdf](http://arxiv.org/pdf/2410.10813v1.pdf), cication: [**-1**](None)

	 *Di Wu, Hongwei Wang, Wenhao Yu, ..., Kai-Wei Chang, Dong Yu*
- **CompassJudger-1: All-in-one Judge Model Helps Model Evaluation and 
  Evolution**, `arXiv, 2410.16256`, [arxiv](http://arxiv.org/abs/2410.16256v1), [pdf](http://arxiv.org/pdf/2410.16256v1.pdf), cication: [**-1**](None)

	 *Maosong Cao, Alexander Lam, Haodong Duan, ..., Songyang Zhang, Kai Chen* · ([CompassJudger](https://github.com/open-compass/CompassJudger) - open-compass) ![Star](https://img.shields.io/github/stars/open-compass/CompassJudger.svg?style=social&label=Star)
- **UCFE: A User-Centric Financial Expertise Benchmark for Large Language 
  Models**, `arXiv, 2410.14059`, [arxiv](http://arxiv.org/abs/2410.14059v2), [pdf](http://arxiv.org/pdf/2410.14059v2.pdf), cication: [**-1**](None)

	 *Yuzhe Yang, Yifei Zhang, Yan Hu, ..., Honghai Yu, Benyou Wang*
- **MixEval-X: Any-to-Any Evaluations from Real-World Data Mixtures**, `arXiv, 2410.13754`, [arxiv](http://arxiv.org/abs/2410.13754v2), [pdf](http://arxiv.org/pdf/2410.13754v2.pdf), cication: [**-1**](None) 

	 *Jinjie Ni, Yifan Song, Deepanway Ghosal, ..., Yang You, Michael Shieh*
- **JudgeBench: A Benchmark for Evaluating LLM-based Judges**, `arXiv, 2410.12784`, [arxiv](http://arxiv.org/abs/2410.12784v1), [pdf](http://arxiv.org/pdf/2410.12784v1.pdf), cication: [**-1**](None) 

	 *Sijun Tan, Siyuan Zhuang, Kyle Montgomery, ..., Raluca Ada Popa, Ion Stoica* · ([JudgeBench](https://github.com/ScalerLab/JudgeBench) - ScalerLab) ![Star](https://img.shields.io/github/stars/ScalerLab/JudgeBench.svg?style=social&label=Star)
- **Large Language Model Evaluation via Matrix Nuclear-Norm**, `arXiv, 2410.10672`, [arxiv](http://arxiv.org/abs/2410.10672v1), [pdf](http://arxiv.org/pdf/2410.10672v1.pdf), cication: [**-1**](None) 

	 *Yahan Li, Tingyu Xia, Yi Chang, ..., Yuan Wu* · ([MatrixNuclearNorm](https://github.com/MLGroupJLU/MatrixNuclearNorm) - MLGroupJLU) ![Star](https://img.shields.io/github/stars/MLGroupJLU/MatrixNuclearNorm.svg?style=social&label=Star)

## Leaderboard

- **Prompt-to-Leaderboard**, `arXiv, 2502.14855`, [arxiv](http://arxiv.org/abs/2502.14855v1), [pdf](http://arxiv.org/pdf/2502.14855v1.pdf), cication: [**-1**](None) 

	 *Evan Frick, Connor Chen, Joseph Tennyson, ..., Anastasios N. Angelopoulos, Ion Stoica* · ([𝕏](https://x.com/lmarena_ai/status/1894767022791438490)) · ([p2l](https://github.com/lmarena/p2l) - lmarena) ![Star](https://img.shields.io/github/stars/lmarena/p2l.svg?style=social&label=Star)
- [Judge Arena: Benchmarking LLMs as Evaluators](https://huggingface.co/blog/arena-atla)  🤗 

	 · ([huggingface](https://huggingface.co/spaces/AtlaAI/judge-arena))
- [TIGER-Lab / MMLU-Pro](https://huggingface.co/spaces/TIGER-Lab/MMLU-Pro/tree/main) 🤗 

## Projects

- [**Sudoku-Bench**](https://github.com/SakanaAI/Sudoku-Bench) - SakanaAI ![Star](https://img.shields.io/github/stars/SakanaAI/Sudoku-Bench.svg?style=social&label=Star) 
- [**QwQ**](https://github.com/QwenLM/QwQ) - QwenLM ![Star](https://img.shields.io/github/stars/QwenLM/QwQ.svg?style=social&label=Star) 

	 · ([huggingface](https://huggingface.co/Qwen/QwQ-32B))
- [**GamingAgent**](https://github.com/lmgame-org/GamingAgent) - lmgame-org ![Star](https://img.shields.io/github/stars/lmgame-org/GamingAgent.svg?style=social&label=Star) 
- [**ZeroSumEval**](https://github.com/haidark/ZeroSumEval) - haidark ![Star](https://img.shields.io/github/stars/haidark/ZeroSumEval.svg?style=social&label=Star) 
- [**generalization**](https://github.com/lechmazur/generalization) - lechmazur ![Star](https://img.shields.io/github/stars/lechmazur/generalization.svg?style=social&label=Star) 
- [**MisguidedAttention**](https://github.com/cpldcpu/MisguidedAttention) - cpldcpu ![Star](https://img.shields.io/github/stars/cpldcpu/MisguidedAttention.svg?style=social&label=Star) 
- [**mini_dev**](https://github.com/bird-bench/mini_dev) - bird-bench ![Star](https://img.shields.io/github/stars/bird-bench/mini_dev.svg?style=social&label=Star) 

	 · ([𝕏](https://x.com/Subhash_Peshwa/status/1869204331372888513))
- [**Awesome-LLMs-as-Judges**](https://github.com/CSHaitao/Awesome-LLMs-as-Judges) - CSHaitao ![Star](https://img.shields.io/github/stars/CSHaitao/Awesome-LLMs-as-Judges.svg?style=social&label=Star) 
- [**evaluation-guidebook**](https://github.com/huggingface/evaluation-guidebook/blob/main/contents/automated-benchmarks/some-evaluation-datasets.md) - huggingface ![Star](https://img.shields.io/github/stars/huggingface/evaluation-guidebook.svg?style=social&label=Star) 

	 · ([𝕏](https://x.com/clefourrier/status/1861487337936388448))
- [**CS-Eval**](https://github.com/CS-EVAL/CS-Eval) - CS-EVAL ![Star](https://img.shields.io/github/stars/CS-EVAL/CS-Eval.svg?style=social&label=Star) 
- [**RPBench-Auto**](https://github.com/boson-ai/RPBench-Auto) - boson-ai ![Star](https://img.shields.io/github/stars/boson-ai/RPBench-Auto.svg?style=social&label=Star) 

	 · ([boson](https://boson.ai/rpbench-blog/))
- [**documind**](https://github.com/DocumindHQ/documind) - DocumindHQ ![Star](https://img.shields.io/github/stars/DocumindHQ/documind.svg?style=social&label=Star) 
- 🌟 [**evalchemy**](https://github.com/mlfoundations/evalchemy) - mlfoundations ![Star](https://img.shields.io/github/stars/mlfoundations/evalchemy.svg?style=social&label=Star) 
- [**OLMo-Eval**](https://github.com/allenai/OLMo-Eval) - allenai ![Star](https://img.shields.io/github/stars/allenai/OLMo-Eval.svg?style=social&label=Star) 
- [**olmes**](https://github.com/allenai/olmes) - allenai ![Star](https://img.shields.io/github/stars/allenai/olmes.svg?style=social&label=Star) 
- 🌟 [**simple-evals**](https://github.com/openai/simple-evals) - openai ![Star](https://img.shields.io/github/stars/openai/simple-evals.svg?style=social&label=Star) 

## Misc

- [AIME 2025 performance of Grok and OpenAI models](https://x.com/teortaxesTex/status/1892471638534303946)  𝕏 
- [AI Model Comparison](https://countless.dev/) 
- [LLM LeaderboardsA trusted source for real-world AI model performance and insights](https://llm-stats.com/) 
- [Crisp and fuzzy tasks](https://aligned.substack.com/p/crisp-and-fuzzy-tasks) 
- [Letting Large Models Debate: The First Multilingual LLM Debate Competition](https://huggingface.co/blog/debate)  🤗 
- [Say What You Mean: A Response to 'Let Me Speak Freely'](https://blog.dottxt.co/say-what-you-mean.html) 
- [Chatbot Arena Categories](https://lmarena.github.io/blog/2024/arena-category/) 

	 · ([x](https://x.com/lmarena_ai/status/1852400728935150017))