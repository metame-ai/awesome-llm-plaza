# LLM Eval

- [LLM Eval](#llm-eval) 
  - [Survey](#survey)
  - [LLM Evaluation](#llm-evaluation)
  - [Leaderboard](#leaderboard)
  - [Projects](#projects)
  - [Misc](#misc)


## Survey


## LLM Evaluation

- [SimpleQA that measures the ability for language models to answer short, fact-seeking questions.](https://openai.com/index/introducing-simpleqa/) 

	 · ([cdn.openai](https://cdn.openai.com/papers/simpleqa.pdf)) · ([simple-evals](https://github.com/openai/simple-evals/) - openai) ![Star](https://img.shields.io/github/stars/openai/simple-evals.svg?style=social&label=Star)
- **Are LLMs Better than Reported? Detecting Label Errors and Mitigating 
  Their Effect on Model Performance**, `arXiv, 2410.18889`, [arxiv](http://arxiv.org/abs/2410.18889v1), [pdf](http://arxiv.org/pdf/2410.18889v1.pdf), cication: [**-1**](None)

	 *Omer Nahum, Nitay Calderon, Orgad Keller, ..., Idan Szpektor, Roi Reichart*
- :clapper: [How to Construct Domain Specific LLM Evaluation Systems: Hamel Husain and Emil Sedgh](https://www.youtube.com/watch?v=eLXF0VojuSs) 
- :star2: [Creating a LLM-as-a-Judge That Drives Business Results](https://hamel.dev/blog/posts/llm-judge/) 
- **LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive 
  Memory**, `arXiv, 2410.10813`, [arxiv](http://arxiv.org/abs/2410.10813v1), [pdf](http://arxiv.org/pdf/2410.10813v1.pdf), cication: [**-1**](None)

	 *Di Wu, Hongwei Wang, Wenhao Yu, ..., Kai-Wei Chang, Dong Yu*
- **CompassJudger-1: All-in-one Judge Model Helps Model Evaluation and 
  Evolution**, `arXiv, 2410.16256`, [arxiv](http://arxiv.org/abs/2410.16256v1), [pdf](http://arxiv.org/pdf/2410.16256v1.pdf), cication: [**-1**](None)

	 *Maosong Cao, Alexander Lam, Haodong Duan, ..., Songyang Zhang, Kai Chen* · ([CompassJudger](https://github.com/open-compass/CompassJudger) - open-compass) ![Star](https://img.shields.io/github/stars/open-compass/CompassJudger.svg?style=social&label=Star)
- **UCFE: A User-Centric Financial Expertise Benchmark for Large Language 
  Models**, `arXiv, 2410.14059`, [arxiv](http://arxiv.org/abs/2410.14059v2), [pdf](http://arxiv.org/pdf/2410.14059v2.pdf), cication: [**-1**](None)

	 *Yuzhe Yang, Yifei Zhang, Yan Hu, ..., Honghai Yu, Benyou Wang*
- **MixEval-X: Any-to-Any Evaluations from Real-World Data Mixtures**, `arXiv, 2410.13754`, [arxiv](http://arxiv.org/abs/2410.13754v2), [pdf](http://arxiv.org/pdf/2410.13754v2.pdf), cication: [**-1**](None) 

	 *Jinjie Ni, Yifan Song, Deepanway Ghosal, ..., Yang You, Michael Shieh*
- **JudgeBench: A Benchmark for Evaluating LLM-based Judges**, `arXiv, 2410.12784`, [arxiv](http://arxiv.org/abs/2410.12784v1), [pdf](http://arxiv.org/pdf/2410.12784v1.pdf), cication: [**-1**](None) 

	 *Sijun Tan, Siyuan Zhuang, Kyle Montgomery, ..., Raluca Ada Popa, Ion Stoica* · ([JudgeBench](https://github.com/ScalerLab/JudgeBench) - ScalerLab) ![Star](https://img.shields.io/github/stars/ScalerLab/JudgeBench.svg?style=social&label=Star)
- **Large Language Model Evaluation via Matrix Nuclear-Norm**, `arXiv, 2410.10672`, [arxiv](http://arxiv.org/abs/2410.10672v1), [pdf](http://arxiv.org/pdf/2410.10672v1.pdf), cication: [**-1**](None) 

	 *Yahan Li, Tingyu Xia, Yi Chang, ..., Yuan Wu* · ([MatrixNuclearNorm](https://github.com/MLGroupJLU/MatrixNuclearNorm) - MLGroupJLU) ![Star](https://img.shields.io/github/stars/MLGroupJLU/MatrixNuclearNorm.svg?style=social&label=Star)

## Leaderboard

- [TIGER-Lab / MMLU-Pro](https://huggingface.co/spaces/TIGER-Lab/MMLU-Pro/tree/main)  🤗

## Projects

- :star2: [**simple-evals**](https://github.com/openai/simple-evals) - openai ![Star](https://img.shields.io/github/stars/openai/simple-evals.svg?style=social&label=Star) 

## Misc
## Misc
- [Chatbot Arena Categories](https://lmarena.github.io/blog/2024/arena-category/) 

	 · ([x](https://x.com/lmarena_ai/status/1852400728935150017))