# LLM Model

- [LLM Model](#llm-model) 
  - [Survey](#survey)
  - [LLM Models](#llm-models)
  - [State Space Model](#state-space-model)
  - [Projects](#projects)
  - [Misc](#misc)


## Survey

- **Technologies on Effectiveness and Efficiency: A Survey of State Spaces 
  Models**, `arXiv, 2503.11224`, [arxiv](http://arxiv.org/abs/2503.11224v1), [pdf](http://arxiv.org/pdf/2503.11224v1.pdf), cication: [**-1**](None) 

	 *Xingtai Lv, Youbang Sun, Kaiyan Zhang, ..., Ning Ding, Bowen Zhou*

## LLM Models

- [The Big LLM Architecture Comparison](https://magazine.sebastianraschka.com/p/the-big-llm-architecture-comparison) 
- **Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive
  Token-Level Computation**, `arXiv, 2507.10524`, [arxiv](http://arxiv.org/abs/2507.10524v2), [pdf](http://arxiv.org/pdf/2507.10524v2.pdf), cication: [**-1**](None) 

	 *Sangmin Bae, Yujin Kim, Reza Bayat, ..., Aaron Courville, Se-Young Yun* · ([mixture_of_recursions](https://github.com/raymin0223/mixture_of_recursions) - raymin0223) ![Star](https://img.shields.io/github/stars/raymin0223/mixture_of_recursions.svg?style=social&label=Star)
- **The Diffusion Duality**, `arXiv, 2506.10892`, [arxiv](http://arxiv.org/abs/2506.10892v1), [pdf](http://arxiv.org/pdf/2506.10892v1.pdf), cication: [**-1**](None) 

	 *Subham Sekhar Sahoo, Justin Deschenaux, Aaron Gokaslan, ..., Justin Chiu, Volodymyr Kuleshov*
- **ATLAS: Learning to Optimally Memorize the Context at Test Time**, `arXiv, 2505.23735`, [arxiv](http://arxiv.org/abs/2505.23735v1), [pdf](http://arxiv.org/pdf/2505.23735v1.pdf), cication: [**-1**](None) 

	 *Ali Behrouz, Zeman Li, Praneeth Kacham, ..., Meisam Razaviyayn, Vahab Mirrokni*
- [Gemini DiffusionOur state-of-the-art, experimental text diffusion model](https://deepmind.google/models/gemini-diffusion/) 
- **Continuous Thought Machines**, `arXiv, 2505.05522`, [arxiv](http://arxiv.org/abs/2505.05522v2), [pdf](http://arxiv.org/pdf/2505.05522v2.pdf), cication: [**-1**](None) 

	 *Luke Darlow, Ciaran Regan, Sebastian Risi, ..., Jeffrey Seely, Llion Jones* · ([pub.sakana](https://pub.sakana.ai/ctm/))
- **Chain-of-Model Learning for Language Model**, `arXiv, 2505.11820`, [arxiv](http://arxiv.org/abs/2505.11820v1), [pdf](http://arxiv.org/pdf/2505.11820v1.pdf), cication: [**-1**](None) 

	 *Kaitao Song, Xiaohua Wang, Xu Tan, ..., Dongsheng Li, Lili Qiu*
- **Diffusion vs. Autoregressive Language Models: A Text Embedding 
  Perspective**, `arXiv, 2505.15045`, [arxiv](http://arxiv.org/abs/2505.15045v1), [pdf](http://arxiv.org/pdf/2505.15045v1.pdf), cication: [**-1**](None) 

	 *Siyue Zhang, Yilun Zhao, Liyuan Geng, ..., Anh Tuan Luu, Chen Zhao*
- **Nemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer 
  Models**, `arXiv, 2504.03624`, [arxiv](http://arxiv.org/abs/2504.03624v3), [pdf](http://arxiv.org/pdf/2504.03624v3.pdf), cication: [**-1**](None) 

	 *NVIDIA, :, Aaron Blakeman, ..., Zhuolin Yang, Zijia Chen*
- **RWKV-7 "Goose" with Expressive Dynamic State Evolution**, `arXiv, 2503.14456`, [arxiv](http://arxiv.org/abs/2503.14456v2), [pdf](http://arxiv.org/pdf/2503.14456v2.pdf), cication: [**-1**](None) 

	 *Bo Peng, Ruichong Zhang, Daniel Goldstein, ..., Daniel Wuttke, Christian Zhou-Zheng* · ([RWKV-LM](https://github.com/RWKV/RWKV-LM) - RWKV) ![Star](https://img.shields.io/github/stars/RWKV/RWKV-LM.svg?style=social&label=Star)
- **xLSTM 7B: A Recurrent LLM for Fast and Efficient Inference**, `arXiv, 2503.13427`, [arxiv](http://arxiv.org/abs/2503.13427v1), [pdf](http://arxiv.org/pdf/2503.13427v1.pdf), cication: [**-1**](None) 

	 *Maximilian Beck, Korbinian Pöppel, Phillip Lippe, ..., Sebastian Böck, Sepp Hochreiter* · ([xlstm](https://github.com/NX-AI/xlstm) - NX-AI) ![Star](https://img.shields.io/github/stars/NX-AI/xlstm.svg?style=social&label=Star) · ([huggingface](https://huggingface.co/NX-AI/xLSTM-7b))
- **FFN Fusion: Rethinking Sequential Computation in Large Language Models**, `arXiv, 2503.18908`, [arxiv](http://arxiv.org/abs/2503.18908v1), [pdf](http://arxiv.org/pdf/2503.18908v1.pdf), cication: [**-1**](None) 

	 *Akhiad Bercovich, Mohammad Dabbah, Omri Puny, ..., Ran Zilberstein, Ran El-Yaniv*
- [Introducing Mercury, the first commercial-scale diffusion large language model](https://www.inceptionlabs.ai/news) 

	 · ([𝕏](https://x.com/InceptionAILabs/status/1894847919624462794))
- **Test-time regression: a unifying framework for designing sequence models 
  with associative memory**, `arXiv, 2501.12352`, [arxiv](http://arxiv.org/abs/2501.12352v1), [pdf](http://arxiv.org/pdf/2501.12352v1.pdf), cication: [**-1**](None) 

	 *Ke Alexander Wang, Jiaxin Shi, Emily B. Fox* · ([youtube](https://www.youtube.com/watch?v=C7KnW8VFp4U&ab_channel=ASAPSeminarSeries))
- **Continuous Diffusion Model for Language Modeling**, `arXiv, 2502.11564`, [arxiv](http://arxiv.org/abs/2502.11564v1), [pdf](http://arxiv.org/pdf/2502.11564v1.pdf), cication: [**-1**](None) 

	 *Jaehyeong Jo, Sung Ju Hwang* · ([RDLM](https://github.com/harryjo97/RDLM) - harryjo97) ![Star](https://img.shields.io/github/stars/harryjo97/RDLM.svg?style=social&label=Star)
- **Multimodal Mamba: Decoder-only Multimodal State Space Model via 
  Quadratic to Linear Distillation**, `arXiv, 2502.13145`, [arxiv](http://arxiv.org/abs/2502.13145v1), [pdf](http://arxiv.org/pdf/2502.13145v1.pdf), cication: [**-1**](None) 

	 *Bencheng Liao, Hongyuan Tao, Qian Zhang, ..., Wenyu Liu, Xinggang Wang* · ([mmMamba](https://github.com/hustvl/mmMamba) - hustvl) ![Star](https://img.shields.io/github/stars/hustvl/mmMamba.svg?style=social&label=Star)
- **LLM Pretraining with Continuous Concepts**, `arXiv, 2502.08524`, [arxiv](http://arxiv.org/abs/2502.08524v1), [pdf](http://arxiv.org/pdf/2502.08524v1.pdf), cication: [**-1**](None) 

	 *Jihoon Tack, Jack Lanchantin, Jane Yu, ..., Jason Weston, Xian Li* · ([RAM](https://github.com/facebookresearch/RAM/tree/main/projects/cocomix) - facebookresearch) ![Star](https://img.shields.io/github/stars/facebookresearch/RAM.svg?style=social&label=Star)
- 🌟 **Large Language Diffusion Models**, `arXiv, 2502.09992`, [arxiv](http://arxiv.org/abs/2502.09992v1), [pdf](http://arxiv.org/pdf/2502.09992v1.pdf), cication: [**-1**](None) 

	 *Shen Nie, Fengqi Zhu, Zebin You, ..., Ji-Rong Wen, Chongxuan Li* · ([ml-gsai.github](https://ml-gsai.github.io/LLaDA-demo/))
- **ARWKV: Pretrain is not what we need, an RNN-Attention-Based Language 
  Model Born from Transformer**, `arXiv, 2501.15570`, [arxiv](http://arxiv.org/abs/2501.15570v1), [pdf](http://arxiv.org/pdf/2501.15570v1.pdf), cication: [**-1**](None) 

	 *Lin Yueyu, Li Zhiyuan, Peter Yue, ..., Liu Xiao* · ([huggingface](https://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1))
- **Mixture-of-Mamba: Enhancing Multi-Modal State-Space Models with 
  Modality-Aware Sparsity**, `arXiv, 2501.16295`, [arxiv](http://arxiv.org/abs/2501.16295v1), [pdf](http://arxiv.org/pdf/2501.16295v1.pdf), cication: [**-1**](None) 

	 *Weixin Liang, Junhong Shen, Genghan Zhang, ..., Luke Zettlemoyer, Lili Yu* · ([Mixture-of-Mamba](https://github.com/Weixin-Liang/Mixture-of-Mamba) - Weixin-Liang) ![Star](https://img.shields.io/github/stars/Weixin-Liang/Mixture-of-Mamba.svg?style=social&label=Star)
- **It's All in The [MASK]: Simple Instruction-Tuning Enables BERT-like 
  Masked Language Models As Generative Classifiers**, `arXiv, 2502.03793`, [arxiv](http://arxiv.org/abs/2502.03793v2), [pdf](http://arxiv.org/pdf/2502.03793v2.pdf), cication: [**-1**](None) 

	 *Benjamin Clavié, Nathan Cooper, Benjamin Warner* · ([𝕏](https://x.com/bclavie/status/1888963894296936616))
- **Enabling Autoregressive Models to Fill In Masked Tokens**, `arXiv, 2502.06901`, [arxiv](http://arxiv.org/abs/2502.06901v1), [pdf](http://arxiv.org/pdf/2502.06901v1.pdf), cication: [**-1**](None) 

	 *Daniel Israel, Aditya Grover, Guy Van den Broeck*
- [RWKV (pronounced RwaKuv) is an RNN with great LLM performance,            which can also be directly trained like a GPT transformer            (parallelizable).](https://www.rwkv.com/) 
- [2024 in Post-Transformer Architectures: State Space Models, RWKV [Latent Space LIVE! @ NeurIPS 2024]](http://www.youtube.com/playlist?list=PLWEAb1SXhjlfG63F03R52DZXpHzVB1_5j)  :clapper: 
- [Bamba: Inference-Efficient Hybrid Mamba2 Model](https://huggingface.co/blog/bamba)  🤗 

	 · ([bamba](https://github.com/foundation-model-stack/bamba) - foundation-model-stack) ![Star](https://img.shields.io/github/stars/foundation-model-stack/bamba.svg?style=social&label=Star)
- [QRWKV6 32B Instruct Preview is one of the largest and strongest RWKV model to date.](https://huggingface.co/recursal/QRWKV6-32B-Instruct-Preview-v0.1)  🤗 
- [RWKV Flock of Finches 37B-A11B v0.1 Mixture of Experts Model](https://huggingface.co/recursal/Finch-MoE-37B-A11B-v0.1-HF)  🤗 
- [This xLSTM-7B was pre-trained on the DCLM and selected high-quality data for in a total of approx. 2.3 T tokens using the xlstm-jax framework.](https://huggingface.co/NX-AI/xLSTM-7b)  🤗 
- **Ultra-Sparse Memory Network**, `arXiv, 2411.12364`, [arxiv](http://arxiv.org/abs/2411.12364v1), [pdf](http://arxiv.org/pdf/2411.12364v1.pdf), cication: [**-1**](None) 

	 *Zihao Huang, Qiyang Min, Hongzhi Huang, ..., Ran Guo, Xun Zhou*
- **Bi-Mamba: Towards Accurate 1-Bit State Space Models**, `arXiv, 2411.11843`, [arxiv](http://arxiv.org/abs/2411.11843v1), [pdf](http://arxiv.org/pdf/2411.11843v1.pdf), cication: [**-1**](None) 

	 *Shengkun Tang, Liqun Ma, Haonan Li, ..., Mingjie Sun, Zhiqiang Shen* · ([𝕏](https://x.com/omarsar0/status/1858878654736199850))
- **Learning to (Learn at Test Time): RNNs with Expressive Hidden States**, `arXiv, 2407.04620`, [arxiv](http://arxiv.org/abs/2407.04620v2), [pdf](http://arxiv.org/pdf/2407.04620v2.pdf), cication: [**19**](https://scholar.google.com/scholar?cites=4859112994803550513&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII) 

	 *Yu Sun, Xinhao Li, Karan Dalal, ..., Tatsunori Hashimoto, Carlos Guestrin* · ([yueatsprograms.github](https://yueatsprograms.github.io/ttt/home.html)) · ([ttt-lm-pytorch](https://github.com/test-time-training/ttt-lm-pytorch) - test-time-training) ![Star](https://img.shields.io/github/stars/test-time-training/ttt-lm-pytorch.svg?style=social&label=Star)
- **Wave Network: An Ultra-Small Language Model**, `arXiv, 2411.02674`, [arxiv](http://arxiv.org/abs/2411.02674v3), [pdf](http://arxiv.org/pdf/2411.02674v3.pdf), cication: [**-1**](None) 

	 *Xin Zhang, Victor S. Sheng*
- **TokenFormer: Rethinking Transformer Scaling with Tokenized Model 
  Parameters**, `arXiv, 2410.23168`, [arxiv](http://arxiv.org/abs/2410.23168v1), [pdf](http://arxiv.org/pdf/2410.23168v1.pdf), cication: [**-1**](None)

	 *Haiyang Wang, Yue Fan, Muhammad Ferjad Naeem, ..., Federico Tombari, Bernt Schiele* · ([TokenFormer](https://github.com/Haiyang-W/TokenFormer) - Haiyang-W) ![Star](https://img.shields.io/github/stars/Haiyang-W/TokenFormer.svg?style=social&label=Star)
- **MrT5: Dynamic Token Merging for Efficient Byte-level Language Models**, `arXiv, 2410.20771`, [arxiv](http://arxiv.org/abs/2410.20771v1), [pdf](http://arxiv.org/pdf/2410.20771v1.pdf), cication: [**-1**](None) 

	 *Julie Kallini, Shikhar Murty, Christopher D. Manning, ..., Christopher Potts, Róbert Csordás*
- **Scaling Diffusion Language Models via Adaptation from Autoregressive 
  Models**, `arXiv, 2410.17891`, [arxiv](http://arxiv.org/abs/2410.17891v1), [pdf](http://arxiv.org/pdf/2410.17891v1.pdf), cication: [**-1**](None)

	 *Shansan Gong, Shivam Agarwal, Yizhe Zhang, ..., Hao Peng, Lingpeng Kong*

	 · ([arxiv](https://arxiv.org/abs/2410.17891)) · ([DiffuLLaMA](https://github.com/HKUNLP/DiffuLLaMA) - HKUNLP) ![Star](https://img.shields.io/github/stars/HKUNLP/DiffuLLaMA.svg?style=social&label=Star) · ([huggingface](https://huggingface.co/diffusionfamily))

## State Space Model

- [AI21 Jamba 1.6 family of models is state-of-the-art, hybrid SSM-Transformer instruction following foundation models.](https://huggingface.co/ai21labs/AI21-Jamba-Large-1.6)  🤗 
- [What’s Next for Mamba? Towards More Expressive Recurrent Update Rules”](https://sustcsonglin.github.io/) 

## Projects

- [**MMaDA**](https://github.com/Gen-Verse/MMaDA) - Gen-Verse ![Star](https://img.shields.io/github/stars/Gen-Verse/MMaDA.svg?style=social&label=Star) 
- [**dKV-Cache**](https://github.com/horseee/dKV-Cache) - horseee ![Star](https://img.shields.io/github/stars/horseee/dKV-Cache.svg?style=social&label=Star) 

	 *The Cache for Diffusion Language Models*
- [**large_concept_model**](https://github.com/facebookresearch/large_concept_model) - facebookresearch ![Star](https://img.shields.io/github/stars/facebookresearch/large_concept_model.svg?style=social&label=Star) 

	 · ([ai.meta](https://ai.meta.com/blog/meta-fair-updates-agents-robustness-safety-architecture/)) · ([ai.meta](https://ai.meta.com/research/publications/large-concept-models-language-modeling-in-a-sentence-representation-space/)) · ([𝕏](https://x.com/melbayad/status/1867689627189952956))

## Misc
## Misc
- [why don't people build transformers with recursive blocks?](https://x.com/jxmnop/status/1894567121793028158)  𝕏 